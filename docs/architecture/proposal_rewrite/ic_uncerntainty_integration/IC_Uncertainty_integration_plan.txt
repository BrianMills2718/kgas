# IC-Informed Uncertainty Integration Plan (REVISED)
## Intelligence Community Methodologies with CERQual Evidence Assessment

## üö® Critical Implementation Update

**Timeline**: 12 weeks - Realistic timeline for implementing actual IC methodologies
**Risk Level**: HIGH - Complex LLM integration implementing ACH, Key Assumptions Check, and ICD-203 standards
**Prerequisites**: Must verify dependencies and research IC methodologies before starting
**Scope**: Full IC analytical framework with CERQual-based evidence quality assessment

## Implementation Roadmap

### Phase 0: Dependencies Verification & IC Methodology Research (Week 1-2)
**Goal**: Verify dependencies, research IC methodologies, and assess integration risks

**Critical Tasks**:
1. **Research IC Methodologies** (NEW - Week 1)
   ```bash
   # Research and document IC analytical techniques
   # Key areas to research:
   # - ICD-203: Probability expressions and confidence levels
   # - Analysis of Competing Hypotheses (ACH) implementation
   # - ICD-206: Key Assumptions Check methodology
   # - Diagnostic Evidence Evaluation (Heuer's 4 Types)
   # - Cognitive bias detection frameworks
   
   # Create research notes
   mkdir -p docs/research/ic_methodologies
   # Document findings for implementation team
   ```
2. **Verify Experimental Code Dependencies** (Week 1)
   ```bash
   # Verify uncertainty engine exists
   ls -la /home/brian/projects/Digimons/experiments/uncertainty_stress_test_system/core_services/uncertainty_engine.py
   
   # Verify bayesian components exist
   ls -la /home/brian/projects/Digimons/experiments/uncertainty_stress_test_system/bayesian/llm_bayesian_inference.py
   
   # Test experimental code can be imported
   python -c "
   import sys
   sys.path.append('/home/brian/projects/Digimons/experiments/uncertainty_stress_test_system')
   from core_services.uncertainty_engine import UncertaintyEngine, ConfidenceScore
   print('‚úÖ Experimental uncertainty code accessible - will use CERQual components')
   "
   
   # Note: Experimental code provides CERQual assessment, not full IC methodologies
   # We'll use it for evidence quality assessment within IC framework
   ```

3. **Analyze Current ServiceManager API** (Week 2)
   ```bash
   # Check actual ServiceManager methods available
   python -c "
   from src.core.enhanced_service_manager import EnhancedServiceManager
   sm = EnhancedServiceManager()
   methods = [m for m in dir(sm) if not m.startswith('_')]
   print('Available ServiceManager methods:', methods)
   "
   
   # Verify provenance service current capabilities
   python -c "
   from src.services.provenance_service import ProvenanceService
   ps = ProvenanceService()
   cursor = ps.conn.execute('SELECT name FROM sqlite_master WHERE type=\"table\"')
   tables = [row[0] for row in cursor.fetchall()]
   print('Current provenance tables:', tables)
   "
   ```

4. **Resource Requirements Analysis** (Week 2)
   ```bash
   # Test memory requirements for uncertainty analysis
   python -c "
   import psutil
   import os
   initial_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024
   print(f'Baseline memory usage: {initial_memory:.1f} MB')
   
   # Simulate uncertainty analysis load
   import sys
   sys.path.append('/home/brian/projects/Digimons/experiments/uncertainty_stress_test_system')
   try:
       from core_services.uncertainty_engine import UncertaintyEngine
       # Don't actually initialize with API key, just test memory impact
       print('‚úÖ Uncertainty engine importable without API initialization')
   except Exception as e:
       print(f'‚ùå Import error: {e}')
   "
   ```

5. **API Dependency Risk Assessment** (Week 2)
   ```bash
   # Test OpenAI API availability (if key exists)
   python -c "
   import os
   if os.getenv('OPENAI_API_KEY'):
       print('‚úÖ OpenAI API key available')
       # Test basic API connectivity
       import requests
       headers = {'Authorization': f'Bearer {os.getenv(\"OPENAI_API_KEY\")}'}
       try:
           response = requests.get('https://api.openai.com/v1/models', headers=headers, timeout=5)
           if response.status_code == 200:
               print('‚úÖ OpenAI API accessible') 
           else:
               print(f'‚ö†Ô∏è OpenAI API returned {response.status_code}')
       except Exception as e:
           print(f'‚ùå OpenAI API connection failed: {e}')
   else:
       print('‚ùå No OpenAI API key - will need fallback strategy')
   "
   ```

**Risk Assessment Results**:
- [ ] IC methodology research complete ‚úÖ/‚ùå (NEW)
- [ ] Experimental code accessible for CERQual components ‚úÖ/‚ùå
- [ ] ServiceManager API compatible ‚úÖ/‚ùå  
- [ ] Memory requirements acceptable (<200MB) ‚úÖ/‚ùå
- [ ] LLM API connectivity verified ‚úÖ/‚ùå
- [ ] Database migration path clear ‚úÖ/‚ùå

**Success Criteria**: ALL risk assessment items pass before proceeding to Phase 1

### Phase 1: IC Methodology Implementation (Week 3-5)
**Goal**: Implement actual IC analytical methodologies with CERQual evidence assessment

**Core IC Implementation Tasks**:
1. **Implement IC Analytical Framework** (Week 3)
   - Create `src/services/uncertainty/ic_methodologies.py`
   - Implement Analysis of Competing Hypotheses (ACH)
   - Implement Key Assumptions Check (ICD-206)
   - Implement Diagnostic Evidence Evaluation (Heuer's 4 Types)
   - Implement ICD-203 probability expressions
   - Create prompts for each IC methodology

2. **Implement IC+CERQual Integration Service** (Week 4)
   - File: `src/services/ic_uncertainty_service.py`
   - Integrate CERQual for evidence quality assessment
   - Use CERQual scores as input to IC methodologies
   - Implement cognitive bias detection
   - Add comprehensive IC analysis with audit trails

3. **ServiceManager Integration** (Week 5)
   - Update `src/core/enhanced_service_manager.py` CAREFULLY
   - Use backwards-compatible extension pattern (no breaking changes to existing API)
   - Add lazy initialization with error recovery
   - Test with existing services to ensure no regression

**Verification Commands**:
```bash
# Test IC methodologies implementation
python -c "
from src.services.uncertainty.ic_methodologies import ICMethodologies
ic = ICMethodologies()
# Test ACH implementation
hypotheses = ic.generate_competing_hypotheses('What causes organizational change?')
print(f'‚úÖ ACH generates {len(hypotheses)} competing hypotheses')
# Test Key Assumptions Check
assumptions = ic.identify_key_assumptions('Leadership drives organizational change')
print(f'‚úÖ Key Assumptions Check identifies {len(assumptions)} assumptions')
"

# Test IC+CERQual integration service
python -c "
from src.services.ic_uncertainty_service import ICUncertaintyService
service = ICUncertaintyService(None, api_key=None)  # Test without API
# Test integrated analysis
result = service.analyze_with_ic_cerqual('Research question', [{'text': 'evidence'}])
print(f'‚úÖ IC+CERQual analysis: {result.final_confidence:.3f}')
print(f'‚úÖ ACH hypotheses: {len(result.competing_hypotheses)}')
print(f'‚úÖ Key assumptions: {len(result.key_assumptions)}')
"

# Test ServiceManager backwards compatibility
python -c "
from src.core.enhanced_service_manager import EnhancedServiceManager
sm = EnhancedServiceManager()
# Test existing services still work
try:
    ps = sm.get_provenance_service() if hasattr(sm, 'get_provenance_service') else 'not available'
    print(f'‚úÖ Existing services preserved: {type(ps).__name__ if ps != \"not available\" else ps}')
except Exception as e:
    print(f'‚ùå ServiceManager regression: {e}')
"
```

**Success Criteria**: 
- IC methodologies implemented (ACH, Key Assumptions Check, ICD-203)
- CERQual integration provides evidence quality scores
- IC+CERQual service works without LLM (fallback mode)
- No regression in existing services

### Phase 2: Database Migration & Provenance Extension (Week 6-7)
**Goal**: Extend ProvenanceService with proper database migration and uncertainty audit trails

**Critical Database Migration Tasks**:
1. **Pre-Migration Safety** (Week 6)
   ```bash
   # MANDATORY: Backup existing database before any changes
   cp provenance.db provenance_backup_$(date +%Y%m%d_%H%M%S).db
   
   # Verify backup integrity
   python -c "
   import sqlite3
   conn = sqlite3.connect('provenance_backup_$(date +%Y%m%d_%H%M%S).db')
   cursor = conn.execute('SELECT COUNT(*) FROM operations')
   count = cursor.fetchone()[0]
   print(f'‚úÖ Backup contains {count} operations')
   conn.close()
   "
   ```

2. **Create Migration Scripts** (Week 6)
   - File: `src/services/migrations/001_add_uncertainty_tables.py`
   - Add uncertainty_calculations table (with proper foreign keys)
   - Add calculation_steps table for audit trail
   - Add rollback capability for failed migrations
   
3. **Safe Schema Extension** (Week 7)
   ```python
   # Migration approach - ADDITIVE ONLY (no breaking changes)
   def migrate_uncertainty_tables(conn):
       # Add new tables without modifying existing ones
       conn.execute("""
           CREATE TABLE IF NOT EXISTS uncertainty_calculations (
               calculation_id TEXT PRIMARY KEY,
               operation_id TEXT,
               tool_id TEXT NOT NULL,
               research_question TEXT,
               stage_confidences TEXT,  -- JSON array
               final_confidence REAL,
               calculation_method TEXT DEFAULT 'cerqual_basic',
               audit_trail TEXT,  -- JSON audit steps
               created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
               FOREIGN KEY (operation_id) REFERENCES operations(operation_id)
           )
       """)
   ```

4. **Gradual Provenance Integration**
   - Extend ProvenanceService with NEW methods (don't modify existing ones)
   - Add uncertainty tracking as optional feature
   - Test backward compatibility extensively

**Verification Commands**:
```bash
# Test database migration safety
python -c "
from src.services.provenance_service import ProvenanceService
# Test existing functionality still works
ps = ProvenanceService()
op_id = ps.start_operation('TEST_TOOL', 'test_op', [], {})
ps.complete_operation(op_id, {}, {})
print('‚úÖ Existing provenance functionality preserved')

# Test new uncertainty tables exist
cursor = ps.conn.execute('SELECT name FROM sqlite_master WHERE type=\"table\" AND name LIKE \"%uncertainty%\"')
uncertainty_tables = [row[0] for row in cursor.fetchall()]
print(f'New uncertainty tables: {uncertainty_tables}')
"

# Test rollback capability
python -c "
# Verify we can rollback if needed
import sqlite3
import os
if os.path.exists('provenance_backup_*'):
    print('‚úÖ Rollback capability available')
else:
    print('‚ùå No backup available - CRITICAL ISSUE')
"
```

**Success Criteria**: 
- Database migration completes without data loss
- All existing functionality preserved  
- New uncertainty tracking works
- Rollback procedure tested

### Phase 3: Limited Tool Integration (Week 8-9)
**Goal**: Integrate IC uncertainty into ONE tool first (T31 Entity Builder) as proof of concept

**Conservative Integration Strategy**:
1. **Single Tool Focus** (Week 8)
   - Choose T31EntityBuilderKGAS as integration target (simplest tool)
   - Create optional IC uncertainty assessment (off by default)
   - Add uncertainty_enabled parameter to tool configuration
   - NO changes to ToolResult class initially (extend later)

2. **Backwards-Compatible Extension** (Week 9)
   ```python
   # Safe extension pattern - no breaking changes
   class T31EntityBuilderKGAS:
       def __init__(self, service_manager, enable_uncertainty=False):
           self.service_manager = service_manager
           self.enable_uncertainty = enable_uncertainty
           if enable_uncertainty:
               self.ic_service = service_manager.get_ic_uncertainty_service()
   
       def execute(self, request):
           result = self._original_execute(request)  # Existing logic unchanged
           
           if self.enable_uncertainty:
               # Add uncertainty assessment AFTER original processing
               result = self._add_uncertainty_assessment(result, request)
           
           return result
   ```

3. **Comprehensive Testing**
   ```bash
   # Test tool without uncertainty (default behavior)
   python -c "
   from src.tools.phase1.t31_entity_builder_kgas import T31EntityBuilderKGAS
   from src.core.enhanced_service_manager import EnhancedServiceManager
   sm = EnhancedServiceManager()
   tool = T31EntityBuilderKGAS(sm, enable_uncertainty=False)
   print('‚úÖ Tool works without uncertainty (backward compatible)')
   "
   
   # Test tool with uncertainty (new feature)
   python -c "
   from src.tools.phase1.t31_entity_builder_kgas import T31EntityBuilderKGAS
   from src.core.enhanced_service_manager import EnhancedServiceManager
   sm = EnhancedServiceManager()
   tool = T31EntityBuilderKGAS(sm, enable_uncertainty=True)
   print('‚úÖ Tool supports uncertainty assessment')
   "
   ```

**Success Criteria**: 
- T31 works identically to before when uncertainty disabled
- T31 provides uncertainty assessment when enabled
- No performance regression in default mode
- Uncertainty assessment completes in <10 seconds

### Phase 4: Performance & Caching Implementation (Week 10-11)
**Goal**: Add caching and performance monitoring (AFTER basic functionality proven)

**Realistic Performance Tasks**:
1. **Baseline Performance Measurement** (Week 10)
   ```bash
   # Establish baseline performance metrics
   python -c "
   import time
   from src.services.ic_uncertainty_service import ICUncertaintyService
   service = ICUncertaintyService(None)
   
   start_time = time.time()
   # Test basic confidence calculation 10 times
   for i in range(10):
       result = service.calculate_basic_confidence([0.7, 0.8, 0.6])
   end_time = time.time()
   
   avg_time = (end_time - start_time) / 10
   print(f'Baseline: {avg_time:.3f}s per calculation')
   print(f'Target: <2.0s per calculation')
   "
   ```

2. **Simple Caching Implementation** (Week 10)
   - Add basic LRU cache for identical calculations (using functools.lru_cache)
   - Add cache hit/miss metrics
   - Add cache invalidation strategy (TTL: 1 hour)
   - **REALISTIC TARGET**: 2x performance improvement for repeated calculations

3. **Memory Management** (Week 11)
   ```python
   # Conservative caching - avoid memory issues
   from functools import lru_cache
   
   class ICUncertaintyService:
       @lru_cache(maxsize=100)  # Small cache to avoid memory issues
       def _cached_calculation(self, confidence_tuple):
           # Cache only deterministic calculations
           return self._calculate_uncertainty(confidence_tuple)
   
       def clear_cache_if_needed(self):
           # Clear cache if memory usage > 500MB
           import psutil
           if psutil.Process().memory_info().rss > 500 * 1024 * 1024:
               self._cached_calculation.cache_clear()
   ```

4. **Performance Monitoring**
   ```bash
   # Test caching effectiveness
   python -c "
   from src.services.ic_uncertainty_service import ICUncertaintyService
   service = ICUncertaintyService(None)
   
   # Test cache hit ratio
   test_input = (0.7, 0.8, 0.6)
   
   # First call (cache miss)
   import time
   start = time.time()
   result1 = service._cached_calculation(test_input)
   time1 = time.time() - start
   
   # Second call (cache hit)
   start = time.time()
   result2 = service._cached_calculation(test_input)
   time2 = time.time() - start
   
   print(f'Cache miss: {time1:.4f}s')
   print(f'Cache hit: {time2:.4f}s')
   print(f'Speedup: {time1/time2:.1f}x')
   "
   ```

**Success Criteria**: 
- 2x performance improvement for repeated calculations (NOT 10x)
- Memory usage stays under 200MB
- Cache hit rate >30% in typical usage

### Phase 5: Testing & Validation (Week 12)
**Goal**: Comprehensive testing and validation before production deployment

**Critical Testing Tasks**:
1. **Integration Test Suite**
   ```bash
   # Create comprehensive test suite
   cat > test_ic_integration_comprehensive.py << 'EOF'
   #!/usr/bin/env python3
   """Comprehensive IC uncertainty integration test"""
   
   import sys
   import time
   from src.core.enhanced_service_manager import EnhancedServiceManager
   from src.services.ic_uncertainty_service import ICUncertaintyService
   
   def test_backward_compatibility():
       """Test that existing services still work"""
       print("Testing backward compatibility...")
       sm = EnhancedServiceManager()
       # Test existing services aren't broken
       ps = sm.get_provenance_service()
       op_id = ps.start_operation('TEST', 'test', [], {})
       ps.complete_operation(op_id, {}, {})
       print("‚úÖ Backward compatibility OK")
   
   def test_uncertainty_service_basic():
       """Test basic uncertainty service"""
       print("Testing basic uncertainty service...")
       service = ICUncertaintyService(None)
       result = service.calculate_basic_confidence([0.7, 0.8, 0.6])
       assert 0 < result < 1, f"Invalid confidence: {result}"
       print(f"‚úÖ Basic uncertainty service OK: {result:.3f}")
   
   def test_database_migration():
       """Test database migration didn't break anything"""
       print("Testing database migration...")
       from src.services.provenance_service import ProvenanceService
       ps = ProvenanceService()
       # Check both old and new tables exist
       cursor = ps.conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
       tables = [row[0] for row in cursor.fetchall()]
       assert 'operations' in tables, "Original operations table missing"
       print("‚úÖ Database migration OK")
   
   def test_performance_requirements():
       """Test performance meets requirements"""
       print("Testing performance requirements...")
       service = ICUncertaintyService(None)
       
       start_time = time.time()
       for i in range(5):
           result = service.calculate_basic_confidence([0.7, 0.8, 0.6])
       end_time = time.time()
       
       avg_time = (end_time - start_time) / 5
       print(f"Average calculation time: {avg_time:.3f}s")
       assert avg_time < 2.0, f"Performance requirement failed: {avg_time}s > 2.0s"
       print("‚úÖ Performance requirements met")
   
   if __name__ == "__main__":
       try:
           test_backward_compatibility()
           test_uncertainty_service_basic()
           test_database_migration()
           test_performance_requirements()
           print("\nüéâ ALL INTEGRATION TESTS PASSED")
       except Exception as e:
           print(f"\n‚ùå INTEGRATION TEST FAILED: {e}")
           sys.exit(1)
   EOF
   
   python test_ic_integration_comprehensive.py
   ```

2. **Load Testing**
   ```bash
   # Test system under load
   python -c "
   import concurrent.futures
   import time
   from src.services.ic_uncertainty_service import ICUncertaintyService
   
   def run_uncertainty_calculation():
       service = ICUncertaintyService(None)
       return service.calculate_basic_confidence([0.7, 0.8, 0.6])
   
   print('Testing under concurrent load...')
   start_time = time.time()
   
   with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
       futures = [executor.submit(run_uncertainty_calculation) for _ in range(20)]
       results = [f.result() for f in futures]
   
   end_time = time.time()
   total_time = end_time - start_time
   
   print(f'‚úÖ Processed 20 calculations in {total_time:.2f}s')
   print(f'Average: {total_time/20:.3f}s per calculation')
   assert total_time < 40, f'Load test failed: {total_time}s > 40s'
   "
   ```

3. **Error Recovery Testing**
   ```bash
   # Test error conditions and recovery
   python -c "
   from src.services.ic_uncertainty_service import ICUncertaintyService
   
   # Test with invalid inputs
   service = ICUncertaintyService(None)
   
   try:
       result = service.calculate_basic_confidence([])  # Empty input
       print(f'Empty input handled: {result}')
   except Exception as e:
       print(f'‚úÖ Empty input error handled: {e}')
   
   try:
       result = service.calculate_basic_confidence([1.5, -0.2, 0.7])  # Invalid values
       print(f'Invalid values handled: {result}')
   except Exception as e:
       print(f'‚úÖ Invalid values error handled: {e}')
   "
   ```

**Success Criteria**:
- All integration tests pass
- System handles 20 concurrent requests in <40 seconds
- Graceful error handling for all invalid inputs
- No memory leaks after 100 calculations
- Database rollback procedure works

## üìä Revised Expected Benefits & Timeline

### Realistic Benefits
- **IC Analytical Framework**: Full Intelligence Community methodologies (ACH, Key Assumptions Check, ICD-203)
- **Evidence Quality Assessment**: CERQual-based evidence quality scoring integrated into IC analysis
- **Academic Rigor**: Systematic bias detection and competing hypothesis analysis
- **Transparency**: Complete audit trail for all IC analytical steps
- **Research Guidance**: Clear probability expressions and decision support using IC standards
- **Integration**: Seamless integration preserving ALL existing functionality
- **Performance**: 2x performance improvement for repeated calculations (realistic)

### Realistic Performance Expectations
- **Processing Time**: 1-3 seconds per uncertainty calculation (not <1 second)
- **Memory Usage**: <200MB additional overhead (conservative estimate)
- **Caching Hit Rate**: 30-50% in typical usage (not 60-80%)
- **API Dependency**: Graceful fallback when LLM APIs unavailable

### Risk Mitigation Strategies
1. **Phase 0 Verification**: Comprehensive dependency checking before starting
2. **Database Backup**: Mandatory backup before any schema changes
3. **Rollback Procedures**: Tested rollback for every migration step  
4. **Gradual Integration**: Single tool integration before pipeline-wide changes
5. **Performance Monitoring**: Continuous monitoring during each phase
6. **Backward Compatibility**: Zero breaking changes to existing functionality

## üéØ Implementation Priorities & Decision Points

### MUST COMPLETE Phase 0 First
- **STOP SIGN**: Do not proceed to Phase 1 unless ALL Phase 0 verification tasks pass
- **If ANY dependency verification fails**: Reassess approach, don't force implementation
- **If API connectivity fails**: Implement offline-first strategy

### Key Decision Points
1. **Week 2**: If IC methodology research incomplete ‚Üí Extend research phase, don't start implementation
2. **Week 2**: If experimental code not accessible ‚Üí Implement CERQual from scratch within IC framework
3. **Week 5**: If ServiceManager integration causes regression ‚Üí Roll back and redesign
4. **Week 7**: If database migration fails ‚Üí Use external uncertainty service approach
5. **Week 9**: If tool integration degrades performance ‚Üí Make uncertainty opt-in only
6. **Week 12**: If tests don't pass ‚Üí Do not deploy to production

### Realistic Success Metrics
- **IC Implementation Success**: ACH, Key Assumptions Check, and ICD-203 probability bands working
- **Integration Success**: Zero breaking changes to existing functionality  
- **Performance Success**: <3 second IC+CERQual calculations, <200MB memory overhead
- **Quality Success**: IC+CERQual assessments provide comprehensive research guidance
- **Analytical Success**: Competing hypotheses analysis and bias detection functional
- **Reliability Success**: System degrades gracefully when LLM APIs unavailable

## üìù Implementation Notes for Future Developers

This revised plan implements actual IC methodologies while addressing critical issues:

1. **Actual IC Implementation**: Full Intelligence Community methodologies (ACH, Key Assumptions Check, ICD-203)
2. **CERQual Integration**: Uses experimental code for evidence quality assessment within IC framework
3. **Timeline Reality**: 12 weeks for comprehensive IC+CERQual implementation
4. **Risk Management**: Phase 0 verification prevents implementation failures
5. **Academic Rigor**: Systematic competing hypothesis analysis and bias detection
6. **Backward Compatibility**: Zero breaking changes through careful extension patterns
7. **Comprehensive Testing**: Real load testing and error recovery validation
8. **Database Safety**: Mandatory backups and rollback procedures

The integration provides sophisticated IC analytical capabilities with CERQual evidence quality assessment, ensuring academic-quality uncertainty analysis with complete calculation transparency.

---

**CRITICAL**: This plan requires careful, methodical execution with extensive testing at each phase. The original plan's optimistic timeline and unrealistic performance claims have been replaced with conservative, achievable goals that prioritize production stability over ambitious features.

## Complete IC+CERQual Integration Architecture

```python
# src/services/uncertainty/ic_methodologies.py
"""
Intelligence Community Analytical Methodologies Implementation
Implements ACH, Key Assumptions Check, ICD-203, and other IC techniques
"""

from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class ICDProbabilityBand(str, Enum):
    """ICD-203 Standard Probability Expressions"""
    ALMOST_NO_CHANCE = "almost_no_chance"      # 01-05%
    VERY_UNLIKELY = "very_unlikely"            # 05-20%
    UNLIKELY = "unlikely"                      # 20-45%
    ROUGHLY_EVEN_CHANCE = "roughly_even_chance" # 45-55%
    LIKELY = "likely"                          # 55-80%
    VERY_LIKELY = "very_likely"                # 80-95%
    ALMOST_CERTAIN = "almost_certain"          # 95-99%

@dataclass
class CompetingHypothesis:
    """ACH Competing Hypothesis"""
    hypothesis: str
    supporting_evidence: List[str]
    contradicting_evidence: List[str]
    consistency_score: float
    plausibility: float

@dataclass
class KeyAssumption:
    """ICD-206 Key Assumption"""
    assumption: str
    criticality: str  # low/moderate/high
    evidence_basis: str
    impact_if_wrong: str
    testability: str

class ICMethodologies:
    """Intelligence Community Analytical Methodologies"""
    
    def __init__(self, llm_service=None):
        self.llm_service = llm_service
    
    async def analysis_of_competing_hypotheses(self, research_question: str, 
                                             evidence: List[Dict]) -> List[CompetingHypothesis]:
        """Implement Analysis of Competing Hypotheses (ACH)"""
        
        if self.llm_service:
            prompt = f"""
            Generate 3-5 plausible competing hypotheses for: {research_question}
            
            For each hypothesis:
            1. State the hypothesis clearly
            2. List supporting evidence
            3. List contradicting evidence (focus on disconfirming evidence)
            4. Rate consistency with evidence (0.0-1.0)
            5. Rate plausibility (0.0-1.0)
            
            Evidence available: {evidence[:3]}...
            
            Use ACH methodology: focus on which hypothesis is LEAST disproven.
            """
            
            response = await self.llm_service.analyze_structured(prompt)
            return self._parse_ach_response(response)
        else:
            # Fallback: basic competing hypotheses
            return self._generate_fallback_hypotheses(research_question)
    
    async def key_assumptions_check(self, research_question: str, 
                                  analysis_context: str) -> List[KeyAssumption]:
        """Implement Key Assumptions Check (ICD-206)"""
        
        if self.llm_service:
            prompt = f"""
            Identify key assumptions underlying this analysis: {research_question}
            Context: {analysis_context}
            
            For each assumption:
            1. State the assumption explicitly
            2. Rate criticality (low/moderate/high)
            3. Describe evidence basis
            4. Analyze impact if assumption is wrong
            5. Assess testability of assumption
            
            Focus on assumptions that most affect the conclusion.
            """
            
            response = await self.llm_service.analyze_structured(prompt)
            return self._parse_assumptions_response(response)
        else:
            # Fallback: basic assumption identification
            return self._generate_fallback_assumptions(research_question)
    
    def diagnostic_evidence_evaluation(self, evidence: List[Dict], 
                                     hypotheses: List[CompetingHypothesis]) -> List[Dict]:
        """Evaluate evidence using Heuer's 4 Types (Diagnostic/Consistent/Anomalous/Irrelevant)"""
        
        diagnostic_evidence = []
        
        for evidence_item in evidence:
            # Evaluate how well this evidence discriminates between hypotheses
            discrimination_score = self._calculate_evidence_discrimination(evidence_item, hypotheses)
            
            if discrimination_score > 0.7:
                evidence_type = "diagnostic"
            elif discrimination_score > 0.4:
                evidence_type = "consistent"
            elif discrimination_score > 0.1:
                evidence_type = "anomalous"
            else:
                evidence_type = "irrelevant"
            
            diagnostic_evidence.append({
                "evidence": evidence_item,
                "type": evidence_type,
                "discrimination_score": discrimination_score,
                "diagnostic_value": discrimination_score
            })
        
        return diagnostic_evidence
    
    def assess_ic_probability(self, analysis_results: Dict) -> Dict[str, Any]:
        """Convert analysis to ICD-203 probability expressions"""
        
        # Aggregate confidence from competing hypotheses analysis
        if analysis_results.get('competing_hypotheses'):
            best_hypothesis = max(analysis_results['competing_hypotheses'], 
                                key=lambda h: h.consistency_score)
            confidence = best_hypothesis.consistency_score
        else:
            confidence = 0.5  # Default even chance
        
        # Map to ICD-203 probability bands
        if confidence >= 0.95:
            probability_band = ICDProbabilityBand.ALMOST_CERTAIN
        elif confidence >= 0.80:
            probability_band = ICDProbabilityBand.VERY_LIKELY
        elif confidence >= 0.55:
            probability_band = ICDProbabilityBand.LIKELY
        elif confidence >= 0.45:
            probability_band = ICDProbabilityBand.ROUGHLY_EVEN_CHANCE
        elif confidence >= 0.20:
            probability_band = ICDProbabilityBand.UNLIKELY
        elif confidence >= 0.05:
            probability_band = ICDProbabilityBand.VERY_UNLIKELY
        else:
            probability_band = ICDProbabilityBand.ALMOST_NO_CHANCE
        
        return {
            "probability_band": probability_band.value,
            "numeric_confidence": confidence,
            "confidence_in_assessment": "moderate"  # Could be enhanced
        }


# src/services/ic_uncertainty_service.py
"""
IC-Informed Uncertainty Service with CERQual Evidence Assessment
Integrates IC methodologies with CERQual evidence quality scoring
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from src.core.confidence_scoring.data_models import ConfidenceScore
from src.services.provenance_service import ProvenanceService
from src.services.uncertainty.ic_methodologies import ICMethodologies, CompetingHypothesis, KeyAssumption

@dataclass
class ICCERQualAnalysisResult:
    """Comprehensive IC+CERQual Analysis Result"""
    
    # IC Analysis Components
    competing_hypotheses: List[CompetingHypothesis]
    key_assumptions: List[KeyAssumption]
    diagnostic_evidence: List[Dict[str, Any]]
    cognitive_biases_detected: List[str]
    information_gaps: List[str]
    
    # IC Probability Assessment (ICD-203)
    probability_band: str
    numeric_confidence: float
    confidence_in_assessment: str
    
    # CERQual Evidence Quality Assessment
    cerqual_methodological_quality: float
    cerqual_relevance: float  
    cerqual_coherence: float
    cerqual_adequacy: float
    
    # Integrated Assessment
    final_confidence: float
    research_guidance: Dict[str, str]
    
    # Audit Trail
    calculation_trace: Dict[str, Any]
    processing_time: float


class ICUncertaintyService:
    """
    IC-Informed Uncertainty Service with CERQual Evidence Assessment
    Provides comprehensive uncertainty analysis using IC methodologies
    """
    
    def __init__(self, provenance_service: ProvenanceService, llm_service=None):
        self.provenance_service = provenance_service
        self.llm_service = llm_service
        
        # Initialize IC methodologies
        self.ic_methods = ICMethodologies(llm_service)
        
        # Initialize CERQual assessment (from experimental code)
        if llm_service:
            try:
                import sys
                sys.path.append('/path/to/experiments/uncertainty_stress_test_system')
                from core_services.uncertainty_engine import UncertaintyEngine
                self.cerqual_engine = UncertaintyEngine(api_key=llm_service.api_key)
            except ImportError:
                self.cerqual_engine = None
        else:
            self.cerqual_engine = None
    
    async def analyze_with_ic_cerqual(self, research_question: str, 
                                    evidence: List[Dict[str, Any]],
                                    context: Dict = None) -> ICCERQualAnalysisResult:
        """
        Comprehensive IC+CERQual uncertainty analysis
        1. CERQual assessment of evidence quality
        2. IC methodologies analysis using quality-assessed evidence
        3. Integration and final assessment
        """
        start_time = datetime.now()
        
        # Step 1: CERQual Evidence Quality Assessment
        cerqual_scores = await self._assess_evidence_quality_cerqual(evidence)
        
        # Step 2: IC Methodologies Analysis
        competing_hypotheses = await self.ic_methods.analysis_of_competing_hypotheses(
            research_question, evidence
        )
        
        key_assumptions = await self.ic_methods.key_assumptions_check(
            research_question, context or {}
        )
        
        diagnostic_evidence = self.ic_methods.diagnostic_evidence_evaluation(
            evidence, competing_hypotheses
        )
        
        # Step 3: IC Probability Assessment
        ic_probability = self.ic_methods.assess_ic_probability({
            'competing_hypotheses': competing_hypotheses,
            'diagnostic_evidence': diagnostic_evidence
        })
        
        # Step 4: Integration of IC + CERQual
        final_confidence = self._integrate_ic_cerqual_confidence(
            ic_probability['numeric_confidence'],
            cerqual_scores
        )
        
        # Step 5: Research Guidance
        research_guidance = self._generate_research_guidance(
            final_confidence, 
            ic_probability['probability_band'],
            competing_hypotheses
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return ICCERQualAnalysisResult(
            competing_hypotheses=competing_hypotheses,
            key_assumptions=key_assumptions,
            diagnostic_evidence=diagnostic_evidence,
            cognitive_biases_detected=self._detect_cognitive_biases(evidence),
            information_gaps=self._identify_information_gaps(evidence, competing_hypotheses),
            probability_band=ic_probability['probability_band'],
            numeric_confidence=ic_probability['numeric_confidence'],
            confidence_in_assessment=ic_probability['confidence_in_assessment'],
            cerqual_methodological_quality=cerqual_scores['methodological_quality'],
            cerqual_relevance=cerqual_scores['relevance'],
            cerqual_coherence=cerqual_scores['coherence'],
            cerqual_adequacy=cerqual_scores['adequacy'],
            final_confidence=final_confidence,
            research_guidance=research_guidance,
            calculation_trace=self._create_calculation_trace(ic_probability, cerqual_scores),
            processing_time=processing_time
        )
    
    # Calculation Audit Trail
    calculation_trace: Dict[str, Any]
    uncertainty_provenance: Dict[str, Any]
    
    # Performance Metrics
    processing_time: float
    api_calls_made: int
    cached_results_used: int


class ICUncertaintyService:
    """
    Production IC-Informed Uncertainty Service
    Integrates experimental uncertainty engine into ServiceManager architecture
    """
    
    def __init__(self, provenance_service: ProvenanceService, llm_service):
        self.provenance_service = provenance_service
        self.llm_service = llm_service
        
        # Import and initialize experimental components
        from experiments.uncertainty_stress_test_system.core_services.uncertainty_engine import UncertaintyEngine
        from experiments.uncertainty_stress_test_system.bayesian.llm_bayesian_inference import BayesianAggregationService
        
        self.uncertainty_engine = UncertaintyEngine(api_key=llm_service.api_key)
        self.bayesian_service = BayesianAggregationService(api_key=llm_service.api_key)
        
        self.analysis_cache = {}
        self.performance_metrics = {
            "total_analyses": 0,
            "cache_hits": 0,
            "average_processing_time": 0.0
        }
    
    async def analyze_with_ic_methods(self, request: ICAnalysisRequest) -> ICAnalysisResult:
        """
        Perform comprehensive IC-informed uncertainty analysis
        This is the main integration point for IC methodologies
        """
        start_time = datetime.now()
        operation_id = self.provenance_service.start_operation(
            tool_id="IC_UNCERTAINTY_SERVICE",
            operation_type="ic_analysis",
            inputs={"research_question": request.research_question},
            parameters={"domain": request.domain, "evidence_count": len(request.evidence)}
        )
        
        try:
            # Single comprehensive LLM analysis (ADR-029 pattern)
            ic_analysis = await self._single_integrated_llm_analysis(request)
            
            # Mathematical uncertainty propagation (hard-coded, not LLM)
            propagation_result = self._mathematical_propagation(ic_analysis)
            
            # Create audit trail for calculation transparency
            audit_trail = self._create_calculation_audit_trail(ic_analysis, propagation_result)
            
            # Build result
            result = ICAnalysisResult(
                key_assumptions=ic_analysis.get('key_assumptions', []),
                competing_hypotheses=ic_analysis.get('competing_hypotheses', []),
                diagnostic_evidence=ic_analysis.get('diagnostic_evidence', []),
                information_gaps=ic_analysis.get('information_gaps', []),
                cognitive_biases=ic_analysis.get('cognitive_biases', []),
                probability_band=ic_analysis.get('probability_band', 'unknown'),
                confidence_level=ic_analysis.get('confidence_level', 'moderate'),
                stage_confidences=propagation_result['stage_confidences'],
                final_confidence=propagation_result['final_confidence'],
                propagation_method=propagation_result['method'],
                calculation_trace=audit_trail,
                uncertainty_provenance=self._create_uncertainty_provenance(request, ic_analysis),
                processing_time=(datetime.now() - start_time).total_seconds(),
                api_calls_made=self.uncertainty_engine.api_calls_made,
                cached_results_used=self.uncertainty_engine.cache_hits
            )
            
            # Complete provenance tracking
            self.provenance_service.complete_operation(
                operation_id=operation_id,
                outputs={"final_confidence": result.final_confidence},
                metadata={
                    "ic_analysis": True,
                    "probability_band": result.probability_band,
                    "processing_time": result.processing_time
                }
            )
            
            return result
            
        except Exception as e:
            self.provenance_service.complete_operation(
                operation_id=operation_id,
                success=False,
                error_message=str(e)
            )
            raise
    
    async def _single_integrated_llm_analysis(self, request: ICAnalysisRequest) -> Dict[str, Any]:
        """
        Single comprehensive LLM call for all IC methodologies
        Implements ADR-029 pattern of integrated analysis
        """
        prompt = f"""
        Conduct comprehensive IC-informed uncertainty analysis following Intelligence Community methodologies.
        
        Research Question: {request.research_question}
        Domain: {request.domain}
        Evidence Count: {len(request.evidence)}
        
        Apply all IC methodologies systematically:
        
        1. KEY ASSUMPTIONS CHECK (ICD-206)
        - List critical assumptions underlying the analysis
        - Rate assumption criticality: low/moderate/high
        - Identify which assumptions most affect conclusions
        
        2. ANALYSIS OF COMPETING HYPOTHESES (ACH)
        - Generate 3-5 plausible alternative explanations
        - Evaluate evidence FOR and AGAINST each hypothesis
        - Focus on disconfirming evidence
        - Identify least disproven hypothesis
        
        3. DIAGNOSTIC EVIDENCE EVALUATION (Heuer's 4 Types)
        - Categorize evidence: Diagnostic/Consistent/Anomalous/Irrelevant
        - Assign diagnostic value scores (0.0-1.0)
        - Identify most discriminating evidence
        
        4. INFORMATION GAPS ASSESSMENT
        - Identify critical information gaps
        - Assess impact of gaps on conclusion confidence
        - Suggest monitoring indicators
        
        5. COGNITIVE BIAS DETECTION
        - Check for confirmation bias indicators
        - Assess availability heuristic risks
        - Identify anchoring effects
        - Flag mirror imaging assumptions
        
        6. IC PROBABILITY ASSESSMENT (ICD-203)
        - Express likelihood using standard IC terms:
          [almost_no_chance, very_unlikely, unlikely, roughly_even_chance, 
           likely, very_likely, almost_certain]
        - Separate confidence IN assessment from probability OF outcome
        - Confidence levels: low/moderate/high
        
        7. STRUCTURED ANALYTIC TECHNIQUES
        - Devil's Advocacy: Best argument against conclusion
        - What-if Analysis: Key assumption violations
        - Alternative Futures: Different evolution scenarios
        
        Evidence Details:
        {json.dumps(request.evidence[:3], indent=2)}...
        
        Provide structured JSON output with all sections clearly labeled.
        Focus on analytical rigor and transparency.
        """
        
        # Use existing LLM service infrastructure
        response = await self.llm_service.analyze_structured(
            prompt=prompt,
            max_tokens=2000,
            output_format="json"
        )
        
        return self._parse_ic_analysis_response(response)
    
    def _parse_ic_analysis_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response into structured IC analysis"""
        try:
            # Extract JSON from response
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                parsed = json.loads(json_str)
                
                # Validate required fields and provide defaults
                return {
                    'key_assumptions': parsed.get('key_assumptions', []),
                    'competing_hypotheses': parsed.get('competing_hypotheses', []),
                    'diagnostic_evidence': parsed.get('diagnostic_evidence', []),
                    'information_gaps': parsed.get('information_gaps', []),
                    'cognitive_biases': parsed.get('cognitive_biases', []),
                    'probability_band': parsed.get('probability_band', 'roughly_even_chance'),
                    'confidence_level': parsed.get('confidence_level', 'moderate'),
                    'assumption_robustness': parsed.get('assumption_robustness', 0.7),
                    'evidence_quality_score': parsed.get('evidence_quality_score', 0.8),
                    'hypothesis_discrimination': parsed.get('hypothesis_discrimination', 0.6),
                    'methodologies_applied': ['key_assumptions', 'competing_hypotheses', 'diagnostic_evidence']
                }
            else:
                return self._default_ic_analysis()
        except Exception as e:
            print(f"Error parsing IC analysis response: {e}")
            return self._default_ic_analysis()
    
    def _default_ic_analysis(self) -> Dict[str, Any]:
        """Default IC analysis when parsing fails"""
        return {
            'key_assumptions': [{"assumption": "Analysis may be incomplete", "criticality": "moderate"}],
            'competing_hypotheses': [{"hypothesis": "Default hypothesis", "support": 0.5}],
            'diagnostic_evidence': [{"evidence": "Limited evidence available", "diagnostic_value": 0.3}],
            'information_gaps': ["LLM response parsing failed"],
            'cognitive_biases': ["Potential confirmation bias"],
            'probability_band': 'roughly_even_chance',
            'confidence_level': 'low',
            'assumption_robustness': 0.5,
            'evidence_quality_score': 0.4,
            'hypothesis_discrimination': 0.3,
            'methodologies_applied': ['fallback_analysis']
        }
    
    def _mathematical_propagation(self, ic_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Hard-coded mathematical uncertainty propagation
        Implements root-sum-squares for independent uncertainties
        """
        # Extract confidence components from IC analysis
        assumption_confidence = ic_analysis.get('assumption_robustness', 0.7)
        evidence_quality = ic_analysis.get('evidence_quality_score', 0.8)
        hypothesis_differentiation = ic_analysis.get('hypothesis_discrimination', 0.6)
        
        stage_confidences = [assumption_confidence, evidence_quality, hypothesis_differentiation]
        
        # Root-sum-squares propagation for independent uncertainties
        # œÉ_total = ‚àö(œÉ‚ÇÅ¬≤ + œÉ‚ÇÇ¬≤ + ... + œÉ‚Çô¬≤)
        variances = [(1 - conf)**2 for conf in stage_confidences]
        combined_variance = sum(variances)
        combined_uncertainty = (combined_variance ** 0.5)
        final_confidence = max(0.01, min(0.99, 1 - combined_uncertainty))
        
        return {
            "stage_confidences": stage_confidences,
            "final_confidence": final_confidence,
            "method": "root_sum_squares",
            "calculation_details": {
                "variances": variances,
                "combined_variance": combined_variance,
                "combined_uncertainty": combined_uncertainty
            }
        }
    
    def _create_calculation_audit_trail(self, ic_analysis: Dict, propagation: Dict) -> Dict[str, Any]:
        """
        Create detailed audit trail for calculation transparency
        This solves the "uncertainty provenance" requirement
        """
        return {
            "calculation_method": propagation["method"],
            "input_confidences": propagation["stage_confidences"],
            "calculation_steps": [
                {
                    "step": 1,
                    "description": "Convert confidences to variances",
                    "formula": "(1 - confidence)¬≤",
                    "inputs": propagation["stage_confidences"],
                    "outputs": propagation["calculation_details"]["variances"]
                },
                {
                    "step": 2,
                    "description": "Sum variances for independent uncertainties",
                    "formula": "œÉ‚ÇÅ¬≤ + œÉ‚ÇÇ¬≤ + ... + œÉ‚Çô¬≤",
                    "inputs": propagation["calculation_details"]["variances"],
                    "outputs": propagation["calculation_details"]["combined_variance"]
                },
                {
                    "step": 3,
                    "description": "Take square root for combined uncertainty",
                    "formula": "‚àö(combined_variance)",
                    "inputs": propagation["calculation_details"]["combined_variance"],
                    "outputs": propagation["calculation_details"]["combined_uncertainty"]
                },
                {
                    "step": 4,
                    "description": "Convert back to confidence",
                    "formula": "1 - combined_uncertainty",
                    "inputs": propagation["calculation_details"]["combined_uncertainty"],
                    "outputs": propagation["final_confidence"]
                }
            ],
            "assumptions": [
                "Stage uncertainties are independent",
                "Normal distribution approximation",
                "Linear uncertainty propagation"
            ],
            "sensitivity_analysis": self._calculate_sensitivity(propagation["stage_confidences"]),
            "human_readable": self._generate_human_readable_calculation(propagation)
        }
    
    def _create_uncertainty_provenance(self, request: ICAnalysisRequest, ic_analysis: Dict) -> Dict[str, Any]:
        """
        Create W3C PROV-compliant uncertainty provenance record
        Integrates with existing provenance system
        """
        return {
            "provenance_type": "uncertainty_calculation",
            "activity": {
                "id": f"ic_analysis_{datetime.now().isoformat()}",
                "type": "ic_informed_uncertainty_analysis",
                "started_at": datetime.now().isoformat(),
                "used_methodologies": [
                    "key_assumptions_check",
                    "analysis_competing_hypotheses", 
                    "diagnostic_evidence_evaluation",
                    "cognitive_bias_detection",
                    "ic_probability_assessment"
                ]
            },
            "agent": {
                "id": "ic_uncertainty_service",
                "type": "service",
                "version": "1.0.0"
            },
            "entities": {
                "research_question": request.research_question,
                "evidence_items": len(request.evidence),
                "domain": request.domain
            },
            "derivation": {
                "ic_methodologies": ic_analysis.get('methodologies_applied', []),
                "mathematical_propagation": "root_sum_squares",
                "calculation_transparency": "full_audit_trail_available"
            }
        }
    
    def _calculate_sensitivity(self, stage_confidences: List[float]) -> Dict[str, float]:
        """Calculate sensitivity of final result to input changes"""
        sensitivities = {}
        base_result = self._mathematical_propagation({"assumption_robustness": stage_confidences[0], 
                                                     "evidence_quality_score": stage_confidences[1],
                                                     "hypothesis_discrimination": stage_confidences[2]})["final_confidence"]
        
        for i, confidence in enumerate(stage_confidences):
            # Test ¬±0.1 change
            modified_confidences = stage_confidences.copy()
            modified_confidences[i] = min(1.0, confidence + 0.1)
            modified_result = self._mathematical_propagation({
                "assumption_robustness": modified_confidences[0],
                "evidence_quality_score": modified_confidences[1], 
                "hypothesis_discrimination": modified_confidences[2]
            })["final_confidence"]
            
            sensitivities[f"stage_{i}_sensitivity"] = abs(modified_result - base_result)
        
        return sensitivities
    
    def _generate_human_readable_calculation(self, propagation: Dict) -> str:
        """Generate human-readable calculation explanation"""
        stages = propagation["stage_confidences"]
        variances = propagation["calculation_details"]["variances"]
        
        calculation = f"Stage confidences: {[f'{c:.3f}' for c in stages]}\n"
        calculation += f"Convert to variances: {[f'{v:.3f}' for v in variances]}\n"
        calculation += f"Sum variances: {' + '.join([f'{v:.3f}' for v in variances])} = {sum(variances):.3f}\n"
        calculation += f"Square root: ‚àö{sum(variances):.3f} = {sum(variances)**0.5:.3f}\n"
        calculation += f"Final confidence: 1 - {sum(variances)**0.5:.3f} = {propagation['final_confidence']:.3f}"
        
        return calculation


# Integration with ServiceManager
class EnhancedServiceManager:
    """Extended ServiceManager with IC Uncertainty Service"""
    
    def __init__(self):
        # ... existing initialization ...
        self._ic_uncertainty_service = None
    
    def get_ic_uncertainty_service(self) -> ICUncertaintyService:
        """Get or create IC Uncertainty Service"""
        if self._ic_uncertainty_service is None:
            provenance_service = self.get_provenance_service()
            llm_service = self.get_llm_service()
            self._ic_uncertainty_service = ICUncertaintyService(provenance_service, llm_service)
        return self._ic_uncertainty_service
```

## 2. Pipeline Integration Points

### Tool-Level Uncertainty Assessment

```python
# src/core/tool_contract.py - Enhanced ToolResult with IC Uncertainty

@dataclass
class ToolResult:
    """Enhanced ToolResult with IC-informed uncertainty"""
    
    # Existing fields
    status: str
    data: Any
    confidence: ConfidenceScore
    metadata: Dict[str, Any]
    request_id: str
    
    # New IC-informed uncertainty fields
    ic_uncertainty: Optional[ICAnalysisResult] = None
    uncertainty_level: str = "not_assessed"  # not_assessed/low/moderate/high
    research_guidance: Optional[Dict[str, str]] = None
    
    def get_research_suitability(self) -> Dict[str, str]:
        """Get research method recommendations based on uncertainty"""
        if not self.ic_uncertainty:
            return {"status": "UNKNOWN", "note": "No IC analysis performed"}
        
        confidence = self.ic_uncertainty.final_confidence
        
        if confidence >= 0.8:
            return {
                "status": "HIGH_CONFIDENCE",
                "methods": "Quantitative analysis, hypothesis testing appropriate",
                "note": "Suitable for confirmatory research"
            }
        elif confidence >= 0.65:
            return {
                "status": "MODERATE_CONFIDENCE", 
                "methods": "Mixed methods, exploratory analysis recommended",
                "note": "Good for pattern identification"
            }
        else:
            return {
                "status": "LOW_CONFIDENCE",
                "methods": "Qualitative methods, theory building recommended",
                "note": "Focus on understanding, not measurement"
            }


# Pipeline Integration Pattern
class ICUncertaintyMixin:
    """Mixin for tools that want IC uncertainty assessment"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.ic_service = self.service_manager.get_ic_uncertainty_service()
    
    async def assess_uncertainty(self, 
                               research_question: str,
                               evidence: List[Dict],
                               context: Dict = None) -> ICAnalysisResult:
        """Perform IC uncertainty assessment for this tool's operation"""
        
        request = ICAnalysisRequest(
            research_question=research_question,
            evidence=evidence,
            context=context or {},
            domain=getattr(self, 'domain', 'general'),
            tool_id=self.__class__.__name__
        )
        
        return await self.ic_service.analyze_with_ic_methods(request)
    
    def enhance_result_with_uncertainty(self, 
                                      result: ToolResult,
                                      research_question: str,
                                      evidence: List[Dict]) -> ToolResult:
        """Enhance tool result with IC uncertainty assessment"""
        
        # Perform IC analysis asynchronously
        ic_analysis = asyncio.create_task(
            self.assess_uncertainty(research_question, evidence)
        )
        
        # Wait for analysis
        ic_result = asyncio.get_event_loop().run_until_complete(ic_analysis)
        
        # Enhance result
        result.ic_uncertainty = ic_result
        result.uncertainty_level = ic_result.confidence_level
        result.research_guidance = result.get_research_suitability()
        
        return result


# Example: Enhanced Entity Builder with IC Uncertainty
class T31EntityBuilderWithIC(ICUncertaintyMixin, T31EntityBuilderKGAS):
    """Entity Builder enhanced with IC uncertainty assessment"""
    
    def execute(self, request: ToolRequest) -> ToolResult:
        # Execute normal entity building
        result = super().execute(request)
        
        if result.status == "success":
            # Enhance with IC uncertainty assessment
            entities = request.input_data.get('entities', [])
            evidence = [{"entity": e["text"], "type": e["type"]} for e in entities]
            
            research_question = f"How confident are we in the entity extraction and building for {len(entities)} entities?"
            
            result = self.enhance_result_with_uncertainty(
                result, research_question, evidence
            )
        
        return result
```

## 3. Provenance Extension for Calculation Audit Trails

### Enhanced Provenance Service

```python
# src/services/provenance_service.py - Extended for uncertainty calculations

class ProvenanceService:
    """Extended ProvenanceService with uncertainty calculation tracking"""
    
    def _create_tables(self):
        """Create enhanced tables including uncertainty calculations"""
        
        # Existing tables...
        
        # New uncertainty calculations table
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS uncertainty_calculations (
                calculation_id TEXT PRIMARY KEY,
                operation_id TEXT,
                tool_id TEXT NOT NULL,
                research_question TEXT,
                ic_methodologies TEXT,  -- JSON array of methodologies applied
                stage_confidences TEXT,  -- JSON array of stage confidences
                final_confidence REAL,
                calculation_method TEXT,
                calculation_steps TEXT,  -- JSON array of calculation steps
                assumptions TEXT,  -- JSON array of assumptions
                sensitivity_analysis TEXT,  -- JSON sensitivity data
                human_readable_explanation TEXT,
                processing_time_seconds REAL,
                api_calls_made INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (operation_id) REFERENCES operations (operation_id)
            )
        """)
        
        # New IC analysis components table
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS ic_analysis_components (
                component_id TEXT PRIMARY KEY,
                calculation_id TEXT,
                component_type TEXT,  -- key_assumptions, competing_hypotheses, etc.
                component_data TEXT,  -- JSON data for the component
                confidence_impact REAL,  -- How much this component affects confidence
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (calculation_id) REFERENCES uncertainty_calculations (calculation_id)
            )
        """)
        
        self.conn.commit()
    
    def record_uncertainty_calculation(self, 
                                     ic_result: ICAnalysisResult,
                                     operation_id: str,
                                     tool_id: str,
                                     research_question: str) -> str:
        """Record complete IC uncertainty calculation with audit trail"""
        
        calculation_id = str(uuid.uuid4())
        
        # Insert main calculation record
        self.conn.execute("""
            INSERT INTO uncertainty_calculations 
            (calculation_id, operation_id, tool_id, research_question, 
             ic_methodologies, stage_confidences, final_confidence, 
             calculation_method, calculation_steps, assumptions, 
             sensitivity_analysis, human_readable_explanation,
             processing_time_seconds, api_calls_made)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            calculation_id,
            operation_id,
            tool_id,
            research_question,
            json.dumps(["key_assumptions", "competing_hypotheses", "diagnostic_evidence"]),
            json.dumps(ic_result.stage_confidences),
            ic_result.final_confidence,
            ic_result.propagation_method,
            json.dumps(ic_result.calculation_trace.get("calculation_steps", [])),
            json.dumps(ic_result.calculation_trace.get("assumptions", [])),
            json.dumps(ic_result.calculation_trace.get("sensitivity_analysis", {})),
            ic_result.calculation_trace.get("human_readable", ""),
            ic_result.processing_time,
            ic_result.api_calls_made
        ))
        
        # Insert IC analysis components
        components = [
            ("key_assumptions", ic_result.key_assumptions),
            ("competing_hypotheses", ic_result.competing_hypotheses),
            ("diagnostic_evidence", ic_result.diagnostic_evidence),
            ("information_gaps", ic_result.information_gaps),
            ("cognitive_biases", ic_result.cognitive_biases)
        ]
        
        for component_type, component_data in components:
            self.conn.execute("""
                INSERT INTO ic_analysis_components 
                (component_id, calculation_id, component_type, component_data)
                VALUES (?, ?, ?, ?)
            """, (
                str(uuid.uuid4()),
                calculation_id,
                component_type,
                json.dumps(component_data)
            ))
        
        self.conn.commit()
        return calculation_id
    
    def get_uncertainty_calculation_history(self, tool_id: str = None) -> List[Dict]:
        """Get uncertainty calculation history with full audit trails"""
        
        query = """
            SELECT uc.*, GROUP_CONCAT(iac.component_type) as components
            FROM uncertainty_calculations uc
            LEFT JOIN ic_analysis_components iac ON uc.calculation_id = iac.calculation_id
        """
        
        params = []
        if tool_id:
            query += " WHERE uc.tool_id = ?"
            params.append(tool_id)
        
        query += " GROUP BY uc.calculation_id ORDER BY uc.created_at DESC"
        
        cursor = self.conn.execute(query, params)
        return [dict(row) for row in cursor.fetchall()]
    
    def trace_confidence_calculation(self, calculation_id: str) -> Dict[str, Any]:
        """Trace a specific confidence calculation with complete audit trail"""
        
        # Get main calculation
        cursor = self.conn.execute("""
            SELECT * FROM uncertainty_calculations WHERE calculation_id = ?
        """, (calculation_id,))
        
        calculation = dict(cursor.fetchone())
        
        # Get IC components
        cursor = self.conn.execute("""
            SELECT * FROM ic_analysis_components WHERE calculation_id = ?
        """, (calculation_id,))
        
        components = [dict(row) for row in cursor.fetchall()]
        
        return {
            "calculation": calculation,
            "components": components,
            "audit_trail": {
                "calculation_steps": json.loads(calculation.get("calculation_steps", "[]")),
                "assumptions": json.loads(calculation.get("assumptions", "[]")),
                "sensitivity_analysis": json.loads(calculation.get("sensitivity_analysis", "{}")),
                "human_readable": calculation.get("human_readable_explanation", "")
            }
        }


# W3C PROV Compliance Extension
class W3CProvUncertaintyExtension:
    """W3C PROV-compliant uncertainty provenance tracking"""
    
    def __init__(self, provenance_service: ProvenanceService):
        self.provenance_service = provenance_service
    
    def create_uncertainty_provenance_record(self, ic_result: ICAnalysisResult, 
                                           context: Dict) -> Dict[str, Any]:
        """Create W3C PROV-compliant uncertainty provenance"""
        
        timestamp = datetime.now().isoformat()
        
        return {
            "@context": "https://www.w3.org/ns/prov",
            "@type": "Activity",
            "id": f"uncertainty_calculation_{timestamp}",
            "type": "ic_informed_uncertainty_analysis",
            "startedAtTime": timestamp,
            "endedAtTime": timestamp,
            
            # Activity details
            "used": [
                {
                    "@type": "Entity",
                    "id": "research_question",
                    "value": context.get("research_question", "")
                },
                {
                    "@type": "Entity", 
                    "id": "evidence_collection",
                    "count": len(context.get("evidence", []))
                }
            ],
            
            # Generated entities
            "generated": [
                {
                    "@type": "Entity",
                    "id": "final_confidence_score",
                    "value": ic_result.final_confidence,
                    "method": ic_result.propagation_method
                },
                {
                    "@type": "Entity",
                    "id": "ic_probability_assessment",
                    "probability_band": ic_result.probability_band,
                    "confidence_level": ic_result.confidence_level
                }
            ],
            
            # Agent (service) that performed the activity
            "wasAssociatedWith": {
                "@type": "Agent",
                "id": "ic_uncertainty_service",
                "type": "software_service",
                "version": "1.0.0"
            },
            
            # Derivation relationships
            "wasDerivedFrom": [
                {
                    "@type": "Derivation",
                    "entity": "final_confidence_score",
                    "activity": "mathematical_propagation",
                    "type": "root_sum_squares"
                },
                {
                    "@type": "Derivation",
                    "entity": "ic_assessment",
                    "activity": "single_integrated_llm_analysis",
                    "methodologies": ["ACH", "key_assumptions_check", "diagnostic_evidence"]
                }
            ],
            
            # Custom uncertainty extensions
            "uncertainty:calculationTrace": ic_result.calculation_trace,
            "uncertainty:auditTrail": {
                "calculation_steps": ic_result.calculation_trace.get("calculation_steps", []),
                "assumptions": ic_result.calculation_trace.get("assumptions", []),
                "sensitivity": ic_result.calculation_trace.get("sensitivity_analysis", {})
            }
        }
```

## 4. Performance Optimization Strategy

### Caching and Batching

```python
# src/services/ic_uncertainty_service.py - Performance optimizations

class OptimizedICUncertaintyService(ICUncertaintyService):
    """Performance-optimized IC Uncertainty Service"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # Analysis cache with TTL
        self.analysis_cache = {}
        self.cache_ttl = 3600  # 1 hour TTL
        
        # Batch processing queue
        self.batch_queue = []
        self.batch_size = 5
        self.batch_timeout = 30  # seconds
        
        # Performance monitoring
        self.performance_monitor = {
            "cache_hits": 0,
            "cache_misses": 0,
            "batch_processed": 0,
            "average_processing_time": 0.0
        }
    
    async def analyze_with_caching(self, request: ICAnalysisRequest) -> ICAnalysisResult:
        """Analysis with intelligent caching"""
        
        # Create cache key
        cache_key = self._create_cache_key(request)
        
        # Check cache first
        if cache_key in self.analysis_cache:
            cached_entry = self.analysis_cache[cache_key]
            if self._is_cache_valid(cached_entry):
                self.performance_monitor["cache_hits"] += 1
                return cached_entry["result"]
        
        # Cache miss - perform analysis
        self.performance_monitor["cache_misses"] += 1
        result = await self.analyze_with_ic_methods(request)
        
        # Cache result
        self.analysis_cache[cache_key] = {
            "result": result,
            "timestamp": datetime.now(),
            "ttl": self.cache_ttl
        }
        
        return result
    
    async def batch_analyze(self, requests: List[ICAnalysisRequest]) -> List[ICAnalysisResult]:
        """Batch processing for multiple requests"""
        
        # Group requests by similarity for shared LLM calls
        grouped_requests = self._group_similar_requests(requests)
        
        results = []
        for group in grouped_requests:
            # Process group with shared context
            batch_result = await self._process_request_batch(group)
            results.extend(batch_result)
        
        self.performance_monitor["batch_processed"] += len(requests)
        return results
    
    def _create_cache_key(self, request: ICAnalysisRequest) -> str:
        """Create cache key for request"""
        # Hash key components for caching
        key_components = [
            request.research_question,
            request.domain,
            str(len(request.evidence)),
            json.dumps(sorted(request.context.items()) if request.context else {})
        ]
        
        import hashlib
        key_string = "|".join(key_components)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _is_cache_valid(self, cached_entry: Dict) -> bool:
        """Check if cached entry is still valid"""
        age = (datetime.now() - cached_entry["timestamp"]).total_seconds()
        return age < cached_entry["ttl"]
    
    def _group_similar_requests(self, requests: List[ICAnalysisRequest]) -> List[List[ICAnalysisRequest]]:
        """Group similar requests for batch processing"""
        groups = []
        
        # Simple grouping by domain and question similarity
        domain_groups = {}
        for request in requests:
            domain = request.domain
            if domain not in domain_groups:
                domain_groups[domain] = []
            domain_groups[domain].append(request)
        
        # Further group by question similarity within domains
        for domain, domain_requests in domain_groups.items():
            # For now, simple batching by domain
            # Could add semantic similarity grouping here
            groups.append(domain_requests)
        
        return groups
    
    async def _process_request_batch(self, requests: List[ICAnalysisRequest]) -> List[ICAnalysisResult]:
        """Process a batch of similar requests efficiently"""
        
        # Combine evidence for shared analysis
        combined_evidence = []
        for request in requests:
            combined_evidence.extend(request.evidence)
        
        # Single LLM call for batch context
        batch_context = {
            "requests": len(requests),
            "total_evidence": len(combined_evidence),
            "domains": list(set(r.domain for r in requests))
        }
        
        results = []
        for request in requests:
            # Use shared context for efficiency
            result = await self.analyze_with_ic_methods(request)
            results.append(result)
        
        return results


# Integration with Pipeline Orchestrator
class ICUncertaintyOrchestrator:
    """Orchestrate IC uncertainty across pipeline stages"""
    
    def __init__(self, ic_service: OptimizedICUncertaintyService):
        self.ic_service = ic_service
        self.stage_uncertainties = {}
    
    async def orchestrate_pipeline_uncertainty(self, 
                                             pipeline_stages: List[str],
                                             research_question: str) -> Dict[str, ICAnalysisResult]:
        """Orchestrate uncertainty assessment across pipeline stages"""
        
        stage_results = {}
        
        # Collect evidence from each stage
        for stage in pipeline_stages:
            stage_evidence = await self._collect_stage_evidence(stage)
            
            request = ICAnalysisRequest(
                research_question=f"Uncertainty assessment for {stage}: {research_question}",
                evidence=stage_evidence,
                context={"pipeline_stage": stage},
                domain="pipeline_analysis",
                tool_id=stage
            )
            
            result = await self.ic_service.analyze_with_caching(request)
            stage_results[stage] = result
        
        # Final integrated uncertainty assessment
        final_request = ICAnalysisRequest(
            research_question=research_question,
            evidence=self._combine_stage_evidence(stage_results),
            context={"pipeline_complete": True, "stages": pipeline_stages},
            domain="integrated_analysis"
        )
        
        final_result = await self.ic_service.analyze_with_ic_methods(final_request)
        stage_results["final_integrated"] = final_result
        
        return stage_results
    
    async def _collect_stage_evidence(self, stage: str) -> List[Dict]:
        """Collect evidence from pipeline stage"""
        # Implementation depends on stage type
        if stage == "T31_ENTITY_BUILDER":
            return await self._collect_entity_evidence()
        elif stage == "T34_EDGE_BUILDER":
            return await self._collect_relationship_evidence()
        elif stage == "T49_QUERY":
            return await self._collect_query_evidence()
        else:
            return [{"stage": stage, "evidence": "generic_evidence"}]
    
    def _combine_stage_evidence(self, stage_results: Dict[str, ICAnalysisResult]) -> List[Dict]:
        """Combine evidence from multiple stages"""
        combined = []
        for stage, result in stage_results.items():
            if stage != "final_integrated":
                combined.append({
                    "stage": stage,
                    "confidence": result.final_confidence,
                    "key_assumptions": result.key_assumptions[:2],  # Top 2
                    "information_gaps": result.information_gaps[:2]   # Top 2
                })
        return combined
    
    async def _collect_entity_evidence(self) -> List[Dict]:
        """Collect evidence for entity building stage"""
        return [
            {"evidence_type": "entity_extraction", "quality": "high"},
            {"evidence_type": "entity_deduplication", "quality": "moderate"}
        ]
    
    async def _collect_relationship_evidence(self) -> List[Dict]:
        """Collect evidence for relationship building stage"""
        return [
            {"evidence_type": "relationship_extraction", "quality": "moderate"},
            {"evidence_type": "relationship_validation", "quality": "high"}
        ]
    
    async def _collect_query_evidence(self) -> List[Dict]:
        """Collect evidence for query processing stage"""
        return [
            {"evidence_type": "entity_matching", "quality": "high"},
            {"evidence_type": "graph_traversal", "quality": "high"}
        ]
```

## 5. Implementation Roadmap

### Phase 1: Core Service Integration (Week 1-2)

1. **Extract Experimental Code**
   ```bash
   # Copy experimental uncertainty engine to services
   cp -r experiments/uncertainty_stress_test_system/core_services/uncertainty_engine.py src/services/
   cp -r experiments/uncertainty_stress_test_system/bayesian/ src/services/uncertainty_components/
   ```

2. **Create IC Uncertainty Service**
   - Implement `ICUncertaintyService` class
   - Integrate with `ServiceManager`
   - Add to `EnhancedServiceManager`

3. **Test Service Integration**
   ```python
   # Test basic service integration
   from src.core.service_manager import ServiceManager
   sm = ServiceManager()
   ic_service = sm.get_ic_uncertainty_service()
   print("IC Service initialized:", ic_service is not None)
   ```

### Phase 2: Provenance Extension (Week 2-3)

1. **Extend Provenance Database Schema**
   - Add `uncertainty_calculations` table
   - Add `ic_analysis_components` table
   - Update `ProvenanceService` methods

2. **Implement Calculation Audit Trails**
   - Add uncertainty provenance tracking
   - Implement W3C PROV compliance
   - Create audit trail queries

3. **Test Provenance Integration**
   ```python
   # Test uncertainty provenance
   ic_result = await ic_service.analyze_with_ic_methods(request)
   calc_id = provenance_service.record_uncertainty_calculation(ic_result, op_id, "T31", "test question")
   audit_trail = provenance_service.trace_confidence_calculation(calc_id)
   print("Audit trail available:", len(audit_trail["audit_trail"]["calculation_steps"]))
   ```

### Phase 3: Pipeline Integration (Week 3-4)

1. **Create IC Uncertainty Mixin**
   - Implement `ICUncertaintyMixin` for tools
   - Update `ToolResult` with IC fields
   - Add research guidance methods

2. **Integrate with Critical Tools**
   - T31 (Entity Builder) - entity confidence
   - T34 (Edge Builder) - relationship confidence  
   - T49 (Query Tool) - query result confidence
   - T68 (PageRank) - graph analysis confidence

3. **Test Pipeline Integration**
   ```python
   # Test enhanced tool with IC uncertainty
   enhanced_t31 = T31EntityBuilderWithIC(service_manager)
   result = enhanced_t31.execute(request)
   print("IC Analysis:", result.ic_uncertainty is not None)
   print("Research Guidance:", result.research_guidance)
   ```

### Phase 4: Performance Optimization (Week 4-5)

1. **Implement Caching Layer**
   - Add analysis result caching
   - Implement cache TTL and invalidation
   - Add batch processing capabilities

2. **Optimize LLM Usage**
   - Batch similar requests
   - Cache common analyses
   - Implement request deduplication

3. **Test Performance**
   ```bash
   # Performance testing
   python -c "
   import asyncio
   from src.services.ic_uncertainty_service import OptimizedICUncertaintyService
   # Run performance benchmarks
   "
   ```

### Phase 5: Production Deployment (Week 5-6)

1. **Integration Testing**
   - End-to-end pipeline tests with IC uncertainty
   - Performance regression testing
   - Provenance integrity validation

2. **Documentation and Training**
   - Update architecture documentation
   - Create user guides for IC-informed results
   - Document research decision guidance

3. **Deployment and Monitoring**
   - Deploy to production environment
   - Monitor performance metrics
   - Validate uncertainty calculations

---

## Expected Benefits

### Immediate Benefits
- **Calculation Transparency**: Complete audit trails for all confidence scores
- **Research Guidance**: Clear guidance on appropriate research methods
- **Academic Rigor**: IC-proven methodologies for uncertainty assessment

### Long-term Benefits  
- **Publication Quality**: Academic-standard uncertainty reporting
- **Bias Mitigation**: Structured detection and mitigation of cognitive biases
- **Research Reproducibility**: Complete provenance and calculation transparency

### Performance Expectations
- **Caching**: 60-80% cache hit rate for similar analyses
- **Processing Time**: <5 seconds for typical IC analysis
- **Memory Usage**: <100MB additional overhead for uncertainty service

This integration plan leverages your existing sophisticated infrastructure while adding the proven IC-informed uncertainty capabilities from your experimental code. The result will be a production-ready system with academic-quality uncertainty assessment and complete calculation transparency.