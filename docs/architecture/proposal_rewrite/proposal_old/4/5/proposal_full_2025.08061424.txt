# Chapter 1: Introduction and Research Context

## 1.1 Scaling Theory-Aware Computational Social Science

This dissertation explores how Large Language Models (LLMs) can be leveraged in the analysis online discourse within the field of computational social science through a theory-first approach. Unlike existing systems that apply computational methods and then interpret results through theoretical lenses, this research proposes that theories should guide the analytical process from the outset, determining how data flows between different representational formats and what patterns to seek. Many current social science methods require human labor for theory application, limiting analysis to small datasets or single theoretical perspectives. This research intends to show that theory-aware computational social science is possible by establishing LLM performance baselines. While current system performance may not meet real-world operational demands, this work establishes foundations for more capable language models that will enable more sophisticated and accurate theory application at scale.

This research will test whether LLMs can automate theory application through systematic operationalization of social science theories, scaling these methods to analyze interactions at scale while maintaining theoretical rigor. This approach lets analysts describe patterns, explain what causes them, predict what will happen next, and guide interventions. The implications of this work for policy analysis, among other applied fields, are significant and will be discussed in Chapter 2, which examines how theory-aware computational methods can address practical analytical challenges.

This dissertation builds directly upon prior academic work. The project's intellectual origins began with a tutorial on conspiracy theories with Dr. Eric Larson in 2021. The research was then developed into the current proposal during an independent study, also with Dr. Larson, in 2024. Since the conclusion of the independent study, we have been meeting weekly to refine the theoretical framework and system architecture, ensuring the research maintains both academic rigor and policy relevance.

## 1.2 System Architecture and Capabilities

The Knowledge Graph Analysis System (KGAS) will operationalize this theory-first vision through a low-code (code only involved in the initial installation, setup, and startup) tool that extracts theories from academic papers and converts them into computer instructions that analyze data. Unlike data-driven systems that find patterns first, KGAS starts with a theory to guide the analysis and data flow.

The system will provide traceability linking results back to theoretical foundations, interpretability through explanations at each analytical step, uncertainty estimation and propagation through traceable methods, and reproducibility within the constraints of LLM non-determinism. The theory-first architecture synthesizes perspectives across disciplines without requiring users to be experts in each field, automating a process that once took teams of analysts months to complete.

## 1.3 Demonstration: COVID Conspiracy Discourse

The Kunst et al. (2024) dataset provides a testbed by combining 7.7 million tweets from 2,506 users with psychological assessments - including conspiracy mentality, collective narcissism, need for chaos, and misinformation susceptibility scales. This combination of user data and psychological profiles lets us test if our methods can find online patterns that match specific psychological traits.

The COVID dataset's scale, psychological validation, temporal depth, and policy relevance make it appropriate for demonstration. This proposal will use Social Identity Theory as a consistent example to demonstrate a complete analytical workflow from theory selection through validation, while also introducing other theoretical frameworks to highlight the system's flexibility and theory-agnostic design.

## 1.4 KGAS in Action: A Policy Analyst's Workflow

Consider a policy analyst investigating vaccine hesitancy narratives. Using the proposed conversational interface, they type: "Why do vaccine hesitant groups reject mainstream health information?". The system identifies Social Identity Theory as a relevant framework because it explains the in-group/out-group dynamics and rejection of outside information that are central to the analyst's question. The user could accept this recommendation or select a different theory. The conversational interface would then work with the user to specify the specific methodology to apply the theory to the data. In this example, Social Identity Theory is selected to demonstrate the complete analytical workflow:

Figure 1.1. Theory Operationalization
```
    User Query: "Why do vaccine hesitant groups 
         reject mainstream health information?"
                      ↓
            Theory Selection:
      Social Identity Theory (SIT)
                      ↓
       SIT Theoretical Requirements:
    ┌──────────────────────────────────────────────┐
    │ 1. Identify group boundaries (in/out-group) │
    │ 2. Measure identity strength for each group │
    │ 3. Assess intergroup bias in information    │
    │ 4. Test: Identity → Bias → Rejection        │
    └──────────────────────────────────────────────┘
                      ↓
    Theory Specifies Cross-Modal Data Flow:
    Graph(communities) → Table(psychology) → Vector(language)
                      ↓
              Hypothesis Testing:
    "High-identity groups will show greater semantic
     distance from out-groups and reject their information"
```
NOTE: Social Identity Theory doesn't just select tools - it specifies how data must flow between modalities to test theoretical predictions. The theory requires identifying groups (graph), measuring their psychological traits (table), and analyzing their language (vector).

Figure 1.2. Theory-Guided Cross-Modal Data Flow
```
    GRAPH MODALITY: Community Detection
    [Input: 7.7M tweets from 2,506 users]
                      ↓
    Output: Community A (512 users: ID001, ID047, ...)
            Community B (887 users: ID023, ID091, ...)
                      ↓
    ┌───────────────────────────────────────────┐
    │ User IDs from communities flow to Table    │
    └───────────────────────────────────────────┘
                      ↓
    TABLE MODALITY: Psychological Analysis
    [Input: User IDs from graph communities]
                      ↓
    Finding: Community A: M=5.8 conspiracy mentality
             Community B: M=2.1 conspiracy mentality
    High-identity users: {ID047, ID091, ID238...}
                      ↓
    ┌───────────────────────────────────────────┐
    │ High-identity user IDs flow to Vector      │
    └───────────────────────────────────────────┘
                      ↓
    VECTOR MODALITY: Language Analysis
    [Input: Tweets from high-identity users]
                      ↓
    Finding: Semantic distance between communities = 0.84
             Network distance × Semantic distance: r=0.67
                      ↓
    THEORY VALIDATION:
    SIT Hypothesis Confirmed: Groups with high identity
    (M>5.0) + high network distance (>3 hops) show 3.2x
    higher rejection of out-group health information
```
NOTE: In this example using Social Identity Theory, data flows from graph to table to vector based on the theory's requirements. Each modality's output becomes the next modality's input, testing specific theoretical predictions about how group identity drives information rejection. Different theories would specify different data flow patterns.

In this example workflow, Social Identity Theory guides the cross-modal data flow: graph analysis would first identify communities, passing those user IDs to table analysis for psychological profiling, which would identify high-identity users whose discourse would flow to vector analysis for semantic patterns. The theory specifies that data from graph communities must flow to psychological tables to identify high-identity users whose language should then show distinctive patterns. Different theories would specify different data flows - for example, Network Contagion Theory might trace information cascades through the graph then analyze adopter characteristics, while Motivated Reasoning might start with psychological profiles then trace their information consumption patterns. This example workflow is expanded throughout the proposal to demonstrate framework selection (Chapter 3), system architecture (Chapter 4), and validation approaches (Chapter 8).

## 1.5 Research Scope and Approach

The proposed system will test application of multiple theories to the COVID dataset through cross-modal analysis. The scope will focus on online discourse as a bounded context, testing whether communication patterns can predict behavioral outcomes while acknowledging the complexity of inferring offline actions from online discourse. The research proceeds through three integrated essays: Essay 1 establishes the theoretical framework and technical specifications for what system to build, integrating both theoretical foundations and computational methods; Essay 2 will describe the KGAS implementation, architecture, and validation; Essay 3 will demonstrate the system's application to the COVID fringe discourse dataset, showcasing analytical capabilities including agent-based modeling (ABM) and structural equation modeling (SEM).

## 1.6 Contributions to Policy Analysis and Computational Social Science

This research lays the groundwork for theory-aware computational social science. The Theory Meta-Schema (Annex A) will bridge academic theories and computational automation. 

Technical contributions include automated theory extraction from academic literature (using LLMs to identify and formalize theoretical components from papers), uncertainty propagation across modalities, and adaptation of Intelligence Community standards for academic research (Office of the Director of National Intelligence, 2007; 2012). We have developed prototype plugins for qualitative coding and process tracing, which demonstrated feasibility. Integrating them into the full system is a key part of this proposal. The system will be validated using the COVID dataset (see Chapter 8), though it will work with any theoretical domain. (Human subjects protection considerations detailed in Annex D)

# Chapter 2: Research Questions and Contributions

## 2.1 Primary Research Questions

Where Chapter 1 established the policy need for scalable theory application, this chapter articulates the research questions driving the dissertation.

The first research question will examine automated theory operationalization through a theory-first approach: How can computational systems automate the operationalization and application of social science theories for large-scale discourse analysis? This question addresses the current fragmentation where researchers apply familiar theories without systematic consideration of alternatives, and where computational tools apply data-driven methods that require post-hoc theoretical interpretation. The theory-first architecture will test automated extraction of theories from academic literature using the Theory Meta-Schema and conversion into computational specifications that determine analytical workflows. The system will generate dynamic workflows (DAGs) based on theoretical needs instead of using generic data processing templates. The framework will organize theories based on established communication principles and analytical goals (description, explanation, prediction, and intervention).

The second research question will explore multi-theoretical analysis: What complementary insights emerge when the same discourse is analyzed through different theoretical lenses (such as Social Identity Theory, Network Contagion Theory, or Terror Management Theory) using cross-modal analysis? The research will document how theoretical lens selection shapes both the data flow patterns and the insights generated.

The third research question will address practical impact: To what extent does theory-aware workflow composition improve the detection and understanding of policy-relevant patterns in fringe discourse? The evaluation will focus on whether theory-guided analysis will reveal patterns that single-method approaches miss.

## 2.2 Positioning Relative to Existing Approaches

Current computational methods for discourse analysis are limited in their flexibility and theoretical foundation. Standard RAG and GraphRAG systems operate through data-driven pipelines without the ability to dynamically chain tools or adapt workflows based on theoretical requirements. These systems optimize for information retrieval and pattern discovery, then require human analysts to interpret findings through theoretical frameworks post-hoc (detailed comparison in Section 4.1).

KGAS's theory-first architecture inverts this relationship by using the cross-modal data flow described in Chapter 1. Unlike existing data-driven systems that seek patterns and then apply theoretical interpretations, KGAS starts with theories that specify which patterns to look for and how to analyze them. This approach uses a theory to guide the analytical process, with each theory specifying its own data flow pattern (graph, table, and vector) to operationalize its concepts.

## 2.3 Policy and Academic Contributions

For policy practitioners, the system will automate workflows that previously took months of manual analysis. The multi-level analytical approach will enable testing how different intervention strategies might affect discourse dynamics at individual, group, and population levels.

The methodological contribution will be a framework for computational theory application that builds on Larson et al. (2009) and integrates Lasswell's (1948) and Druckman's (2022) communication models. This dissertation will apply their multi-level approach using computational methods.

The Theory Meta-Schema (introduced in Section 1.6) will provide a standardized format for computational theory representation (full specification in Annex A).

The system architecture enables cross-modal analysis following Druckman's (2022) framework.

## 2.4 Validation Approach

The system will be validated through assessment of its theoretical and computational capabilities (detailed methodology in Chapter 8, framework in Annex C). The validation will focus on whether the system can extract and apply theories correctly, not whether the theories themselves are true. Multiple validation methods will be employed based on data availability and dimension requirements.

## 2.5 Scope and Limitations

The research will focus on fringe discourse for demonstration, analyzing discourse patterns and their theoretical implications. Theory discovery and generation remain future work. The research acknowledges limitations in predicting offline behavior from online discourse.


# Chapter 3: Theoretical Framework

Where Chapter 2 identified the research questions, this chapter develops the theoretical framework that will guide their investigation.

## 3.1 Systematic Theory Selection Framework

Computational social science needs a systematic way to match research questions with theories based on their analytical capabilities, not just disciplinary familiarity. This dissertation proposes a framework that maps research questions across analytical goals (Describe, Explain, Predict, Intervene), analysis levels (Individual, Group, Mass Public), and discourse elements (Who, What, Whom, Channel, Settings, Effect) to enable precision in theory selection.

For example, analyzing COVID discourse with the research question "Why did vaccination become a group identity marker?" would be mapped as: Goal = Explain (causal mechanisms), Level = Group (collective dynamics), Focus = Who/Effect (identity formation/polarization). This mapping reveals options like Social Identity Theory, Terror Management Theory, and Network Contagion Theory. The framework enables researchers to select theories based on precise analytical requirements rather than approximate disciplinary fits.

The proposed framework organizes theories by their analytical capabilities using Lasswell's (1948) communication structure - who says what to whom in what channel in what settings with what effect - combined with analytical goals, creating a structured space for systematic theory selection and application.

## 3.2 DEPI×Level×Component Framework

The framework organizes theories along three dimensions that capture their analytical capabilities and scope. This three-dimensional space enables matching between research questions and theoretical frameworks.

Figure 3.1: Three-Dimensional Theory Organization Framework

```
    Analytical Goals         Levels              Components
    ┌──────────────┐      ┌──────────┐       ┌──────────┐
    │ • Describe   │      │• Individual│      │ • Who    │
    │ • Explain    │  ×   │• Group     │   ×  │ • What   │
    │ • Predict    │      │• Mass Public│     │ • To Whom│
    │ • Intervene  │      └──────────┘       │ • Channel│
    └──────────────┘                          │ • Settings│
                                              │ • Effect │
                                              └──────────┘
                              ↓
          72 potential analytical configurations
```
NOTE: This framework organizes theories by their analytical capabilities rather than disciplinary origins, enabling systematic theory selection based on research needs.

The first dimension uses the DEPI taxonomy: Describe, Explain, Predict, and Intervene. The second dimension specifies levels of analysis (Individual, Group, Mass Public). The third dimension identifies communication components from Lasswell/Druckman (Who, What, To Whom, Channel, Settings, Effect).

## 3.3 Theory Meta-Schema Architecture

The Theory Meta-Schema will operate through a three-level architecture that enables systematic theory application to specific datasets while preserving theoretical fidelity:

**Theory Meta-Schema (Level 1)** defines the universal template structure for representing any social science theory. This template has standardized components like metadata (theory name, authors, and other theories it builds on), theoretical structure (entities, relations, and modifiers), computational representation (data formats), and computational patterns (methods the theory uses). This template ensures consistent theory representation across diverse theoretical frameworks.

**Theory Schema (Level 2)** instantiates specific theories using abstract theoretical concepts. For example, Social Identity Theory contains abstract entities such as "in-group" and "out-group," abstract relations such as "exhibits favoritism toward" and "competes with," and abstract modifiers such as "under intergroup threat conditions." This level preserves the theory's original terminology and conceptual structure without reference to specific empirical contexts.

**Theory-Text Schema (Level 3)** applies the abstract theory to specific data contexts, mapping abstract concepts to concrete instances. When applied to COVID discourse, "in-group" becomes "vaccine hesitant groups," "out-group" becomes "pro-vaccine groups," and the relation "exhibits favoritism toward" becomes "endorses vaccine hesitant posts." This mapping enables computational analysis while maintaining a clear link to the theory.

This architecture allows for systematic theory application by providing structure (Meta-Schema), preserving concepts (Theory Schema), and enabling analysis (Theory-Text Schema). The system maintains provenance across all levels, ensuring results can be traced from specific findings back to abstract theoretical principles. (See Annex A for full specification with Social Identity Theory example) 

## 3.4 Computational Implementation

The Theory Meta-Schema will encode theories using seven methodological patterns that theories prescribe for data analysis:

- **EXPRESSIONS**: Direct mathematical calculations (e.g., Social Identity Theory's in-group favoritism = (in-group_rating - out-group_rating) / scale_range)
- **ALGORITHMS**: Iterative computational methods (e.g., Social Identity Theory's community detection to identify group boundaries through modularity optimization)
- **PROCEDURES**: Branching analytical workflows (e.g., Social Identity Theory's threat assessment: if intergroup competition detected, assess resource scarcity; if resource scarcity high, examine zero-sum beliefs; iterate until threat level determined)
- **SEQUENCES**: Linear analytical stages (e.g., Social Identity Theory's identity formation analysis: categorization → identification → comparison → positive distinctiveness)
- **RULES**: Classification logic (e.g., Social Identity Theory: IF shared_group_membership AND out-group_threat > threshold THEN classify as in-group_bias_context)
- **STATISTICAL_MODELS**: Relationship testing in data (e.g., Social Identity Theory SEM: group_identification → collective_self_esteem → in-group_favoritism with mediation analysis)
- **SIMULATIONS**: Agent-based pattern generation (e.g., Social Identity Theory ABM: agents dynamically form groups, develop identification levels, and exhibit bias based on threat, generating polarization patterns)

Each pattern represents a specific operation a theory requires. EXPRESSIONS calculate metrics directly. ALGORITHMS iterate to convergence. PROCEDURES branch through decision trees. SEQUENCES progress through ordered stages. RULES classify based on conditions. STATISTICAL_MODELS test theoretical relationships. SIMULATIONS generate emergent patterns from theoretical rules. This comprehensive categorization enables KGAS to implement the full range of methodological operations social science theories prescribe.

## 3.5 Cross-Modal Data Representation

The framework allows analysis across graph, table, and vector representations, with theories specifying the data formats and flow patterns (as shown in Chapter 1). The system converts data between formats without losing key information and tracks the data back to its source.

## 3.6 Theory Selection and Application Process

The framework enables two approaches to theory selection and application, supporting different research needs and expertise levels.

**Guided Theory Selection** combines automated matching with interactive exploration to identify appropriate theories. The system parses research questions to identify key elements - actors, behaviors, outcomes, and scope - which map to the DEPI×Level×Component framework to determine analytical requirements. When automated matching requires clarification, the system engages users through conversational interface, asking about analytical goals, phenomenon scope, and desired insights. The Master Concept Library provides vocabulary for matching question concepts to theoretical constructs, while vector embeddings enable semantic similarity matching when exact terminology differs. Based on this analysis, the system presents relevant theoretical options organized by analytical capability, allowing researchers to explore trade-offs between different theoretical perspectives. This approach supports both experienced researchers seeking confirmation of theory choices and those less familiar with the full range of available theories.

Users can either select from existing theories in the system or upload documents containing theories or methodologies they wish to apply. When uploading new theoretical content, the system extracts the theoretical framework using the Theory Meta-Schema format, enabling immediate integration into the analytical workflow.

**Manual Browsing** presents theories organized by analytical purpose rather than disciplinary boundaries. Researchers can browse theories that describe patterns, explain mechanisms, predict outcomes, or design interventions. Within each category, theories are grouped by the level of analysis and communication components they address. This organization reveals theoretical options that disciplinary training might overlook.

The framework focuses on theories with operationalizable methods - those that can be translated into computational procedures for data analysis. Theories that can't be systematically applied through computational methods are outside the system's scope, though this limitation aligns with policy analysis requirements where theoretical insights must translate into actionable analytical approaches. Most theories span multiple analytical categories (such as Social Identity Theory, which can describe group formation, explain polarization mechanisms, and predict response to interventions), so the framework's categorization supports rather than constrains theoretical flexibility.


# Chapter 4: System Architecture

Where Chapter 3 established the theoretical framework for organizing and applying theories, this chapter describes the computational architecture that operationalizes that framework.

## 4.1 Positioning Relative to Existing Systems

Current approaches to computational discourse analysis face limitations in handling theoretical complexity and analytical diversity. Older methods like SpaCy and traditional social network analysis tools offer specific capabilities but need manual integration to apply theories. These tools extract entities or calculate network metrics without understanding the significance of what they measure. Researchers must manually interpret outputs through theoretical lenses, creating a workflow where computational tools and theoretical analysis remain disconnected. In essence, these tools and methods are data-driven, not theory-driven.

Standard Retrieval-Augmented Generation (RAG) systems retrieve text chunks based on semantic similarity, treating documents as collections of passages. This approach cannot represent the causal relationships in social science theories (such as Social Identity Theory's chain) from threat through categorization to bias. While RAG improves LLM accuracy through external knowledge, it focuses on information retrieval rather than theoretical application.

Microsoft GraphRAG and similar systems like DIGIMON (with its 16 atomic operators for graph retrieval) improve on flat retrieval by extracting entities and relationships to build graph structures. GraphRAG specifically uses hierarchical community summaries and LLM-based graph construction to capture document structure, while DIGIMON provides modular operators for graph traversal and subgraph extraction. However, these systems create a single, data-driven representation based on what appears in the text. They excel at graph-based retrieval optimization but lack the capability to apply different theoretical lenses to the same data. 

StructGPT demonstrates reasoning over structured data through its Iterative Reading-then-Reasoning (IRR) framework, using interfaces to query databases and having the LLM reason over results. Google's Concordia framework and Park et al.'s (2023) generative agents show promise for agent-based simulation but require manual behavioral rule specification. These limitations are problematic when analyzing polarized discourse where echo chamber effects require different theoretical frameworks to understand information diffusion versus attitude formation.

KGAS differs by using its theory-first architecture to generate different representations based on the selected theory. Unlike other systems designed for current applications, KGAS will be designed as infrastructure for future autonomous research agents. The system's theory-driven workflow differs from data-driven approaches:

Figure 4.2: Comparison of Standard RAG and KGAS Theory-First Workflows

```
    Standard RAG/GraphRAG:
    Query → Retrieve → Generate → Response
    
    KGAS Theory-First:
    Research Question
           ↓
    Theory Selection
           ↓
    Theory Extraction ←─── Academic Literature
           ↓
    Multi-Modal Analysis
    ┌──────┴──────┬──────────┐
    Graph      Table      Vector
    Analysis   Analysis   Analysis
    └──────┬──────┴──────────┘
           ↓
    Integrated Results with Uncertainty
```
NOTE: Unlike data-driven retrieval systems, KGAS uses theoretical frameworks to guide all analytical operations from extraction through integration. The theory-first approach ensures that analysis is grounded in established social science knowledge rather than emergent patterns alone. 

For example, when analyzing COVID discourse through Social Identity Theory, the system would construct graphs with in-group and out-group nodes connected by identification edges, then flow identified communities to table analysis for psychological profiling, then high-identity users to vector analysis for language patterns. The same discourse analyzed through Network Contagion Theory would generate individual nodes with influence pathway edges, flowing through different analytical paths based on that theory's requirements.

Table 4.1: Computational Discourse Analysis System Capabilities

| Capability                  | Pre-LLM Tools | Standard RAG | GraphRAG/DIGIMON | KGAS      |
|                            | (SpaCy, SNA)  |              |                  |           |
|----------------------------|---------------|--------------|------------------|-----------|
| Entity Extraction          | ✓             | ✓            | ✓                | ✓         |
| Theory-Aware Processing    | ✗             | ✗            | ✗                | ✓         |
| Cross-Modal Analysis       | ✗             | ✗            | Partial          | ✓         |
| Statistical Integration    | Manual Only   | ✗            | ✗                | ✓         |
| ABM Integration           | ✗             | ✗            | ✗                | ✓         |
| Uncertainty Quantification | ✗             | Basic        | Basic            | IC-Std    |


The system's scale is a key difference from existing approaches. While DIGIMON offers 16 operators and StructGPT provides 8 interfaces, KGAS integrates tools from multiple categories, with suites for document processing, graph operations, statistical analysis (like SEM), vector operations, and cross-modal converters. This scale enables capabilities absent in other systems: automated theory operationalization from academic literature, agent-based model integration for counterfactual analysis, and complete statistical analysis producing APA-formatted outputs.

## 4.2 Cross-Modal Analysis Architecture

The system will implement the cross-modal analysis approach described in Chapter 1, differing from standard RAG systems through theory-guided data flow between analytical representations rather than parallel processing of different data types.


## 4.3 Provenance and Uncertainty Infrastructure

The system will implement W3C PROV-compliant provenance tracking (recording the complete history of data transformations and analytical decisions) with uncertainty audit trails. Every operation records not just its inputs and outputs, but a trail of decisions—like why a theory was chosen or how evidence was weighted. Transformation lineage tracks preservation across format conversions (graph↔table↔vector), maintaining bidirectional links from any finding to originating evidence. The provenance system will capture uncertainty provenance including calculation methods, input confidences, mathematical parameters, and confidence bounds on the uncertainty assessments themselves.

The IC-informed uncertainty framework adapts Intelligence Community methodologies for academic research, using probability bands with mathematical propagation through root-sum-squares.

## 4.4 Two-Layer Theory Architecture

The system will implement a two-layer architecture that separates theoretical structure extraction from application, addressing the problem of theory-question coupling in traditional approaches.

Figure 4.3: Two-Layer Theory Processing Architecture

```
    Layer 1: Theory Extraction (One-time)
    ┌─────────────────────────────────────┐
    │  Academic Literature                │
    │  ↓                                  │
    │  LLM Extraction with Meta-Schema    │
    │  ↓                                  │
    │  Theory Repository                  │
    └─────────────────────────────────────┘
                    ↓
    Layer 2: Question-Driven Application (Repeated)
    ┌─────────────────────────────────────┐
    │  Research Question                  │
    │  ↓                                  │
    │  Select Theory Components           │
    │  ↓                                  │
    │  Apply to Data                      │
    │  ↓                                  │
    │  Generate Results                   │
    └─────────────────────────────────────┘
```
NOTE: Separation of extraction from application enables reuse of theoretical structures across multiple research questions without re-extraction. This architecture solves the efficiency problem of traditional approaches that re-extract theories for each new analysis.

Layer 1 performs theoretical extraction independent of analytical goals. Using a 3-phase LLM extraction process with the Theory Meta-Schema, the system extracts theoretical structures - entities with original terminology, relationships with causal directions, assumptions and boundary conditions, mathematical formulations, and analytical methods. Multi-agent validation helps improve extraction consistency through consensus-based assessment. This extraction occurs once per theory, creating reusable theoretical representations.

Layer 2 enables question-driven analysis using pre-extracted theories. When researchers pose questions, the system selects relevant theoretical components from the extracted structures, applies appropriate analytical methods (formulas, algorithms, procedures, rules, sequences), and generates theory-grounded results. This separation enables capabilities: the same extraction supports multiple research questions, theories can be combined for multi-perspective analysis, and analytical approaches can evolve without re-extraction. The architecture ensures theoretical integrity and allows flexible application to analyze real-world phenomena, not just textual patterns.

## 4.5 Low-Code Interface and User Interaction

The system will provide a low-code interface that enables researchers to apply computational methods without programming expertise. The primary interaction occurs through a conversational chatbot interface that translates natural language requests into computational workflows. Advanced users retain full control through configuration options and direct workflow specification.

The chatbot interface guides users through theory selection by asking clarifying questions about analytical goals. Based on responses, it refines theory selection and explains its choices, helping analysts understand the analytical process.

For researchers with specific theoretical expertise, the system will provide direct theory selection and configuration options. Users can browse available theories organized by analytical capabilities, select specific theories for application, and adjust parameters based on domain knowledge. This flexibility ensures that computational automation enhances rather than replaces scholarly judgment.

The interface abstracts technical complexity while maintaining transparency. Users see which theories are being applied and why, what analytical methods are being used, and how uncertainty is being quantified. Results include natural language explanations linking findings to theoretical constructs, making computational analysis accessible to researchers without technical backgrounds.

## 4.6 Workflow Orchestration and DAG Construction

The system orchestrates complex analytical workflows through Directed Acyclic Graphs (DAGs) that specify sequences of analytical operations. These DAGs ensure reproducible analysis while enabling multi-step processing pipelines.

Each node in the DAG represents a specific analytical operation implemented by a tool. Tools range from basic operations like entity extraction to complex theoretical calculations. The edges specify data flow between operations, ensuring that outputs from one step become inputs to the next. This structure enables complex workflows while maintaining clear execution logic.

The system constructs DAGs automatically based on research questions and selected theories. Advanced users can modify generated DAGs or create custom workflows for specific analytical needs.

DAG execution will include error handling and validation. Each step validates inputs against expected formats and ranges. Failed operations generate informative error messages rather than silent failures. The system tracks provenance throughout execution, recording which tools were used, what parameters were applied, and how results were generated.

## 4.7 System Integration

The system will use a bi-store architecture optimized for different analytical requirements: graph database for network structures (and vector embeddings) and relational database for metadata and provenance. Processing will implement theory-specific algorithms through standardized tool interfaces. The modular architecture will support specialized analytical plugins, with existing prototypes demonstrating qualitative coding capabilities for systematic discourse categorization and process tracing functionality for tracking causal sequences through temporal data. The architecture will prioritize reproducibility through audit trails and version control, supporting academic requirements for transparent research. (Technical specifications in Annex B)


# Chapter 5: Essay 1 Method - Theory Integration Approach

Where Chapter 4 specified the system architecture, this chapter details the methodology for developing the theoretical integration framework that will drive that system.

## 5.1 Framework Development Methodology

This essay develops a framework organizing theories by analytical capabilities rather than origins, addressing fragmentation through computational structure. The framework integrates three complementary organizational approaches. Larson et al.'s (2009) influence operations research provides the analytical levels: individual (cognitive processing, attitude formation), group (social influence, network effects), and Mass Public (media effects, cultural narratives). Lasswell's (1948) communication model, extended by Druckman (2022) with settings, provides the discourse components: Who (source credibility), What (message content), To Whom (audience characteristics), Channel (medium affordances), Settings (temporal context), Effect (behavioral outcomes). Together with the DEPI taxonomy (Describe, Explain, Predict, Intervene), these frameworks create a three-dimensional space for systematic theory organization and selection.

## 5.2 Theory Organization Process

The framework organizes theories from communication, psychological, social, and behavioral sciences based on their analytical capabilities. Most theories can operate across multiple dimensions of the DEPI×Level×Component framework. For example, Social Identity Theory can be descriptive (describing group formation patterns), explanatory (explaining bias mechanisms), and predictive (predicting intergroup conflict), while operating primarily at the group level but extending to individual psychology and Mass Public dynamics. Similarly, Diffusion of Innovations spans descriptive (adoption patterns), predictive (forecasting spread), and intervention (designing adoption strategies) purposes across individual adoption decisions, group influence networks, and Mass Public communication channels. This multi-dimensional nature reveals complementarities for integration and gaps requiring development, enabling systematic theory selection based on research questions rather than disciplinary boundaries.

## 5.3 Theory Meta-Schema Development

The meta-schema standardizes theory representation while preserving theoretical integrity. Components include: metadata (citations, relationships, scope), classification (DEPI position, data requirements, outputs), and methodological patterns (expressions, algorithms, procedures, sequences, rules, statistical models, simulations). This structure enables automated theory selection and execution while maintaining fidelity to original theoretical formulations. (Specifications in Annex A)

## 5.4 Framework Assessment

The framework's effectiveness will be assessed through its ability to guide theory selection and application (see Chapter 8 for validation methodology). Key aspects include whether the organizational structure helps users identify appropriate theories and whether multi-theoretical analysis produces complementary insights.

## 5.5 Expected Deliverables

This essay will produce several key deliverables that establish the theoretical foundation for the computational system. The primary deliverable is the three-dimensional organizing framework that classifies theories by their analytical capabilities across the DEPI taxonomy, levels of analysis, and communication components. This framework will be presented through structured tables and visual representations showing theoretical relationships.

The Theory Meta-Schema specification provides templates for representing theories in computational form. Example schemas for key theories (such as Social Identity Theory, Diffusion of Innovations), and Motivated Reasoning demonstrate how theoretical concepts translate to machine-readable specifications. These examples show how different types of theories - mathematical, logical, procedural - can be consistently represented.

A populated theory library will include initial theory-schemas for theories spanning the framework dimensions. This library will emphasize theories relevant to fringe discourse analysis, including those addressing identity formation, information processing, social influence, and belief dynamics. Each theory will be documented with its theoretical origins, computational specifications, and application examples.

The framework documentation will include guidelines for adding new theories, extending existing theories, and identifying theoretical gaps. This ensures the framework can evolve as new theories emerge or existing theories are refined. The documentation will also address how to handle theories that span multiple dimensions or challenge the organizational structure.

# Chapter 6: Essay 2 Method - System Development

Where Chapter 5 developed the theoretical integration framework, this chapter describes the methodology for implementing the computational system.

## 6.1 Dataset Specification and Preparation

The system development will use the Kunst et al. (2024) COVID conspiracy discourse dataset, which provides a combination of behavioral data and psychological profiles. Table 6.1 summarizes the key dataset characteristics.

Table 6.1: Kunst COVID-19 Conspiracy Discourse Dataset Specifications

| Dataset Component         | Specification                           |
|--------------------------|----------------------------------------|
| Users with Profiles      | 2,506 Twitter users                   |
| Behavioral Interactions  | 7.7 million tweets and engagements    |
| Time Period             | December 2019 - December 2021         |
| Psychological Measures   | Conspiracy mentality scale            |
|                         | Collective narcissism scale           |
|                         | Need for chaos scale                  |
|                         | Misinformation susceptibility scale   |
| Network Construction    | Interaction-based (replies, reposts)  |
| Analysis Approach       | Theory-guided cross-modal analysis    |

NOTE: The dataset combines self-reported psychological assessments with behavioral traces, enabling testing of computational methods against psychological measures.

Data preparation will focus on interaction-based network construction rather than follower relationships. Networks will be defined by actual engagement - replies, reposts, and likes - which better reflect active discourse participation than passive following relationships. Data filtering will use LLM-based quality assessment to assign coherence scores (0-1) based on semantic consistency and discourse relevance. Content relevance filtering will identify posts related to COVID, vaccines, and associated conspiracy narratives using keyword matching and semantic similarity.

The psychological data provides a way to compare our computational estimates. The temporal span allows tracking of belief evolution and network dynamics over the pandemic's progression.

Cross-platform considerations will be documented but not implemented in the initial system. While the current dataset focuses on Twitter, the framework will be designed to accommodate other platforms. Platform-specific features such as retweet cascades are Twitter-specific, while general engagement metrics such as likes and replies exist across platforms. The system architecture will maintain flexibility for future multi-platform integration.

## 6.2 System Implementation

The low-code chatbot interface translates natural language requests into computational workflows, democratizing access to theoretical analysis. Directed Acyclic Graphs orchestrate tool execution with automatic construction from research questions. The two-layer architecture separates one-time theory extraction (Layer 1) from repeated application (Layer 2), enabling theory reuse across datasets. 

Building on prototype work, the system incorporates plugins for qualitative analysis methods. A qualitative coding plugin enables systematic categorization of discourse elements according to theoretical frameworks, while a process tracing plugin tracks causal sequences and decision pathways through discourse evolution. These prototypes demonstrate the feasibility of integrating traditional qualitative methods with computational approaches. (Implementation details in Annex B)

## 6.3 Theory Application Examples

The implementation will demonstrate application of multiple theories (such as Social Identity Theory for group identification, Motivated Reasoning for information processing, and Diffusion of Innovations for narrative spread) to test the system's multi-theoretical analysis capabilities. Performance assessment will follow the validation framework described in Chapter 8.

## 6.4 Performance Considerations

While the system prioritizes theoretical validity over computational performance, practical considerations ensure usability through appropriate tool selection, vectorized operations, and batched language model calls to finish workflows within hours rather than days.

Scalability employs sampling strategies when necessary - complete analysis of 7.7 million interactions may require representative sampling for computationally expensive operations while maintaining data analysis for important steps. Error handling follows fail-fast principles, validating inputs at each step and generating clear error messages rather than questionable results.

## 6.6 Expected Outcomes

The system development will produce a prototype demonstrating theory-aware discourse analysis that validates the feasibility of the approach and provide a foundation for future development.

The primary deliverable will be a system capable of extracting theories from academic literature, applying them to the COVID dataset, and generating analytical results across multiple modalities. The system will demonstrate that computational methods can systematically apply social science theories to large-scale discourse analysis.

Validation results provide evidence about the system's capabilities and limitations. Correlation analyses between computational estimates and psychological measures establish baseline expectations for construct validity. Extraction accuracy metrics identify areas where human validation remains necessary. Cross-modal comparisons reveal which analytical approaches best suit different theoretical questions.

Documentation will include system architecture specifications, theory application examples, and user guides. The architecture documentation will enable future development and extension. Application examples will show how different theories generate insights from discourse data. User guides will help researchers understand when and how to apply the system to their research questions.

# Chapter 7: Essay 3 Method - Analysis and Application

Where Chapter 6 described system implementation and validation, this chapter details the analytical methods enabled by the integrated theoretical and computational framework.

## 7.1 Analytical Methods Progression

This essay demonstrates analytical capabilities enabled by the Theory Meta-Schema, progressing from basic computational methods to cross-modal integration. The demonstration uses the COVID discourse dataset to show how theories translate into analytical approaches.

The analysis moves from simple to complex social science methods. Basic methods apply formulas and rules directly to data. Intermediate methods integrate across data modalities and statistical relationships. Advanced methods include agent-based modeling (ABM) and structural equation modeling (SEM). This progression demonstrates that the Theory Meta-Schema can accommodate the spectrum of social science analytical approaches.

## 7.2 Basic Computational Methods

The system will implement five patterns from theoretical specifications. Formula-based analysis calculates mathematical relationships (Social Learning Theory's influence scores). Algorithmic analysis traces patterns through networks (contagion cascades via breadth-first search). Procedural analysis executes sequential processes (Spiral of Silence decision steps). Rule-based analysis applies logical conditions (Terror Management Theory's threat-defense mechanisms). Sequence analysis tracks temporal progressions, enabling theory combination across time scales - applying Larson's framework for immediate tactical influence processes (emotional appeals, source credibility effects) while simultaneously tracking McGuire's six-step model for longer-term attitude hardening (exposure→attention→comprehension→acceptance→retention→action), revealing how short-term influence operations evolve into durable belief changes.

## 7.3 Cross-Modal Integration

The system will implement cross-modal analysis as established in Chapter 1, testing whether theoretical relationships (such as identity → bias → rejection for Social Identity Theory) hold across connected analytical steps.

## 7.4 Statistical Modeling

The system will demonstrate theory-generated statistical models through regression analysis testing Motivated Reasoning predictions with interaction terms for belief moderation effects, multilevel modeling accounting for users nested within groups with random effects and cross-level interactions, and time series analysis validating Diffusion of Innovations' S-curve patterns for conspiracy narrative adoption. These methods move beyond correlations to test theoretical relationships.

## 7.5 Statistical Integration

Statistical integration demonstrates correlation analysis between graph metrics and psychological measures, pattern detection across different data representations, and testing of theoretical predictions against empirical data. This enables assessment of which theoretical perspectives reveal patterns.

## 7.6 Agent-Based Modeling Implementation

Agent-based modeling (ABM - computational simulation where individual agents follow rules to generate emergent collective behaviors) demonstrates how the Theory Meta-Schema enables theory testing through simulation. The approach would differ from recent LLM-based agent systems by implementing theory-constrained behavioral rules rather than emergent behaviors from language models.

Agent parameterization will utilize the Kunst dataset's personality characteristics. Each agent will receive conspiracy mentality, narcissism, need for chaos, and misinformation susceptibility scores drawn from the distributions. Table 7.1 provides illustrative examples of how psychological traits map to behavioral rules.

Table 7.1: Agent Behavioral Rules

| Psychological Trait        | Potential Behavioral Rule                 |
|---------------------------|--------------------------------------------|
| High Conspiracy Mentality | Preferentially share confirming content   |
| High Narcissism          | Increase posting frequency                |
| High Need for Chaos      | Engage with controversial topics          |
| Low Trust in Authority   | Question official sources                 |

NOTE: These rules demonstrate potential mappings between dataset traits and behaviors. The implementation will explore various theory-behavior combinations to identify patterns that align with observed data, treating these as exploratory relationships rather than validated theoretical claims.

The Reasoned Action Model provides an approach where attitudes, subjective norms, and perceived control generate behavioral intentions.

Figure 7.1. Theory of Reasoned Action and Theory of Planned Behavior
```
External Variables          Behavioral Determinants    Intention    Outcome
┌─────────────────┐        ┌──────────────────────┐               
│ Demographic     │        │   Attitude toward    │               
│ variables       │───────▶│   the Behavior       │──┐            
├─────────────────┤        └──────────────────────┘  │            
│ Attitudes       │                                   │            
│ towards         │        ┌──────────────────────┐  │ ┌─────────────┐  ┌──────────┐
│ targets         │───────▶│   Subjective Norm    │──┼▶│ Intention to│─▶│ Behavior │
├─────────────────┤        └──────────────────────┘  │ │ Perform the │  └──────────┘
│ Personality     │                                   │ │ Behavior    │             
│ traits          │        ┌──────────────────────┐  │ └─────────────┘             
├─────────────────┤        │   Perceived          │──┘                             
│ Other individual│───────▶│   Behavioral         │                                
│ difference      │        │   Control            │                                
│ variables       │        └──────────────────────┘                                
└─────────────────┘                                                                
                                                                                   
         Each behavior is defined by: Action, Target, Context, Time
```
NOTE: Upper portion shows Theory of Reasoned Action (attitude + subjective norm → intention → behavior), complete figure shows Theory of Planned Behavior (adding perceived behavioral control). External variables influence the three main behavioral determinants, which combine to form behavioral intentions that predict actual behavior.

The simulation uses ABM frameworks where agents interact with content, process it based on a theory, update their beliefs, and generate discourse. Multiple theories operate simultaneously governing different behavioral aspects.

The simulation can explore parameter variation within theoretical bounds, with parameter ranges drawn from the distribution's 5th-95th percentiles unless literature specifies otherwise. Assessment would compare aggregate patterns including adoption curves and polarization levels against observed data. Intervention modeling could test strategies such as counter-narratives targeting high-influence nodes or network modifications to reduce echo chamber effects, providing tools for exploring outcomes before resource commitment.

# Chapter 8: Validation Strategy

Where Chapter 7 demonstrated the analytical capabilities of the system, this chapter specifies the validation strategy for assessing its effectiveness.

## 8.1 Multi-Dimensional Validity Framework

The validation strategy will assess multiple aspects of system performance. The system validation will test whether KGAS correctly extracts and applies theories to test theoretical relationships. A comprehensive uncertainty assessment framework spanning fourteen dimensions appears in Annex C, with validation efforts prioritizing dimensions most relevant to demonstrating system capabilities based on data availability and feasibility.

## 8.2 Extraction and Theory Application Validity

Entity extraction validity will assess theory structure completeness via hand-coding theories and comparing extractions using graph edit distance, and inter-LLM reliability tests demonstrating reproducibility though consistency doesn't guarantee accuracy. Continuing with the Social Identity Theory example from Chapter 1, validation would verify that the system correctly identifies anti-vaccine groups as in-groups, public health authorities as out-groups, and vaccine safety concerns as threat narratives - validating that theoretical constructs map accurately to discourse elements.

## 8.3 Construct Validity Assessment

Construct validity will test computational construct accuracy by extracting construct estimates from COVID dataset tweets and correlating with psychological measures (conspiracy mentality, narcissism, need for chaos). Proposed crowd-sourced coding may supplement validation of computational constructs against human judgments. Correlations will be reported to assess pattern extraction capabilities. Theory replication applies extracted theories to original paper datasets comparing structural similarity and directional agreement.

## 8.4 Cross-Modal Coherence and System Capability Assessment

Cross-modal coherence assessment will document modality-specific insights (graph for networks, table for statistics, vector for semantics) and test multi-resolution coherence across individual, network, and population levels. System capability assessment can evaluate multiple dimensions including source credibility ratings, cross-source contradiction detection, entity identity resolution, and reasoning chain validity.

## 8.5 Expected Outcomes Without Predetermined Targets

Validation will report actual metrics without predetermined targets. Process metrics will establish operational baselines: median extraction time per theory, percentage of elements captured, and cross-modal conversion rates. Entity extraction capabilities will be documented. Construct correlations will be reported without required effect sizes. Prediction remains aspirational given social science complexity. Limitations include online discourse restriction, correlational findings, platform specificity, inter-LLM consistency not accuracy, and unavailable ground truth for complex constructs.

# Chapter 9: Timeline and Limitations

Where Chapter 8 established the validation strategy, this final chapter addresses practical implementation considerations and acknowledges research boundaries.

## 9.1 Research Timeline

The dissertation proceeds through three integrated phases over 12 months from proposal approval (September 2025) through defense (February 2026). (See Annex E for detailed timeline with milestones, deliverables, and risk management)

## 9.2 Scope and Limitations

The research demonstrates computational theory application to online discourse without claiming social prediction. Scope encompasses extracting/applying established theories rather than discovering new frameworks. Technical limitations include LLM dependence, single-node constraints, and preserving theoretical nuance computationally. Validation limitations include absent ground truth for constructs, observational data causality challenges, and platform-specific dataset biases.

## 9.3 Contributions to Future Research

This dissertation will establish foundations for theory-aware computational social science through a theory-first architecture designed for the era of increasingly capable language models. The Theory Meta-Schema will provide evolving templates for computational theory representation that will improve as LLM theoretical understanding advances. The validation framework will assess computational theory application against field-standard thresholds, establishing baselines that future systems can build upon. The cross-modal architecture will integrate analytical perspectives while maintaining theoretical fidelity, providing infrastructure that will scale with LLM capabilities.

KGAS will serve as research infrastructure for a future where autonomous research agents conduct sophisticated theoretical analysis. While current system performance may not meet operational demands, the theory-first foundations established here will enable dramatic improvements as LLMs advance. Current metrics provide baselines for next-generation systems - as language models strengthen theoretical understanding, extraction accuracy will improve; as reasoning capabilities expand, workflow generation will become more sophisticated; as context windows grow, multi-theoretical integration will become more nuanced. The research demonstrates that theory-aware computational social science is possible while establishing frameworks that will accommodate the rapidly advancing capabilities of future language models.

---

## References

Kunst, J. R., Gundersen, A. B., Krysińska, I., et al., "Leveraging Artificial Intelligence to Identify the Psychological Factors Associated with Conspiracy Theory Beliefs Online," Nature Communications, Vol. 15, 2024.

Office of the Director of National Intelligence, ICD 203: Analytic Standards, 21 June 2007; ICD 206: Sourcing Requirements for Disseminated Analytic Products, 27 June 2012.

Tajfel, Henri, and John C. Turner, "The Social Identity Theory of Intergroup Behavior," in Stephen Worchel and William G. Austin, eds., The Social Psychology of Intergroup Relations, Brooks/Cole, 1986, pp. 7–24.

Bandura, Albert, Social Learning Theory, Prentice-Hall, 1977.

Lasswell, Harold D., "The Structure and Function of Communication in Society," in Lyman Bryson, ed., The Communication of Ideas, Harper and Row, 1948, pp. 37–51.

Larson, Eric V., et al., Foundations of Effective Influence Operations: A Framework for Enhancing Army Capabilities, RAND Corporation, 2009.

Druckman, James N., "A Framework for the Study of Persuasion," Annual Review of Political Science, Vol. 25, No. 1, 2022, pp. 65–88.

Heuer, Richards J., Psychology of Intelligence Analysis, Center for the Study of Intelligence, 1999.

Park, J. S., et al., "Generative Agents: Interactive Simulacra of Human Behavior," in Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23), Association for Computing Machinery, 2023, pp. 759–789.

Vezhnevets, A. S., et al., Generative Agent-Based Modeling with Actions Grounded in Physical, Social, or Digital Space Using Concordia, arXiv, 2023.

Bail, Chris, et al., "Exposure to Opposing Views on Social Media Can Increase Political Polarization," Proceedings of the National Academy of Sciences, Vol. 115, No. 37, 2018, pp. 9216–9221.

Chong, Dennis, and James N. Druckman, "Framing Theory," Annual Review of Political Science, Vol. 10, 2007, pp. 103–126.

Rogers, Everett M., Diffusion of Innovations, 5th ed., Simon & Schuster, 2010.

Zaller, John R., The Nature and Origins of Mass Opinion, Cambridge University Press, 1992.

---


# Annex A: Theory Meta-Schema Specification

## A.1 Theory Meta-Schema Architecture

The Theory Meta-Schema provides a format for representing social science theories in machine-readable form while preserving terminology and structure. This specification will enable computational application while maintaining theoretical fidelity.

## A.2 Core Schema Components

### Metadata Component
Preserves bibliographic information and theoretical relationships:
- Original authors and publication details
- Connections to related theories through extension, synthesis, or contradiction
- Scope boundaries and application conditions
- Version tracking for schema evolution

### Theoretical Structure Component
Defines entities, relations, and modifiers using original terminology:
- **Entities**: Core concepts with definitions and properties
- **Relations**: Connections between entities with properties (causal, correlational, moderating)
- **Modifiers**: Conditions that affect theoretical elements (boundary conditions, assumptions)

### Computational Representation Component
Specifies how theoretical structures map to data formats:
- **Graph representations**: Network relationships, nodes, and edges
- **Table representations**: Statistical variables and relationships
- **Vector representations**: Semantic similarity comparisons
- **Natural language**: Textual descriptions and explanations

### Algorithms Component
Contains the theory's analytical methods:
- **Mathematical formulas**: Quantitative relationships and calculations
- **Logical rules**: Conditional statements and decision trees
- **Procedural steps**: Operations and workflows
- **Temporal sequences**: Time-based progressions and stages

## A.3 Methodological Pattern Specifications

### EXPRESSIONS
Direct mathematical calculations prescribed by theories:
- Social Identity Theory: in_group_favoritism = (in_group_rating - out_group_rating) / scale_range
- Self-categorization Theory: metacontrast_ratio = intergroup_differences / intragroup_differences  
- Collective self-esteem = (membership_esteem + private_esteem + public_esteem + identity_importance) / 4

### ALGORITHMS
Iterative computational methods for convergence-based analysis:
- Social Identity Theory community detection: Modularity optimization to identify group boundaries
- Optimal distinctiveness calculation: Iterate inclusion/differentiation balance until equilibrium
- Group prototype extraction: K-means clustering on member attributes until prototypes stabilize

### PROCEDURES
Branching analytical workflows with decision points:
- Social Identity Theory threat assessment procedure:
  1. IF intergroup_competition THEN assess resource_scarcity
  2. IF resource_scarcity > threshold THEN examine zero_sum_beliefs
  3. IF historical_conflict EXISTS THEN weight_threat_higher
  4. RETURN threat_level based on accumulated evidence

### SEQUENCES
Linear analytical stages that must be traversed in order:
- Social Identity Theory identity formation analysis:
  1. Social categorization (identify available groups)
  2. Social identification (measure strength of belonging)
  3. Social comparison (assess intergroup differences)
  4. Positive distinctiveness (evaluate favorability)

### RULES
Classification and categorization logic:
- Social Identity Theory context classification:
  - IF shared_group_membership AND out_group_threat THEN bias_likely
  - IF group_status = "low" AND boundaries = "permeable" THEN individual_mobility_strategy
  - IF group_status = "low" AND boundaries = "impermeable" THEN collective_action_strategy

### STATISTICAL_MODELS
Testing theoretical relationships in empirical data:
- Social Identity Theory SEM: group_identification → collective_self_esteem → in_group_favoritism (with mediation)
- Multilevel model: Individual bias nested within group-level threat and competition
- Cross-lagged panel: Reciprocal effects between identification and discrimination over time

### SIMULATIONS
Agent-based and system dynamics pattern generation:
- Social Identity Theory ABM: Agents with group memberships interact, developing identification based on interactions and threat, producing emergent polarization
- Identity fusion simulation: Model extreme group bonding under varying threat conditions
- Schelling-style segregation: Agents relocate based on in-group preference thresholds

## A.4 Social Identity Theory Example

```json
{
  "theory_id": "social_identity_theory",
  "theory_name": "Social Identity Theory",
  "version": "1.0.0",
  "metadata": {
    "authors": ["Henri Tajfel", "John C. Turner"],
    "publication_year": 1979,
    "citation": "Tajfel, Henri, and John C. Turner. 'The Social Identity Theory of Intergroup Behavior.' In The Social Psychology of Intergroup Relations, edited by Stephen Worchel and William G. Austin, 7-24. Brooks/Cole, 1986.",
    "description": "Theory explaining how group membership creates in-group/out-group categorizations that influence behavior through social comparison processes.",
    "theoretical_provenance": {
      "extends": "Social Categorization Theory",
      "related_theories": [
        {
          "theory": "Self-Categorization Theory",
          "relationship": "elaborates",
          "description": "Turner et al. extended SIT to explain categorization at multiple levels"
        }
      ]
    },
    "scope_from_paper": {
      "phenomena_explained": ["Intergroup discrimination", "In-group favoritism", "Group identity formation"],
      "boundary_conditions": ["Requires salient group categorization", "Limited to contexts where group membership is relevant"],
      "level_of_analysis": "group"
    }
  },
  "theory_validity_evidence": {
    "empirical_support": {
      "supporting_studies": [
        {
          "citation": "Tajfel, H., et al. 'Social categorization and intergroup behavior.' European Journal of Social Psychology 1, no. 2 (1971): 149-178.",
          "sample_size": 64,
          "effect_size": 0.73,
          "study_type": "experimental"
        }
      ],
  },
  "theoretical_structure": {
    "entities": [
      {
        "indigenous_name": "in-group",
        "description": "The group that an individual psychologically identifies with",
        "properties": [
          {
            "name": "group_identification",
            "indigenous_name": "degree of identification",
            "measurement": {
              "type": "numeric",
              "scale": "Likert 7-point",
              "range": [1, 7]
            }
          }
        ],
        "examples_from_paper": ["nationality groups", "minimal groups created in laboratory"]
      },
      {
        "indigenous_name": "out-group", 
        "description": "Groups that individuals do not belong to, serving as comparison targets",
        "properties": [
          {
            "name": "perceived_threat",
            "measurement": {
              "type": "numeric",
              "scale": "0-100 continuous"
            }
          }
        ]
      }
    ],
    "relations": [
      {
        "indigenous_name": "identification",
        "from_entity": "individual",
        "to_entity": "in-group",
        "description": "Psychological attachment and belonging to the group",
        "constraints": ["asymmetric"],
        "logical_properties": ["stronger identification leads to stronger in-group favoritism"]
      },
      {
        "indigenous_name": "social comparison",
        "from_entity": "in-group", 
        "to_entity": "out-group",
        "description": "Process of comparing in-group against out-groups on valued dimensions",
        "logical_properties": ["comparison seeks positive distinctiveness for in-group"]
      }
    ],
    "modifiers": [
      {
        "indigenous_name": "intergroup threat",
        "category": "contextual",
        "applies_to": ["identification", "social comparison"],
        "values": ["realistic threat", "symbolic threat"]
      }
    ]
  },
  "computational_representation": {
    "primary_format": "graph",
    "data_structure": {
      "graph_spec": {
        "directed": true,
        "weighted": true,
        "node_types": ["individual", "group"],
        "edge_types": ["membership", "identification", "comparison"]
      }
    }
  },
  "algorithms": {
    "mathematical": [
      {
        "name": "in_group_favoritism_index",
        "indigenous_name": "in-group favoritism",
        "formula": "(in_group_evaluation - out_group_evaluation) / scale_range",
        "parameters": {
          "in_group_evaluation": "numeric rating of in-group",
          "out_group_evaluation": "numeric rating of out-group"
        }
      }
    ],
    "logical": [
      {
        "name": "threat_response_logic",
        "indigenous_name": "identity threat management",
        "rules": [
          {
            "condition": "perceived_threat > threshold AND group_boundaries = permeable",
            "conclusion": "individual_mobility_strategy"
          }
        ]
      }
    ]
  },
  "use_case": {
    "purpose": "Explain and predict intergroup bias through group identity processes",
    "analytical_questions": [
      {
        "question_type": "explanatory",
        "example_questions": ["Why do people favor their own group over others?"]
      }
    ],
    "success_criteria": [
      "Demonstrates in-group favoritism in minimal group conditions",
      "Shows relationship between identification strength and bias"
    ]
  },
  "extraction_process": {
    "entity_extraction": {
      "method": "llm_guided",
      "prompts": ["Extract group entities mentioned in text as collective actors"]
    }
  },
  "validation": {
    "empirical_tests": [
      {
        "claim_from_paper": "In-group favoritism occurs even with minimal group distinctions",
        "expected_result": "positive in-group bias scores"
      }
    ]
  },
  "configuration": {
    "extraction_model": "gpt-4",
    "confidence_threshold": 0.7
  },
  "metadata_tracking": {
    "schema_version": "13.0",
    "validated_against_paper": true
  }
}
```


# Annex B: Technical Architecture Details

## B.1 Tool Suite Organization

The KGAS system integrates specialized tools across multiple functional categories with common interfaces, enabling flexible composition of analytical workflows. Unlike rigid pipeline systems, KGAS's modular architecture allows tools to be chained in any sequence guided by theoretical requirements. Each tool implements a common interface ensuring behavior and integration patterns.

### Document Processing Suite
- **PDF and document extraction**: Multi-format support including academic papers, reports, and web content
- **Text preprocessing**: Language detection, cleaning, normalization
- **Metadata extraction**: Author, date, citation information
- **Reference parsing**: Bibliography and citation extraction

### Graph Operations Suite
- **Entity extraction**: Theory-guided identification of actors, concepts, and relationships
- **Network construction**: Multiple network types (social, semantic, causal)
- **Graph algorithms**: Centrality, clustering, path analysis, community detection
- **Graph transformation**: Format conversion, projection, filtering

### Statistical Analysis Suite
- **Descriptive statistics**: Summary statistics and distributions
- **Inferential testing**: Hypothesis testing, confidence intervals, effect sizes
- **Regression modeling**: Linear, logistic, multilevel, time series
- **Structural equation modeling (SEM)**: Latent variable models, path analysis (statistical technique for testing complex relationships between observed and latent variables)
- **Factor analysis**: Exploratory and confirmatory approaches

### Vector Operations Suite
- **Embedding generation**: Multiple embedding models and techniques
- **Similarity calculation**: Cosine, Euclidean, and custom metrics
- **Clustering**: K-means, hierarchical, DBSCAN, spectral
- **Dimensionality reduction**: PCA, t-SNE, UMAP
- **Semantic search**: Efficient vector similarity search

### Cross-Modal Converters Suite
- **Graph to table**: Network metrics, adjacency matrices, edge lists
- **Table to graph**: Relationship construction, threshold-based networks
- **Graph to vector**: Node embeddings, graph embeddings
- **Vector to graph**: Similarity networks, nearest neighbor graphs
- **Table to vector**: Feature encoding, statistical embeddings
- **Vector to table**: Cluster assignments, similarity matrices

### Agent-Based Modeling Suite
- **Agent initialization**: Theory-based parameterization
- **Behavioral rules**: Theory-derived decision mechanisms
- **Network dynamics**: Evolving interaction patterns
- **Simulation control**: Time steps, convergence, intervention
- **Output analysis**: Aggregate metrics, individual trajectories

## B.2 Workflow Orchestration and DAG Construction

### DAG Architecture
Directed Acyclic Graphs coordinate complex analytical workflows through:


### Automatic DAG Generation
The system will construct DAGs from research questions through:

1. **Question parsing**: Extract key concepts and analytical goals
2. **Theory matching**: Identify relevant theoretical frameworks
3. **Operation selection**: Choose appropriate tools for analysis
4. **Dependency resolution**: Determine execution order
5. **Parameter configuration**: Set operation-specific parameters

### Execution Patterns
- **Sequential execution**: Operations with dependencies
- **Parallel execution**: Independent operations
- **Conditional branching**: Theory-based decision points
- **Iterative refinement**: Feedback loops for optimization
- **Checkpoint recovery**: Resume from intermediate states

## B.3 Data Storage Architecture

### Bi-Store Design Rationale
The system will use complementary storage systems optimized for different data types:

#### Neo4j Graph Database
- **Purpose**: Store and query graph structures
- **Optimizations**: Native graph storage, index-free adjacency
- **Use cases**: Network analysis, relationship queries, path finding
- **Data model**:

#### SQLite Relational Database
- **Purpose**: Structured data and metadata
- **Optimizations**: Local deployment, ACID compliance
- **Use cases**: Configuration, provenance, results storage
- **Schema design**:

### Data Synchronization
- **Entity resolution**: Maintain consistent identities across stores
- **Transaction coordination**: Ensure cross-store consistency
- **Cache management**: Performance optimization layer
- **Backup strategy**: Incremental and full backup support

## B.4 Low-Code User Interface

### Conversational Interface Architecture
The system will provide a low-code interface enabling non-technical users to perform complex analyses through natural language:


### Workflow Generation from Natural Language
- Automatic theory selection based on research questions
- Dynamic DAG construction without coding
- Interactive refinement through dialogue
- Transparent explanation of analytical choices


# Annex C: Validation Framework Details

## C.1 Fourteen Dimensions of Uncertainty Assessment

The validation framework encompasses fourteen dimensions that comprehensively span the analytical pipeline from source assessment through reasoning validation. These dimensions provide a complete assessment framework, though not all dimensions will be equally emphasized in validation. Priority will be given to dimensions that best demonstrate core system capabilities and theoretical contributions.

### Information Source Dimensions
1. **Source Credibility**: Assessment of information source reliability and expertise
2. **Cross-Source Coherence**: Detection of contradictions and agreement across multiple sources  
3. **Temporal Relevance**: Assessment of information currency and temporal validity

### Extraction Quality Dimensions  
4. **Extraction Coverage**: Completeness of entity and relationship extraction from source material
5. **Entity Recognition Accuracy**: Precision in identifying and classifying entities
6. **Construct Validity**: Alignment between computational estimates and theoretical constructs

### Analytical Framework Dimensions
7. **Theory-Data Fit**: Assessment of whether data contains necessary elements for theoretical application
8. **Study Limitations**: Recognition of methodological constraints and biases
9. **Sampling Bias**: Detection of non-representative samples

### Evidence Quality Dimensions
10. **Diagnosticity**: Information value for distinguishing between hypotheses
11. **Sufficiency**: Adequacy of evidence for drawing conclusions
12. **Confidence Assessment**: Documentation of uncertainty levels in analytical outputs

### Integration and Reasoning Dimensions
13. **Entity Identity Resolution**: Correct linking of entity mentions across contexts
14. **Reasoning Chain Validity**: Logical coherence of multi-step theoretical connections

### Primary Validation Methods

**Hand-Coding Validation**
- Manual coding of 5-10 theories using V13 meta-schema for extraction completeness and theory structure accuracy
- Direct comparison of system outputs with researcher assessments using graph edit distance and concept overlap metrics

**Ground Truth Benchmarking**
- HotPotQA benchmark for multi-hop reasoning and complex theoretical connections
- Established NER benchmarks (CoNLL-2003) for entity recognition accuracy
- Precision, recall, and F1-score measurements against objective truth where available

**Construct Correlation Validation**  
- COVID construct correlations testing extracted constructs from 2,506 Twitter users against self-reported psychological measures
- Theory replication studies extracting theories from 3-5 papers with public data and comparing findings

**Inter-LLM Agreement Testing**
- Multiple models (GPT-4, Claude, Gemini) processing identical content to measure consistency across all dimensions
- Recognition that agreement measures reliability rather than accuracy

## C.3 Uncertainty Quantification and Propagation

### Uncertainty Sources
1. **Data uncertainty**: Missing data, measurement error, sampling bias
2. **Extraction uncertainty**: Entity recognition confidence, relationship ambiguity
3. **Analytical uncertainty**: Model specification, parameter estimation
4. **Integration uncertainty**: Cross-modal conversion, aggregation effects

### Propagation Mathematics
**Independent uncertainties**: Root-sum-squares combination
```
σ_total = √(σ₁² + σ₂² + ... + σₙ²)
```

**Correlated uncertainties**: Correlation-adjusted propagation
```
σ_total² = Σσᵢ² + 2ΣΣρᵢⱼσᵢσⱼ
```

### Confidence Bands
Following IC standards:
- **Very Likely**: 80-95% confidence
- **Likely**: 55-80% confidence  
- **Even Chance**: 45-55% confidence
- **Unlikely**: 20-45% confidence
- **Very Unlikely**: 5-20% confidence

### Uncertainty Propagation Framework

For combining multiple uncertainty sources, the system uses root-sum-squares propagation:
```
σ_total = √(σ₁² + σ₂² + ... + σₙ²)
```
where σᵢ represents independent uncertainty components from extraction, construction, and algorithm precision. The combined uncertainty maps to standardized confidence bands for policy-relevant communication.


# Annex D: Ethical Considerations

This study does not involve interaction with, or the collection of identifiable private information from, human subjects. The project will analyze publicly available documents and online content (e.g., social media discourse, policy documents) using an automated Knowledge Graph Analysis System (KGAS). While the system will be iteratively refined based on internal testing, any feedback gathered from subject matter experts (SMEs) regarding the use or usability of the tool will be confined to internal RAND staff, and no identifiable or sensitive data will be collected.

Consistent with RAND HSPC policy and 45 CFR 46.102(e)(1), this activity is not considered "human subjects research" as defined under federal regulations. However, all activities will be submitted for HSPC screening prior to execution to confirm this determination and to ensure ongoing compliance. If any subsequent activities involve systematic collection of evaluative data from SMEs with the intent to contribute to generalizable knowledge, a modification will be submitted for HSPC review and approval.

The project will adhere to RAND's policies and procedures for safeguarding data, including those outlined in the Standard Operating Procedures and the Policy on the Protection of Human Subjects and Ethical Treatment of Research Participants. No collection, use, or disclosure of identifiable private information is planned, and no consent processes (including waivers) are required under current project design. Should this change, the study team will seek appropriate HSPC review and approval.

 The research adheres to RAND Policy on Protection of Human Subjects, HSPC Standard Operating Procedures, Federalwide Assurance (FWA00003425), and applicable federal regulations. Initial screening via RHINO will confirm "Not Human Subjects Research" status prior to project initiation.


# Annex E: Research Timeline

## E.1 Project Overview and Phases

The dissertation research proceeds through three integrated phases over 6 months, from proposal approval (August 2025) through defense (February 2026). This timeline reflects a focused, proof-of-concept methodology with dedicated periods for theoretical development, system prototyping and validation, and advanced modeling, acknowledging the iterative nature of the work.

### Phase Structure
- **Phase 1**: Proposal and Theoretical Foundation (August-September 2025)
- **Phase 2**: System Development and Validation (October-November 2025)  
- **Phase 3**: Advanced Analysis and Modeling (December 2025)
- **Phase 4**: Integration and Finalization (January-February 2026)

## E.2 Phase 1: Proposal and Theoretical Foundation (August-September 2025)

**August 2025: Proposal Defense**
- Finalize and submit the dissertation proposal to the committee
- Schedule and successfully defend the proposal
- Address any immediate conditions from the defense and finalize Chapter 1 (Introduction)

**August-September 2025: Essay 1 (Theoretical Framework)**
- Conduct the intensive critical literature synthesis
- Develop and refine the Three-Dimensional Theoretical Framework and the detailed Theory Meta-Schema
- Codify the 3-5 exemplar theories to be used in the prototype
- Complete and submit a full draft of Essay 1 to the committee for review

**Deliverables**: Approved proposal, HSPC determination letter, Essay 1 draft

## E.3 Phase 2: System Development and Validation (October-November 2025)

**October-November 2025: Essay 2 (KGAS Prototype and Validation)**
- Note: This is a compressed development cycle focused on creating a functional proof-of-concept, not a polished software product
- Incorporate feedback on Essay 1
- Develop the core KGAS prototype based on the proposed technology stack, focusing exclusively on the COVID-19 discourse dataset
- Create the hand-coded "gold standard" dataset for validation
- Crucially, perform the core system validation: Benchmark the LLM extraction pipeline against the gold standard and calculate Precision, Recall, and F1-scores
- Complete and submit a full draft of Essay 2, including the validation results

**Deliverables**: Essay 2 draft, working KGAS system, validation metrics

## E.4 Phase 3: Advanced Analysis and Modeling (December 2025)

**December 2025: Essay 3 (Exemplar Application)**
- Note: This stage will focus on demonstrating the application of one primary advanced model (e.g., the Agent-Based Model) as a proof-of-concept
- Incorporate feedback on Essay 2
- Apply the validated KGAS to parameterize the chosen model
- Run initial simulations and generate illustrative explanatory insights
- Complete and submit a full draft of Essay 3

**Deliverables**: Essay 3 draft, ABM/SEM models, intervention results

## E.5 Phase 4: Integration and Finalization (January-February 2026)

**January 2026: Revisions and Integration**
- Address all committee feedback on the three essays
- Revise Essays 1, 2, and 3 into their final versions
- Draft the overall dissertation conclusion and abstract
- Integrate all chapters, references, and appendices into a single, cohesive manuscript for committee review

**February 2026: Final Defense and Submission**
- Submit the complete dissertation draft to the committee and schedule the final defense
- Prepare and deliver the final defense presentation
- Address any final revisions from the defense committee
- Obtain final signatures and submit the approved dissertation to the Registrar and ProQuest

