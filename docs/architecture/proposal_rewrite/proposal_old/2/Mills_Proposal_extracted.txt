Dissertation Proposal: Using LLMs for Policy Analysis of Fringe Discourse: Brian Mills Table of Contents Dissertation Proposal: Using LLMs for Policy Analysis of Fringe Discourse: Brian Mills 1 1. Introduction: 3 1.1. Policy Specialization: Analysis of Online Discourse Dynamics for Policy Insight 3 1.2. Aims, Objectives, and Overall Policy Relevance of the Dissertation 3 1.2.1. Understand Discourse Impact on Social-Behavioral (SB) Phenomena: 3 1.2.2. Anticipate Plausible Unfoldings: 3 1.2.3. Estimate Effects of Events or Interventions: 4 1.2.4. Inform Policy Decision-Making: 4 1.3. Structure of the Dissertation: A Three-Essay Approach 4 2. Overarching Framework: A Knowledge Graph Analysis System for Discourse 4 2.1. Addressing Limitations of Current Methods 5 2.2. Conceptual and Methodological Linkages Between the Three Essays 5 2.3. Core Thesis Hypothesis 6 2.4. Overall Contribution to Policy Analysis and SB Research 6 3. Essay 1: Theoretical Foundations for LLM-Generated Ontologies and Analysis of Fringe Discourse 7 3.1. Research Questions for Essay 1 7 3.2. Brief Overview of Literature to be Reviewed 7 3.3. Methodological Approach: Critical Literature Synthesis 8 3.4. Expected Contribution: A Theoretical Framework for the Knowledge Graph Analysis System 9 4. Essay 2: Development and Demonstration of a Low-code LLM-Powered Knowledge Graph Analysis Tool for Fringe Discourse 9 4.1. Introduction and Specific Aims of Essay 2 9 4.2. Research Questions and Hypotheses for Essay 2 10 4.3. Background and Literature Review (Specific to Tool Design) 10 4.4. Data Sources and Collection Methods (Case Study: Scientific Corruption/COVID Discourse) 11 4.5. Methodology: Tool Development and Demonstration 12 4.5.1. Knowledge Graph System Architecture: 12 4.5.2. LLM Integration for Coding and Analysis: 13 4.5.3. Application of Descriptive and Predictive Transformations (for Demonstration): 13 4.5.4. Capturing Data for Bayesian Cognitive Modeling 14 4.5.5. Usability Assessment Plan: 14 4.6. Preliminary Analyses 15 4.7. Expected Outcomes and Contribution of Essay 2 15 5. Essay 3: Explanatory Analysis and Intervention Modeling in Fringe Discourse using the Knowledge Graph System 16 5.1. Introduction and Specific Aims of Essay 3 16 5.2. Research Questions for Essay 3 16 5.3. Data Sources (Building on Essay 2 Dataset) 16 5.4. Analytical Approach Sketch 17 5.4.1. Application of Explanatory Transformations: 17 5.4.2. Agent-Based Modeling (ABM): 17 5.4.3. Bayesian Cognitive Modeling and Validation 18 5.5. Expected Outcomes and Contribution of Essay 3: Policy-Relevant Explanatory Insights and Intervention Scenarios 18 6. Ethical Considerations 19 7. Schedule of Work 20 May 2024 20 June 2024 20 August 2024 21 September 2024 21 October 2024 22 November 2024 22 December 2024 23 7. Conclusion 23 9. References 24 Annex A: Potential Data Sources for UFO Case Study (Concise Overview) 27 Introduction: This dissertation addresses the need for improved methods to analyze the evolution of online discourse, particularly within fringe communities, to support policy decision-making. The work focuses on developing and demonstrating an application of what I call a Knowledge Graph Analysis System (KGAS), grounded in social and behavioral (SB) science. This system integrates Large Language Models (LLMs) to facilitate the systematic study of how online communication shapes, and is shaped by, human cognition and behavior, with the goal of informing public policy. This dissertation builds directly upon prior academic work. The project's intellectual origins began with a tutorial on conspiracy theories with Dr. Eric Larson in 2021. The research was then developed into the current proposal during an independent study, also with Dr. Larson, in 2024. 1.1. Policy Specialization: Analysis of Online Discourse Dynamics for Policy Insight This research specializes in the analysis of online discourse dynamics to extract insights relevant to public policy. The proliferation of online platforms has created vast repositories of public discourse that reflect and shape communication, cognition, and behavior relevant to public policy. Understanding these dynamics is fundamental, echoing foundational questions in communication research about the influence of messages and their pathways. These dynamics, especially within influential or rapidly evolving fringe communities, are critical for policymakers addressing issues ranging from public health adherence and information integrity to social cohesion and national security. This work aims to equip policy analysts with more effective tools to navigate and interpret this complex information environment, recognizing that the meaning and impact of discourse are constructed through complex interactions between message content, source, channel, and audience characteristics. 1 Harold D. Lasswell, “The Structure and Function of Communication in Society,” in Lyman Bryson, ed., The Communication of Ideas , New York: Institute for Religious and Social Studies, 1948 2 For theories on attitude formation and change, see Richard E. Petty and John T. Cacioppo, Attitudes and Persuasion: Classic and Contemporary Approaches , Westview Press, 1996; Icek Ajzen and Martin Fishbein, Understanding Attitudes and Predicting Social Behavior , Prentice-Hall, 1980. The role of cognitive frameworks in processing information is central to John R. Zaller, The Nature and Origins of Mass Opinion , Cambridge University Press, 1992. Social learning and observational influences are key in Albert Bandura, Social Foundations of Thought and Action: A Social Cognitive Theory , Prentice-Hall, 1986. 1.2. Aims, Objectives, and Overall Policy Relevance of the Dissertation The overarching policy relevance of this dissertation lies in providing decision-makers with improved methods to comprehend and respond to phenomena driven or reflected by online discourse. The specific objectives are to develop and apply methods that enable descriptive, explanatory, predictive, and potentially interventionary analysis to help: 1.2.1. Understand Discourse Impact on Social-Behavioral (SB) Phenomena: To better comprehend how specific discourse patterns, narratives, and communication strategies influence individual and group attitudes, beliefs, and actions that have policy implications. This involves understanding mechanisms of persuasion, 3 Petty and Cacioppo, 1996.. belief formation, and behavioral change, 4 Zaller, 1992. considering how individuals process information and how discourse frames 5 Ajzen and Fishbein, 1980; Bandura, 1986. can alter perceptions and subsequent actions. The study of these processes is crucial for understanding the interplay of communication, cognition, and behavior. 6 James N. Druckman, “The Implications of Framing Effects for Citizen Competence,” Political Behavior , Vol. 23, No. 3, September 2001 1.2.2. Anticipate Plausible Unfoldings: To develop capabilities to anticipate how these SB phenomena, as observed and influenced by discourse reflecting various aspects of communication, cognition, and behavior, may plausibly evolve over time or in response to changing conditions. 1.2.3. Estimate Effects of Events or Interventions: To provide a framework for estimating the potential effects of external events or specific policy interventions (e.g., communication campaigns, platform moderation changes) on the discourse landscape and associated communication, cognition, and behavior. 1.2.4. Inform Policy Decision-Making: Ultimately, to inform policy decision-making by providing more timely, nuanced, and evidence-based insights derived from the systematic analysis of online discourse. Achieving these objectives requires methods that can scale to large volumes of data while retaining sensitivity to context, theory, and the complexities of human communication. This includes leveraging computational techniques for semantic analysis, building on a long tradition of efforts to systematically extract meaning from text. 7 For an early example of computational content analysis, see Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie, The General Inquirer: A Computer Approach to Content Analysis , MIT Press, 1966. For foundational work on measuring meaning, see Charles E. Osgood, George J. Suci, and Percy H. Tannenbaum, The Measurement of Meaning , University of Illinois Press, 1957. 1.3. Structure of the Dissertation: A Three-Essay Approach This dissertation adopts a three-essay format. Each essay contributes to the overarching goal of developing and demonstrating an improved methodology for policy-relevant discourse analysis focusing on communication, cognition, and behavior: Essay 1 establishes the theoretical foundations from discourse analysis, social-behavioral science, and related methodological fields, defining the requirements for a theoretically grounded, LLM-assisted knowledge graph analysis system. Essay 2 details the design, development, and initial demonstration of this "low-code" LLM-powered tool —the Knowledge Graph Analysis System—focusing on its architecture, LLM integration, and core functionalities. 8 While this dissertation aims to develop a tool requiring minimal coding for core functionalities, the "low-code" paradigm allows subject-matter experts to build applications without traditional programming. Examples of prominent no-code/low-code platforms include Airtable (Airtable, homepage, undated, as of May 28, 2025: ) and Retool (Retool, homepage, undated, as of May 28, 2025: https://www.airtable.com ). https://retool.com Essay 3 applies the developed system to conduct in-depth explanatory analysis and model potential interventions within a specific case study, showcasing its advanced analytical capabilities for generating policy-relevant insights into the mechanisms and processes shaping communication, cognition, and behavior. This proposal will briefly outline all three essays, providing the necessary detail for Essay 2 as the primary focus to demonstrate the feasibility and execution plan for a core component of the dissertation. The subsequent sections will elaborate on the overall research framework, the individual essays, ethical considerations, and the project timeline . 2. Overarching Framework: A Knowledge Graph Analysis System for Discourse The central methodological framework of this dissertation is an application of what I call a Knowledge Graph Analysis System (KGAS). This system is designed to integrate the capabilities of Large Language Models (LLMs) with the structural advantages of knowledge graphs, all while being guided by principles from social-behavioral (SB) science and established discourse analysis techniques. The aim is to create a more powerful and adaptable approach for deriving policy-relevant insights from complex online discourse. 2.1. Addressing Limitations of Current Methods Current methods for analyzing online discourse present distinct challenges. Qualitative approaches, while offering depth and contextual understanding, are often labor-intensive and difficult to scale to the volume and velocity of online data. Quantitative methods, including many computational linguistics techniques, can achieve scale but may overlook nuanced meanings, rhetorical strategies, or the underlying SB drivers of the discourse. Furthermore, while tools for graph-based analysis of text are emerging, some can present a "one size fits all" approach, lacking the flexibility to be easily optimized for specific analytic tasks or deeply integrated with diverse theoretical frameworks. This is a noted challenge with some implementations of Retrieval-Augmented Generation (RAG) that use knowledge graphs. 9 For examples of enterprise-level Graph-RAG architectures, see Tomaz Bratanic, “Awesome-Graph-RAG: A Collection of Community RAG Work with Neo4j,” webpage, GitHub, May 2024. As of May 28, 2025: ; and Microsoft, "Unleash the power of GraphRAG with Neo4j and Azure OpenAI," Microsoft Tech Community, April 25, 2024. As of May 28, 2025: https://github.com/tomasonjo/awesome-graph-rag . https://techcommunity.microsoft.com/t5/azure-ai-services-blog/unleash-the-power-of-graphrag-with-neo4j-and-azure-openai/ba-p/4123508 The KGAS I propose seeks to address these limitations by 10 For a related discussion of frameworks for influence operations that similarly emphasizes the need for targeted, adaptable approaches, see Eric V. Larson, Richard E. Darilek, Daniel Gibran, Brian Nichiporuk, Amy Richardson, Lowell H. Schwartz, and Cathryn Quantic Thurston, Foundations of Effective Influence Operations: A Framework for Enhancing Army Capabilities, RAND Corporation, 2009. enabling flexible, user-defined, use-case-specific ontology modules tailored to specific policy questions and theoretical constructs, rather than relying on fixed schemas. leveraging LLMs for nuanced, scalable interpretation of text, assisting in the extraction of entities, relationships, claims, evidence, sentiment, and other relevant discourse features defined within the ontology module. providing a modular architecture with distinct retrieval and transformation operators, allowing analysts to construct tailored analytical pipelines; and facilitating a "low-code" environment where subject-matter experts, not just data scientists, can harness these advanced analytical capabilities. 2.2. Conceptual and Methodological Linkages Between the Three Essays The three essays of this dissertation are designed as an integrated progression, each building upon the last to develop, demonstrate, and apply the KGAS. The common thread is the system's explicit design to operationalize theories of communication, cognition, and behavior to structure the analysis of online discourse. 11 Here, communication refers to the content, structure, and transmission of messages between actors (e.g., Who says what to whom?). Cognition refers to the mental processes of acquiring knowledge and understanding through thought and experience, including belief formation and attitude change. Behavior refers to the observable actions of individuals or groups, particularly those relevant to policy outcomes (e.g., compliance with public health guidance). Throughout the rest of this proposal, I refer to this triplet—communication, cognition, and behavior—as the core focus of analysis. Essay 1 (Theoretical Foundations) establishes the conceptual requirements for the system. It answers the "why" and "what" of the system's design by reviewing literature from discourse analysis, knowledge representation, and SB science to identify the core elements, relationships, and analytical operations the system must support. This review ensures the KGAS is grounded in foundational models of communication, persuasion, and belief formation. These insights inform the design of the system's transformation operators (e.g., transformations from unstructured text to structured outputs like a topic classification, a belief probability, or an actor-claim graph). The system's ability to extract features like stance, for example, builds on prior work in the field. 12 William Marcellino, Christopher Paul, Erik L. Sayers, Michael Schwille, Ryan Bauer, Jordan R. Vick, and William F. Landgraf III, Developing, Disseminating, and Assessing Command Narrative: Anchoring Command Efforts on a Coherent Story , RAND Corporation, RR-A353-1, 2021. As of May 28, 2025: . https://www.rand.org/pubs/research_reports/RRA353-1.html Essay 2 (System Development and Demonstration) details the "how"—the design, implementation, and initial demonstration of the KGAS as a low-code tool. It describes the architecture, the integration of LLMs for knowledge extraction based on the use-case-specific ontology modules, and the user interface. Essay 3 (In-Depth Application and Validation) provides the "so what" by applying the fully developed system to conduct descriptive, explanatory, predictive, and potentially interventionary analysis. It focuses on using the system to identify the mechanisms and processes of narrative diffusion, map discourse elements to SB models, and explore the utility of modeling potential policy interventions. This validates the system's capacity to generate significant, policy-relevant insights. 2.3. Core Thesis Hypothesis The central analytical proposition of this dissertation is that a low-code tool integrating LLMs and knowledge graphs can demonstrably improve the analysis of fringe discourse for policy-relevant insights. This proposition is guided by two core research questions that will be addressed through the development and application of the KGAS: Can a low-code tool effectively integrate LLMs to enable subject-matter experts (SMEs) to generate tailored knowledge graphs from fringe discourse and perform theoretically grounded analyses based on user-defined interrogatives and use-case-specific ontology modules? Will SMEs using the developed tool be able to identify key discourse elements (e.g., actors, narratives, claims, topics), their properties, and their relationships with greater efficiency and comparable or improved accuracy relative to manual qualitative methods or less-tailored automated tools? This approach draws inspiration from emerging architectures in RAG, such as those being developed by Microsoft and the open-source community using graph databases like Neo4j, but tailors the application specifically for social-behavioral analysis in a low-code environment. 13 Bratanic, 2024; Microsoft, 2024. 2.4. Overall Contribution to Policy Analysis and SB Research This dissertation aims to make several contributions: Methodological: I will introduce a novel, flexible, and theoretically grounded system for discourse analysis that synergizes LLMs, knowledge graphs, and SB principles. The "low-code" aspect aims to democratize access to advanced analytical capabilities for subject-matter experts in policy domains. The 'low-code' philosophy applies to the analytical interface, where SMEs can construct and execute complex queries without writing code. The emphasis on adaptable use-case-specific ontology modules and modular analytical pipelines addresses a key limitation of current tools. Policy Analysis: By enabling more sophisticated and timely analysis of online discourse, the system can enhance the ability of policy analysts and decisionmakers to understand public sentiment, track narrative evolution, identify emerging threats or opportunities, and assess the potential impact of information campaigns or other policy interventions. This contributes directly to the aims of understanding impact, anticipating plausible unfoldings, estimating effects, and informing decisions. Social-Behavioral Research: The proposed system offers a new instrument for researchers studying the interplay between communication, cognition, and behavior in online environments. It can facilitate the testing of SB theories at scale, allow for the discovery of new patterns in discourse, and provide a structured way to link micro-level communication acts to macro-level social phenomena. By enabling an analyst to build a theoretically and empirically grounded model from the data itself, the system's methodology is analogous to grounded theory. Where grounded theory uses iterative coding of qualitative data to allow a theory to emerge, the KGAS uses LLM-assisted coding to allow a structured, computable model of the discourse to emerge, which can then be interrogated using SB frameworks. 14 Grounded theory is a systematic methodology in the social sciences involving the construction of theories through the analysis of data. It involves an iterative process where data collection and analysis inform each other, allowing theoretical concepts to emerge directly from the data rather than from preconceived hypotheses. See Anselm L. Strauss and Juliet Corbin, Basics of Qualitative Research: Grounded Theory Procedures and Techniques , Sage Publications, 1990. Ultimately, I endeavor for this work to advance our capacity to transform complex online discourse into actionable knowledge for policy and research. 3. Essay 1: Theoretical Foundations for LLM-Generated Ontologies and Analysis of Fringe Discourse This first essay serves as the theoretical and conceptual cornerstone for the entire dissertation. Its primary purpose is to conduct a critical literature synthesis that addresses a fundamental challenge in the study of influence: the fragmentation of behavioral theories across dozens of disconnected disciplines. This essay moves beyond a simple review to propose a novel solution: a unified, three-dimensional framework that organizes the vast landscape of social-behavioral theories by their core analytical function and causal logic. This framework, and the Theory Meta-Schema that makes it computable, establishes the intellectual and architectural requirements for the Knowledge Graph Analysis System (KGAS) developed in Essay 2. By synthesizing insights from communication studies, political psychology, sociology, and computational science, this essay provides the essential blueprint for a system designed to translate complex theory into actionable policy insight. 3.1. Research Questions for Essay 1 The literature synthesis and theoretical construction in this essay are guided by two central research questions: How can the fragmented landscape of social-behavioral theories of influence and persuasion be synthesized into a coherent, multi-dimensional framework that is not only analytically powerful but also computationally tractable? How can this new theoretical framework be operationalized into a Theory Meta-Schema—a standardized, machine-readable format—that enables a software system (the KGAS) to dynamically apply diverse and complex social science theories to the analysis of online discourse? 3.2. From a "List of Theories" to a "Theory of Process" A common approach in social science is to organize theories by their academic discipline of origin—a "theoretical traditions" approach. However, for the applied, computational goal of this dissertation, such a structure is insufficient. It presents theories in static, isolated silos, focusing on their intellectual heritage rather than their analytical function. It fails to show how theories interact to explain real-world phenomena and does not align with the dynamic, process-oriented nature of influence. To overcome this, this dissertation adopts the process-oriented approach advocated by scholars of influence . The goal is not to create a static classification, but a dynamic, functional framework that models the process of influence itself. This requires a new typology that organizes theories based on what they do and their underlying assumptions about how change happens 15 Larson, E. V., et al. (2009). Foundations of Effective Influence Operations: A Framework for Enhancing Army Capabilities. RAND Corporation . This framework is not proposed as a universal replacement for existing theoretical traditions, but as a functional typology designed specifically for the task of computational modeling. Its value is not in being 'more correct,' but in being more computable and aligning theory with the data structures required by the KGAS. It makes the theoretical landscape legible to both a human analyst and a machine. 3.3. A Three-Dimensional Framework for Analyzing Influence This essay proposes that the landscape of influence theories can be organized along three distinct axes. This provides a "theory of process" that integrates insights from influence operations, communication science 16 Lasswell, H. D. (1948). The Structure and Function of Communication in Society. In L. Bryson (Ed.), The Communication of Ideas (pp. 37–51). Institute for Religious and Social Studies. , and the broader study of human action 17 Druckman, James N. “A Framework for the Study of Persuasion.” Annual Review of Political Science , vol. 25, May 2022, pp. 65–88. Annual Reviews , . https://doi.org/10.1146/annurev-polisci-051120-110428 . 18 Eyster, Harold N., Terre Satterfield, and Kai M.A. Chan. "Why People Do What They Do: An Interdisciplinary Synthesis of Human Action Theories." Annual Review of Environment and Resources , vol. 47, 2022, pp. 725–751. Annual Reviews , . https://doi.org/10.1146/annurev-environ-020422-125351 This framework is presented not as the single 'correct' way to organize the field, but as a novel, functional typology designed specifically to meet the needs of a computational analysis system. Its value lies in its ability to make the theoretical landscape legible to both a human analyst and a machine. Theories that span multiple categories will be noted as important examples of the rich interplay that this framework helps to reveal, rather than as failures of the classification. Dimension 1: Level of Analysis (The 'Scale'). This dimension defines where influence occurs, classifying theories by their primary unit of analysis. Micro: The individual level (e.g., cognitive processes, personality). Meso: The group and network level (e.g., community dynamics, peer influence). Macro: The mass public and societal level (e.g., media effects, cultural norms). Dimension 2: Component of Influence (The 'Lever'). This dimension defines what part of the communication process a theory seeks to explain, directly operationalizing the classic Lasswell (1948) and modern Druckman (2022) models. Who: Theories of the Speaker/Source. Whom: Theories of the Receiver/Audience. What: Theories of the Message/Treatment. Channel: Theories of the Medium/Context. Effect: Theories of the Outcome/Process. Dimension 3: Causal Metatheory (The 'Logic'). This dimension defines the theory's core assumption about the engine of change . It is derived from the meta-analytic work of Eyster et al. (2022) and provides the primary narrative flow for the analysis. Agentic Logic: Causation flows from individual agency, beliefs, and needs. Structural Logic: Causation flows from external structures, rules, and incentives. Interdependent Logic: Causation is an emergent property of the feedback loops between agents and structures. 3.4. The Analytical Toolkit: Three Typologies of Influence This essay proposes that theories of influence can be organized along three distinct axes. Together, these axes form a "theory of process" that draws on insights from influence operations, communication science, and the study of human action. Dimension 1: Level of Analysis (The 'Scale'). This dimension defines where influence occurs, classifying theories by their primary unit of analysis: Micro: The individual level. Meso: The group and network level. Macro: The mass public and societal level. Dimension 2: Component of Influence (The 'Lever'). This dimension defines what part of the communication process a theory seeks to explain, operationalizing the classic Lasswell (1948) and modern Druckman (2022) models: Who: Theories of the Speaker/Source. Whom: Theories of the Receiver/Audience. What: Theories of the Message/Treatment. Channel: Theories of the Medium/Context. Effect: Theories of the Outcome/Process. Dimension 3: Causal Metatheory (The 'Logic'). This dimension defines the theory's core assumption about the engine of change, derived from the meta-analysis of Eyster et al. (2022): Agentic Logic: Causation flows from individual agency, beliefs, and needs. Structural Logic: Causation flows from external structures, rules, and incentives. Interdependent Logic: Causation is an emergent property of feedback between agents and structures. To present this three-dimensional framework, it is organized as a sequence of three distinct tables, each representing a different causal logic. This creates a logical progression from simple to complex explanations. Table 3.1. A Typology of Theories with Agentic Causal Logic Component of Influence Micro-Level (Individual) Meso-Level (Group/Network) Macro-Level (Mass Public) Who (The Speaker/Source) Source credibility models Incidental punditry Operational code analysis Whom (The Receiver/Audience) Elaboration likelihood model; health belief model Social identity theory; self-determination theory The American voter model What (The Message) Behavioural economics and prospect theory Conformity theory Moral reframing Channel (The Medium) Processing goal alteration Media effects Effect (The Outcome) Transtheoretical model (stages of change); cognitive dissonance theory Theory of normative social behaviour Sleeper effect SOURCES: Features information from Ajzen, 1985; Brady et al., 2017; Broockman and Butler, 2017; Campbell et al., 1960; Cialdini, 2003; Druckman, 2003; Feinberg and Willer, 2019; Festinger, 1957; George, 1969; Green and Brock, 2000; Hovland and Weiss, 1951; Kahneman and Tversky, 1979; Minozzi et al., 2020; Moscovici, 1980; Petty and Cacioppo, 1986; Prochaska, DiClemente, and Norcross, 1992; Rimal and Real, 2005; Ryan and Deci, 2000; Scheufele, 2000; Strecher and Rosenstock, 1997; Tajfel and Turner, 1979; West and Zimmerman, 1987. NOTE: Theories are placed according to their primary locus of explanation. The tables present exemplars and do not represent an exhaustive list of all possible theories. Table 3.2. A Typology of Theories with Structural Causal Logic Component of Influence Micro-Level (Individual) Meso-Level (Group/Network) Macro-Level (Mass Public) Who (The Speaker/Source) Theories of opinion leadership Media slant and market pressures Whom (The Receiver/Audience) Deterrence theory Sociology of organizations Coercive diplomacy What (The Message) Framing theory Social marketing Scope of conflict Channel (The Medium) Diffusion of innovations; social network analysis Socio-ecological model Effect (The Outcome) Governing the commons Social movements theory SOURCES: Features information from Andreasen, 1994; Beccaria, 1778; Black, 1958; Blumer, 1995; Borgatti, 2003; Bronfenbrenner, 1994; Chong and Druckman, 2007b; Crozier and Friedberg, 1992; Gentzkow and Shapiro, 2010; George, 1991; Katz and Lazarsfeld, 1955; McAdam et al., 1996; Ostrom, 1990; Rogers, 2010; Schattschneider, 1960; Valente and Pitts, 2017. NOTE: Theories are placed according to their primary locus of explanation. The tables present exemplars and do not represent an exhaustive list of all possible theories. Table 3.3. A Typology of Theories with Interdependent Causal Logic Component of Influence Micro-Level (Individual) Meso-Level (Group/Network) Macro-Level (Mass Public) Who (The Speaker/Source) Self-presentation goals Presidential responsiveness Whom (The Receiver/Audience) Pretreatment effects Social cognitive theory Cultural evolution What (The Message) Competitive framing Ideation theory Channel (The Medium) Communication for social change model Complex systems theory Effect (The Outcome) Integrated behavioural model Community engagement models Multilevel sociotechnical transitions SOURCES: Features information from Bail et al., 2018; Bandura, 2001; Boyd and Richerson, 1988; Chong and Druckman, 2007a; Druckman and Bolsen, 2011; Druckman and Jacobs, 2015; Druckman and Leeper, 2012b; Figueroa et al., 2002; Hashagen, 2002; Holland et al., 1998; Kincaid, 2000; Kraft et al., 2020; Ladyman, Lambert, and Wiesner, 2013; Montano and Kasprzyk, 2015; Smith et al., 2010. NOTE: Theories are placed according to their primary locus of explanation. The tables present exemplars and do not represent an exhaustive list of all possible theories.. 3.5. Making Theory Computable: The Theory Meta-Schema The three-dimensional framework provides the abstract model for organizing the theoretical landscape. The Theory Meta-Schema is the computational bridge that makes this framework operational. It is a standardized, machine-readable "recipe" that translates any social theory—from a simple heuristic to a complex process model—into a set of explicit instructions that the Knowledge Graph Analysis System (KGAS) can execute. This is the core technical innovation that enables a computer system to "read" and apply social science theory. By defining a theory in a file that conforms to this schema, an analyst gives the KGAS a complete blueprint for how to extract relevant information from text and what analytical operations to perform. The schema is composed of three primary components: the theory's identity (metadata), its dispatch key for automated reasoning (classification), and its substance (schema). A. Metadata: The Theory's Identity This block captures the bibliographic identity of the theory and its relationship to other theories. citation: The full bibliographic citation for the source document, ensuring academic traceability. annotation: A concise, 1-3 sentence summary of the theory's core proposition. extends: A key field that allows for theoretical inheritance. A theory can be defined as an extension or modification of another base theory (e.g., the Theory of Planned Behavior extends the Theory of Reasoned Action ), allowing for efficient and relational model building. B. Classification: The Automated Dispatch Key This block is mission-critical for the KGAS. It serves as a pre-inferred summary that allows the system's "Agent Orchestrator" to automatically select the appropriate analytical strategy and dispatch the correct reasoning engine for a given task. model_type: Defines the primary data structure the theory assumes (e.g., Graph, Sequence, Table). This tells the system whether it needs to build a network, a timeline, or a typology. reasoning_engine: Specifies the computational engine required to analyze the model (e.g., Graph_Engine for centrality analysis, Temporal_Engine for sequence analysis). Link to the 3D Framework : This block also contains the tags that explicitly connect the theory to the dissertation's overarching framework: causal_logic: [Agentic, Structural, Interdependent] level_of_analysis: [Micro, Meso, Macro] component_of_influence: [Speaker, Receiver, Message, Channel, Effect] analytical_purpose: [Descriptive, Explanatory, Predictive, Interventionary] These tags are the hooks that allow an analyst to query the theory library using the 3D framework—for instance, to "find all Meso-level, Agentic theories that help Explain the Receiver." C. Schema: The Theoretical Core This is the main component that defines the substance of the theory itself. It is composed of several, more granular blocks: Ontology: This defines the "nouns" of the theory—the key concepts to be extracted from text. A crucial principle here is the preservation of the author's original language. entities: The fundamental units or actors (e.g., 'Individual', 'Group'). Each entry captures the indigenous_term (the author's exact phrase) and a standardized name. connections: The relationships between entities (e.g., 'influences', 'believes'). properties: The attributes that describe entities or connections (e.g., 'credibility'). Axioms (Optional): Defines the core "rules" or fundamental assumptions of the theory, if explicitly stated by the author. This ensures the KGAS operates within the theory's own logical constraints. Analytics (Optional): Defines the "verbs" of the theory—how it measures or analyzes the world. This can include specific metrics (e.g., 'attitude change score' with a formula) or focal_concepts that the theory directs an analyst to pay attention to. Process (Optional): Defines the sequence of steps for applying the theory, if one exists. The schema supports multiple process modes to capture the diversity of theoretical structures: sequential: For theories that posit ordered stages (e.g., Transtheoretical Model). iterative: For theories that involve repeated cycles until convergence (e.g., some game-theoretic models). workflow: For complex, graph-based processes with conditional branching. Telos: This defines the "purpose" of the theory. It specifies the theory's primary analytical_purpose and level_of_analysis, and defines the structure of the expected output_format and the success_criteria for a valid application. This goal-oriented structure is what allows the KGAS to match a user's analytical query to the most appropriate theoretical tool. By implementing this meta-schema, the dissertation moves beyond merely discussing theories to making them active, computable components in a system for discourse analysis 3.6. Methodological Approach This essay will employ a critical literature synthesis. The objective is not merely to summarize existing work but to actively integrate findings from disparate fields to populate and justify the three-dimensional framework. The synthesis will identify convergences and gaps, extracting core principles that are operationalized in both the framework itself and the design of the Theory Meta-Schema. This translation of abstract theory into a computable schema involves necessary trade-offs. The goal of the meta-schema is not to capture the full, un-translatable nuance of a theory, but to extract its core, computable logic for the specific purpose of large-scale discourse analysis. The 'in-group favoritism score,' for example, should be understood not as a replacement for the richness of Social Identity Theory, but as a computational proxy that allows the system to identify and quantify potential instances of the theorized behavior in data. This dissertation explicitly acknowledges and accepts this trade-off as a prerequisite for computational analysis. 3.6.1. A Worked Example: Codifying Social Identity Theory To demystify the process of translating an abstract social theory into a computable format, this section provides a worked example using a foundational theory in social psychology: Social Identity Theory (SIT). It is important to acknowledge that this translation of abstract theory into a computable schema is not without trade-offs. The goal of the meta-schema is not to capture the full, un-translatable nuance of a theory, but to extract its core, computable logic for the specific purpose of large-scale, automated discourse analysis. The 'in-group favoritism score,' for instance, should be understood not as a replacement for the richness of Social Identity Theory, but as a computational proxy that allows the system to identify and quantify potential instances of the theorized behavior within a massive dataset. Step 1: The Core Theory Social Identity Theory posits that individuals derive a portion of their self-concept and self-esteem from their membership in social groups. This leads to a cognitive process of categorizing the social world into an "in-group" (to which one belongs) and various "out-groups." To maintain a positive self-concept, individuals are motivated to view their in-group favorably, often resulting in behaviors like in-group favoritism and out-group derogation (Tajfel & Turner, 1979). Step 2: Classification First, we classify the theory to enable automated processing by the KGAS. SIT is fundamentally about the relationships between individuals and groups, making a graph the natural data structure. JSON "classification": { "model_type": "Graph", "reasoning_engine": "Graph_Engine", "compatible_operators": ["community_detection", "centrality", "link_prediction"], "summary": "Models individuals and groups as nodes, with relationships of identification and sentiment.", "causal_logic": "Agentic", "level_of_analysis": "Meso", "component_of_influence": "Receiver", "analytical_purpose": "Explanatory" } This block tells the KGAS to use its Graph_Engine and that relevant operations will involve finding communities and influential actors. It also explicitly links SIT to the dissertation's 3D theoretical framework. Step 3: Ontology (The 'Nouns' of the Theory) Next, we define the core concepts of SIT as entities, connections, and properties. The indigenous_term field is critical, as it preserves the exact language of the source theory. Entities: These are the fundamental actors or objects. JSON "entities": [ { "indigenous_term": "individual", "name": "Actor", "description": "A single person within the social context." }, { "indigenous_term": "in-group", "name": "Group", "description": "A social group with which an individual identifies." }, { "indigenous_term": "out-group", "name": "Group", "description": "A social group with which an individual does not identify and often compares their in-group against." } ] Connections: These define the relationships between entities. JSON "connections": [ { "indigenous_term": "identifies_with", "description": "An individual's psychological membership in an in-group.", "domain": ["Actor"], "range": ["Group"] }, { "indigenous_term": "exhibits_favoritism_towards", "description": "An individual expresses positive sentiment or behavior towards a group.", "domain": ["Actor"], "range": ["Group"] }, { "indigenous_term": "exhibits_derogation_towards", "description": "An individual expresses negative sentiment or behavior towards a group.", "domain": ["Actor"], "range": ["Group"] } ] Step 4: Analytics (The 'Verbs' of the Theory) We then define how the theory measures phenomena. This involves creating metrics that the KGAS can compute from the extracted knowledge graph. Metrics: JSON "metrics": [ { "indigenous_term": "in-group favoritism score", "description": "A measure of the degree to which an actor favors their in-group relative to an out-group.", "formula": "COUNT(actor -> exhibits_favoritism_towards -> in_group) - COUNT(actor -> exhibits_derogation_towards -> out_group)", "interpretation": "A higher positive score indicates stronger in-group favoritism." } ] This metric is now an operationalized, computable query. The KGAS can execute this formula on the knowledge graph to assign a favoritism score to every actor, allowing for quantitative analysis. Step 5: Telos (The Theory's Purpose) Finally, we define the theory's primary goal. For SIT, the goal is typically to explain behavior. JSON "Telos": { "analytical_purpose": "Explanatory", "level_of_analysis": "Meso", "output_format": { "type": "object", "properties": { "actor_scores": { "type": "array", "items": {"type": "object"} }, "group_dynamics_summary": { "type": "string" } } }, "success_criteria": "The framework successfully identifies distinct in-groups and out-groups from discourse and generates non-zero favoritism scores that correlate with observed intergroup behavior." } By completing this process, the abstract concepts of Social Identity Theory have been translated into a structured, computable object. The KGAS can now be directed to parse a corpus of text, extract all instances of these entities and connections, and calculate the defined metrics, thereby operationalizing the theory for large-scale analysis. This example demonstrates the feasibility and power of the meta-schema approach for making social theory computationally tractable. 3.7. Expected Contribution The primary contribution of Essay 1 will be the articulation and justification of a comprehensive theoretical architecture for the computational analysis of discourse. This framework: Offers a Novel Typology: It introduces a new, three-dimensional method for organizing the fragmented field of behavioral science, moving beyond static classifications to a functional, process-oriented model. Provides a Computable Blueprint: It details the Theory Meta-Schema, a novel method for translating abstract social theory into a computable format, enabling the flexible application of theory at scale. Justifies the KGAS Architecture: It establishes the set of theoretically grounded requirements for the KGAS, ensuring its design is informed by established science and capable of representing the dynamic processes of influence. In essence, Essay 1 provides the intellectual blueprint for the novel methodological approach developed across the entire dissertation. 4. Essay 2: Development and Demonstration of a Low-code LLM-Powered Knowledge Graph Analysis Tool for Fringe Discourse 4.1. Introduction and Specific Aims of Essay 2 This second essay transitions from the theoretical foundations established in Essay 1 to the practical design, development, and demonstration of the proposed KGAS. The core aim of Essay 2 is to create and evaluate a functional prototype of a "low-code" software tool that empowers Subject Matter Experts (SMEs)—who may not have advanced programming or data science skills—to leverage LLMs and knowledge graph methodologies for the analysis of fringe discourse. This essay directly addresses the overarching research questions outlined in Section 2.3 regarding the feasibility and utility of such a tool. Specific aims include: To design and implement a modular software architecture for the KGAS, incorporating components for LLM-assisted generation of use-case-specific ontology modules, knowledge extraction, data retrieval, and analytical transformation. To develop a user interface that enables SMEs to intuitively define their analytical goals, guide the LLM-driven knowledge graph construction process, and apply analytical operators without needing to write code. To demonstrate the feasibility and core functionalities of the tool using a case study focused on a relevant body of fringe discourse (e.g., discourse surrounding scientific corruption related to COVID-19). To conduct an initial assessment of the tool's usability and its potential to enhance the analytical capabilities of SMEs. To validate the core extraction performance of the prototype by benchmarking its output against a hand-coded "gold standard" dataset, establishing a baseline for its accuracy and reliability before its use in advanced modeling. 4.2. Research Questions and Hypotheses for Essay 2 This essay is designed to provide empirical answers to the core research questions of the dissertation, as stated in Section 2.3: Can a low-code tool effectively integrate LLMs to enable SMEs to generate tailored knowledge graphs from fringe discourse and perform theoretically grounded analyses based on user-defined interrogatives and use-case-specific ontology modules? Will SMEs using the developed tool be able to identify key discourse elements (e.g., actors, narratives, claims, topics), their properties, and their relationships with greater efficiency and comparable or improved accuracy relative to manual qualitative methods or less-tailored automated tools? The KGAS prototype, when guided by a well-defined Theory Meta-Schema, will achieve high fidelity in extracting core theoretical components from text, demonstrated by F1-scores exceeding 0.80 for key entity and relationship extraction tasks when compared against a human-coded gold standard. The development and demonstration activities within this essay will serve to directly test these propositions. 4.3. Comparison to other tools While the KGAS builds on established GraphRAG patterns, its core novelty is not in the basic pipeline but in its specific, theory-driven application. The innovation lies in three areas: (1) Theory-Driven Ontology Construction , which allows the same text to be analyzed through multiple, competing theoretical lenses, unlike data-driven approaches that generate a single graph; (2) Its Explicit Link to a Causal Framework , which transforms the tool from a simple information retrieval system into an instrument for meta-scientific inquiry; and (3) Its Design as an Investigative Workbench , purpose-built to operationalize the concepts and methods of social science. 4.3.1 Retrieval-Augmented Generation (RAG) as a Baseline RAG has emerged as a key technology to mitigate LLM limitations such as hallucination and outdated knowledge . The "Naive RAG" paradigm—indexing documents into chunks, retrieving based on vector similarity, and generating a response—provides a foundational solution for grounding LLM outputs in external text. More "Advanced RAG" techniques improve upon this with pre-retrieval strategies like query expansion and post-retrieval methods like re-ranking. The KGAS incorporates these foundational ideas but is designed to overcome a core limitation of text-based RAG: the "flattening" of knowledge. Standard RAG struggles to represent and reason over the complex, multi-hop relationships inherent in social phenomena, as knowledge is often distributed across many document chunks without explicit connections. 19 Gao, Yunfan, et al. Retrieval-Augmented Generation for Large Language Models: A Survey . Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University; Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University; College of Design and Innovation, Tongji University, 2024. 4.3.2 Extending the GraphRAG Paradigm GraphRAG, as surveyed by Zhang et al. (2025), represents a significant leap forward by using graph structures to address the limitations of traditional RAG. It enhances knowledge representation by capturing explicit entity relationships, enabling more sophisticated, context- preserving retrieval. The KGAS is conceptually a form of GraphRAG, but with three critical distinctions that define its primary contribution: From Data-Driven to Theory-Driven Graph Construction: Most GraphRAG implementations, such as Microsoft's framework, focus on creating a single, comprehensive knowledge graph from a corpus by extracting entities and their semantic relationships. The KGAS, in contrast, does not assume a single, objective graph structure. Instead, its core innovation is the Theory Meta-Schema, which allows it to dynamically construct 20 Zhang, Qinggang, et al. A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models . The Hong Kong Polytechnic University; Jilin University, 2024. multiple, different graph representations of the same data , each one structured according to the specific entities, relationships, and mechanisms of a given social science theory. It moves the graph construction process from a data-mining task to a theory-operationalization task. A Richer Set of Reasoning Operators: The DIGIMON project's breakdown of GraphRAG into a modular set of operators (e.g., Entity Operators, Subgraph Operators) provides a useful vocabulary. The KGAS will implement similar operators, but their selection and execution will be governed by the loaded theoretical framework. For example, a "Diffusion of Innovations" framework would prioritize operators that trace paths through a network, while a "Toulmin Model" framework would prioritize operators that construct logical argument trees. Hybrid Knowledge Integration: The GraphRAG survey identifies "Knowledge-based," "Index-based," and "Hybrid" approaches. The KGAS is fundamentally a Hybrid GraphRAG system. It uses the theoretical framework to create a "knowledge carrier" graph, but each node and edge in this graph remains linked to the original raw text chunks, which serve as an "index." This allows an analyst to move seamlessly from a high-level theoretical model of the discourse down to the specific textual evidence that supports it. 4.3.3 Incorporating Structured Reasoning from StructGPT While GraphRAG provides the blueprint for handling unstructured discourse, the KGAS's internal architecture is heavily inspired by frameworks for reasoning over already-structured data, like StructGPT . StructGPT's Iterative Reading-then-Reasoning (IRR) framework, which uses specialized interfaces to query structured data and an LLM to reason over the results, is a powerful model for human-AI collaboration. 21 Jiang, Jinhao, et al. StructGPT: A General Framework for Large Language Model to Reason over Structured Data . Renmin University of China; University of Electronic Science and Technology of China, 2024. The KGAS adopts and adapts this model. The "interfaces" in the KGAS are not for querying a pre-existing database, but are defined by the loaded Theory Meta-Schema. The process looks like this: Reading (Interface Invocation): The KGAS applies the "Ontology" and "Analytics" portions of a selected theory schema to the raw text, extracting the relevant theoretical components into a structured format. Reasoning (LLM Generation): The LLM then reasons over this newly-structured data to identify patterns, test hypotheses, or generate summaries, guided by the "Process" and "Telos" components of the theory schema. In essence, the KGAS uses its theory-driven methods to first create the structured data that a system like StructGPT is designed to reason over, enabling a far more flexible and powerful form of analysis for messy, real-world discourse. 4.3.4 Synthesis with Traditional Computational Social Science Finally, the KGAS aims to be a unifying framework. Its goal is to integrate the semantic depth of early qualitative methods (like the General Inquirer), the spatial and relational insights of network-based models (like Woelfel's Galileo and Social Network Analysis), and the classification power of modern NLP sentiment and stance analysis (like RAND-Lex) into a single, flexible system, governed by explicit, interchangeable theoretical frameworks.. 22 For an overview of RAND-Lex, see RAND Corporation, “RAND-Lex: A Lexical Analysis Tool,” webpage, undated. As of May 28, 2025: https://www.rand.org/topics/rand-lex.html 4.4. Data Sources and Collection Methods (Case Study: Scientific Corruption/COVID Discourse) To demonstrate the KGAS's capabilities, this research has several rich, multi-modal datasets. This multi-dataset approach allows for robust validation and demonstrates the framework's flexibility across different discourse communities and data types. The COVID-19 dataset is the primary dataset/case study. Other datasets will be used if time permits Case Study 1: COVID-19 Discourse Primary Dataset 23 Kunst, J.R., Gundersen, A.B., Krysińska, I. et al. Leveraging artificial intelligence to identify the psychological factors associated with conspiracy theory beliefs online. Nat Commun 15, 7497 (2024). https://doi.org/10.1038/s41467-024-51740-9 : This unique dataset provides a powerful opportunity to bridge psychological and behavioral analysis. Components: Survey Data (N=2,506): Rich psychometric data on U.S.-based Twitter users, including measures for need for chaos, narcissism, conspiracy mentality, and misinformation susceptibility. Behavioral Data: Over 7.7 million Twitter engagements (likes, posts, replies, reposts) from these same 2,506 users, collected via the Academic Twitter API from Dec 2019 - Dec 2021. Analytical Use: This dataset is ideal for explanatory and predictive analysis . It allows the KGAS to be used to test hypotheses about how pre-existing psychological traits (structured data) correlate with and predict engagement with specific conspiracy narratives online (unstructured data). Case Study 2: Fringe Communities and UFO Discourse This case study will analyze the distinct narrative structures of UFO discourse, a topic that has seen a significant resurgence in public and governmental interest. Available Datasets: AboveTopSecret.com Archive: A comprehensive web-scraped archive of AboveTopSecret.com, formerly one of an extensive forum for conspiracy-related discussions. This provides a deep, longitudinal view of community narratives. Brandwatch Reddit Corpus: Extensive data downloads from relevant subreddits (e.g., r/UFOs, r/aliens), capturing community discussions, debates, and evidence sharing. Brandwatch Twitter Corpus: Two-fold Twitter data collection: 1) Broad keyword searches for general UFO discourse, and 2) Targeted collection of all interactions from a hand-curated list of prominent accounts in the UFO community on Twitter. Analytical Use: These datasets are ideal for descriptive and explanatory analysis . They will be used to demonstrate the KGAS's ability to extract and map the evolution of complex narrative structures, identify key information brokers, and analyze the community's response to external events like government reports. Data Handling and Pre-processing: All datasets will be ingested into the KGAS. Pre-processing will involve text cleaning, data filtering to remove noise and non-English content, and bot detection where applicable. All data will be handled in accordance with the ethical guidelines outlined in Section 6. 4.5. Methodology: Tool Development and Demonstration The development of the KGAS will be iterative, using Python and a web-based framework for the low-code UI. 4.5.1. Proposed Technology Stack To ensure a feasible and robust development process for the proof-of-concept prototype, the KGAS will be built using a modern, well-supported technology stack grounded in the Python data science ecosystem. Backend and Analytical Engine: The core application logic will be developed in Python 3.11+. Key libraries will include LangChain or a similar framework for orchestrating interactions with Large Language Models, spaCy for foundational natural language processing tasks, and NetworkX for graph-based computations and analytics (e.g., centrality, pathfinding). Knowledge Graph Database: Given the graph-centric nature of the methodology, a native graph database is required. The primary candidate is Neo4j, which uses the Cypher query language and is highly optimized for the complex, multi-hop relational queries essential for this project. Large Language Model (LLM) Integration: The system will integrate with a state-of-the-art large language model via its API to perform the structured data extraction defined by the Theory Meta-Schema. The initial development will target the OpenAI API, utilizing models such as GPT-4 for their advanced instruction-following and reasoning capabilities. User Interface (UI) for the "Analyst-in-the-Loop": To facilitate rapid development and create a functional interface for SME evaluation, the prototype will be built using a Python-based web framework like Streamlit or Plotly Dash. These frameworks are designed specifically for creating interactive data science applications and will allow for the intuitive presentation of the knowledge graph and analytical outputs without the overhead of traditional front-end web development. This technology stack is chosen to maximize development speed for the proof-of-concept while ensuring the underlying methods are scalable and computationally sound. 4.5.2. Knowledge Graph System Architecture: The system will follow a modular architecture, visualized internally as a Directed Acyclic Graph (DAG) processing pipeline: 24 A Directed Acyclic Graph (DAG) is a directed graph with no directed cycles. That is, it consists of vertices and edges (also called arcs), with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently directed sequence of edges that eventually loops back to v again. Policy Question & Interrogative Definition: The user defines their policy analysis goal and selects relevant interrogatives (e.g., "Who says what to whom?" based on Lasswell, further refined using framing concepts to scope the analysis, akin to how Druckman discusses the focusing effects of frames). 25 The reference to Druckman here pertains to his work on framing, which emphasizes how the selection and highlighting of certain aspects of an issue can shape understanding and subsequent analysis, thereby providing a way to scope analytical inquiry. See James N. Druckman, “The Implications of Framing Effects for Citizen Competence,” Political Behavior, Vol. 23, No. 3, September 2001, pp. 225–256. LLM-Assisted Generation of Use-Case-Specific Ontology Modules: Based on interrogatives and sample text, the LLM suggests entity types, relationship types, and properties. The SME reviews, refines, and approves this module. LLM-Driven Extraction to Knowledge Graph: The LLM processes the input text, extracting instances defined in the ontology module. Application of Retrieval and Transformation Operators: The user applies operators to query the KG and process retrieved data for descriptive and basic predictive transformations. 4.5.3. LLM Integration for Coding and Analysis: LLMs will be integrated via APIs. Ontology Assistance: LLMs parse user queries/text samples to suggest elements for the use-case-specific ontology module. Information Extraction: Guidance provided through analyst-defined templates and structured inputs will direct LLMs to extract information according to the ontology module. Analytical Assistance: LLMs may assist in translating natural language queries or summarizing results. Transparency: The tool will aim to provide insight into LLM-generated extractions where feasible. 4.5.3. Application of Descriptive and Predictive Transformations (for Demonstration): The demonstration will focus on applying transformations to generate initial insights from the "Scientific Corruption (COVID)" dataset: Descriptive Transformations: to_statistical_distribution: Generating statistics on the frequency of topics, prevalence of certain claims, distribution of sentiment scores associated with entities or narratives, or engagement metrics. to_graph_projection: Creating network visualizations of actors and their relationships, co-occurrence of concepts, or the structure of argument networks (similar to the "Discourse Network of 50 Sample Tweets" example from the PowerPoint). summarize_narratives: Using LLMs to generate summaries of dominant narratives or themes within specific communities or time periods. Basic Predictive Transformations (Proof-of-Concept): predict_edge_weight (simplified): Estimating the likelihood of continued interaction between two actors based on past interaction frequency and recency, or simple trend extrapolation of topic/narrative engagement. identify_emerging_terms: Highlighting terms or n-grams whose frequency is rapidly increasing, potentially indicating emerging narratives. 4.5.4. Capturing Data for Bayesian Cognitive Modeling While the application of complex Bayesian cognitive modeling is a primary goal of Essay 3, the methodology in Essay 2 ensures the Knowledge Graph Analysis System captures the necessary precursors for such analysis. This involves: Representing Uncertainty: Associating LLM-extracted entities, properties (e.g., stance, sentiment), and relationships with confidence scores or probability distributions where provided by the LLM, acknowledging the inherent uncertainty in automated interpretation. Extracting Belief Indicators: Designing the LLM extraction process and mini-ontologies to explicitly identify and store elements crucial for later cognitive modeling. This includes extracting: Stated beliefs or claims made by actors. Actors' expressed stances or attitudes towards specific claims or topics, which can serve as proxies for prior beliefs. Instances of actors being exposed to specific pieces of information or evidence within the discourse. Features of messages or sources that might inform the perceived likelihood or credibility of information (e.g., source reputation markers, use of evidence). Structuring for Input: Ensuring the KG structure organizes this information in a way that facilitates its use as inputs (e.g., prior probabilities, likelihoods based on evidence) for the Bayesian cognitive modeling planned in Essay 3. The focus in Essay 2 is on correctly extracting and structuring these elements. 4.5.5: Usability and Value Assessment Plan A central hypothesis of this dissertation is that the KGAS can provide significant value to Subject Matter Experts (SMEs). The assessment plan is therefore designed to evaluate both the tool's usability and its ability to generate novel, valuable insights compared to existing methods. The primary strategy involves engaging SMEs by applying the KGAS to datasets they have previously analyzed or are deeply familiar with, allowing for a direct comparison of the tool's outputs with their own expert judgment. Process: Identification: A list of potential SMEs, both internal and external to RAND, will be identified based on their expertise in relevant areas (e.g., discourse analysis, specific fringe topics). I will aim to engage with an initial cohort of 3-5 SMEs for this formative assessment. Engagement: Following an introductory meeting to demonstrate the KGAS and its goals, participating SMEs will be given access to a secure, web-based UI of the tool. Task: SMEs will be asked to use the tool to explore the designated dataset over a set period, performing tasks relevant to their expertise (e.g., "Identify the main counter-arguments to this narrative," "Visualize the key influencers on this topic," "Assess the evolution of this theme over time"). Feedback: Feedback will be gathered through semi-structured interviews and qualitative observation, focusing on efficiency (time to insight), accuracy (comparison to their own knowledge), and novelty (discovery of new patterns). Ethical Considerations: All SME engagement, particularly with external participants, is contingent on formal review and approval by the Human Subjects Protection Committee (HSPC), as detailed in Section 6. 4.5. 6. Core System Validation Plan A fundamental prerequisite for the analyses in Essay 3 is ensuring the reliability of the knowledge graph generated by the KGAS. This section outlines the plan for the rigorous validation of the prototype's core LLM-driven extraction pipeline. A Focused Validation Strategy The proposal acknowledges that creating a "gold standard" dataset for complex, subjective social science concepts (e.g., "narratives," "causal claims") is a non-trivial challenge where achieving high inter-coder reliability is difficult. Therefore, to ensure a robust and feasible validation process, this dissertation will focus its quantitative benchmarking on the most concrete and objectively verifiable extraction tasks. The validation will concentrate on the foundational layer of the knowledge graph: its core entities and the sentiment expressed toward them. Gold Standard Dataset Creation A corpus of 300–500 documents (e.g., social media posts) will be randomly sampled from the primary case study dataset. Two independent coders will be trained on a detailed codebook to manually annotate the following specific items: Named Entities: The identification and categorization of key actors (e.g., PERSON, ORGANIZATION). Explicit Stance: The classification of an actor's expressed sentiment (positive, negative, neutral) toward a specific, explicitly mentioned named entity. This focus on concrete entities and expressed stance, rather than abstract propositional claims, provides a more reliable basis for achieving high inter-coder reliability. Reliability will be measured using Cohen’s Kappa, with a target of κ > 0.80 required to establish the dataset as a valid ground truth for benchmarking. Benchmarking and Metrics The KGAS extraction pipeline will be run on the same hand-coded dataset. The system's output for the targeted tasks (entity extraction and stance classification) will be systematically compared against the gold standard. Performance will be measured using standard information retrieval metrics: Precision: Of the items the system extracted, what proportion were correct? Recall: Of all the correct items available in the text, what proportion did the system extract? F1-Score: The harmonic mean of Precision and Recall, providing a single, robust measure of overall accuracy. This validation step ensures that the foundational layer of the knowledge graph is built on a demonstrably reliable footing. The validation of more abstract, higher-order concepts like "narrative structure" is not part of this quantitative benchmark; instead, those will be evaluated qualitatively in Essay 3 through methods like the convergent analysis described in Section 5.4. This sequential approach directly mitigates a primary research risk of the dissertation. 4.6. Expected Outcomes and Contribution of Essay 2 The successful completion of Essay 2 is expected to yield: A Functional Prototype: An operational version of the low-code LLM-powered Knowledge Graph Analysis Tool, capable of performing the core functions described. Prateek Puri 2025-06-15T18:38:00 PP Can you give more details on your evaluation strategy? For example for the descriptive statistics, do you have a hold-out test set of pre-coded statements you can run through your system to evaluate its performance? Can you also provide details on how you will evaluate the narrative summaries and network visualizations? Demonstration of Feasibility: Evidence from the case study application that the tool can effectively process real-world fringe discourse data and generate initial analytical outputs (descriptive statistics, network visualizations, narrative summaries). Prateek Puri 2025-06-15T18:40:00 PP Again can you provide more details on what this study will look like, how many SMEs you will be able to work with, what dataset they will be looking at, etc. Usability Insights: Findings from the formative usability assessment, providing data on how SMEs interact with the tool and identifying areas for future refinement. Methodological Contribution: This essay will provide a concrete implementation of a novel approach to discourse analysis, showcasing how LLMs and KGs can be synergistically combined in a user-friendly manner. It provides the practical foundation for the more advanced explanatory analyses to be conducted in Essay 3. Foundation for Further Research: The tool itself, and the findings from its development, will serve as a basis for further research into human-AI collaboration for complex analytical tasks and into the dynamics of online fringe discourse. This essay is critical as it translates the theoretical framework of Essay 1 into a tangible analytical instrument, paving the way for its application to complex policy-relevant questions in Essay 3. 5. Essay 3: Explanatory Analysis and Intervention Modeling in Fringe Discourse using the Knowledge Graph System 5.1. Introduction and Specific Aims of Essay 3 Building upon the KGAS prototype developed and its core extraction accuracy validated in Essay 2, this third essay aims to apply the tool to conduct more advanced, in-depth analyses. The primary focus shifts from system development and basic description towards identifying underlying mechanisms and processes of discourse dynamics and exploring potential policy interventions. This essay seeks to showcase the system's full potential for generating deep, policy-relevant insights from complex fringe discourse by focusing on the interplay of communication, cognition, and behavior. Specific aims include: To utilize the KGAS to investigate and model the underlying mechanisms and processes driving the propagation and impact of key narratives within the chosen fringe discourse case study. To integrate data from the knowledge graph with social-behavioral (SB) theories to build models that illuminate these discourse effects. To explore and simulate the potential effects of different types of interventions (e.g., counter-messaging strategies, platform policy changes) on the observed discourse dynamics. 5.2. Research Questions for Essay 3 This essay will address the following research questions: What temporal or structural mechanisms and processes (identified via the KGAS and SB models) appear to explain the propagation and impact of specific narratives within the chosen fringe discourse, and what alternative mechanisms or processes might also be at play? How can the KGAS be used to model and evaluate the potential effects of different policy interventions on the observed discourse dynamics and associated communication, cognition, and behavior? 5.3. Data Sources (Building on Essay 2 Dataset) This essay will primarily utilize the knowledge graph populated and refined during the case study in Essay 2 (e.g., discourse surrounding "Scientific Corruption (COVID)"). Depending on the specific mechanisms or processes being investigated or the interventions being modeled, this dataset may be augmented by: extending the timeframe of data collection to observe longer-term trends or lagged effects; incorporating data from additional, related online sources if key actors or narratives span multiple platforms; and integrating publicly available offline data (e.g., policy announcements, real-world event timelines) to contextualize discourse dynamics and explore potential correlations or temporal relationships between online talk and offline events. It is important to clarify the analytical goals of this essay. The objective is not to make definitive, generalizable causal claims about offline behavior. Rather, it is to demonstrate a methodology for building empirically-grounded models that can generate plausible, testable hypotheses about the dynamics within a specific online ecosystem. The models describe and simulate the online portion of a complex system, and all findings will be interpreted within this deliberately bounded context. 5.4. Validation and Reliability Strategy A set of complementary strategies will be used to ensure the validity and reliability of the KGAS's outputs and, more importantly, to apply the framework as a tool for meta-scientific inquiry into the robustness of theories. This approach shifts the focus from merely validating the tool to using it as a means of investigating theory itself. Model and Extraction Validation: The accuracy of the core system will be rigorously tested. Ground-Truth Comparison: For core tasks like entity and claim extraction, a "gold standard" dataset of several hundred posts will be hand-labeled by two independent coders. The KGAS's LLM-driven extraction performance will be measured against this dataset using standard metrics (Precision, Recall, F1-score). Complex Query Validation: The system's ability to answer complex, multi-step questions will be assessed using tasks inspired by benchmarks like HotPotQA , which require reasoning over multiple pieces of information. This will test the integrity of the full analysis pipeline. Qualitative Summary Validation: The narrative summaries generated by the summarize_narratives operator will be presented to SMEs for face validity assessment. They will be asked to rate the summaries on accuracy, coherence, and completeness compared to their own reading of the source material. Theory Robustness and Convergent Analysis: A novel contribution of this dissertation is using the KGAS to test the robustness of social-scientific theories. Replication of Academic Findings: For select theories codified in the Theory Meta-Schema, the KGAS will be applied to the same datasets used in the original source publication. The results generated by the KGAS will be compared against the published findings to validate the fidelity of the theory implementation. Convergent Analysis: The same policy question (e.g., "What factors are driving the spread of this narrative?") will be analyzed through the lens of multiple, different theoretical frameworks (e.g., Social Identity Theory, Elaboration Likelihood Model, and a network-based contagion model). The analysis will focus on identifying where the theories produce convergent results (strengthening confidence in the finding) and where they produce divergent results (highlighting the unique perspective and limitations of each theory). 5.5. Explanatory and Modeling Approach The analytical approach for Essay 3 leverages the advanced capabilities of the KGAS to move beyond description and toward explanation and intervention. The knowledge graph, validated in Essay 2, serves as the foundation for a multi-stage analysis. This begins with direct explanatory queries and can be extended to more advanced computational modeling techniques that simulate influence dynamics. The following sections outline this approach, presenting Agent-Based Modeling and Bayesian Cognitive Modeling as two powerful, illustrative examples of the types of analysis the KGAS is designed to support. 5.5.1. Application of Explanatory Transformations:  The system's transformation operators designed for explanation (e.g., map_to_sb_model ) will be employed.  The knowledge graph will be queried to extract patterns, sequences, and co-occurrences of discourse elements (e.g., claims, evidence, emotional expressions, actor endorsements) that precede or co-occur with narrative amplification or shifts in sentiment. These patterns can be analyzed to understand how different elements of communication, cognition, and behavior interact at both micro (individual message/actor) and macro (community/narrative) levels.  These patterns will be mapped to established SB models to hypothesize how observed discourse features might influence beliefs, attitudes, and intentions. For instance, identifying how the framing of messages aligns with known persuasive strategies. The KGAS can also assist in mapping semantic structures and belief systems, drawing on techniques conceptually similar to Multidimensional Scaling (MDS) approaches like the Galileo system, to visualize proximity and relationships between concepts within the discourse. 26 Joseph Woelfel and Edward L. Fink, The Measurement of Communication Processes: Galileo Theory and Method , Academic Press, 1980. This approach allows for the spatial representation of concepts based on their perceived similarity or relatedness in each corpus  Techniques inspired by Qualitative Comparative Analysis (QCA) or process tracing 27 Charles C. Ragin, The Comparative Method: Moving Beyond Qualitative and Quantitative Strategies , University of California Press, 1987. QCA is a method used to analyze configurations of conditions that lead to a specific outcome could be adapted to identify configurations of conditions within the KG that are consistently associated with specific outcomes (e.g., a narrative going viral). Process tracing, for example, 28 George and Bennett, Case Studies and Theory Development in the Social Sciences , 2005 can be used to identify sequences in discourse patterns that appear to link specific communication strategies to changes in expressed sentiment or belief). 5.5.2. Advanced Modeling and Simulation (Exemplars ): Prateek Puri 2025-06-16T10:26:00 PP Would be nice to reference Google Concordia or the LLM-ABM work from Joog Sung Park and talk about issues in the field that you hope to address with your work A key contribution of the KGAS is its ability to structure discourse data in a way that makes it suitable for advanced computational modeling. To demonstrate this capability, this dissertation will show how the knowledge graph can serve as an input for powerful modeling paradigms. The goal is to provide a proof-of-concept for how the KGAS enables these methods, rather than to conduct an exhaustive implementation of each. The primary contribution here is the demonstration of a novel pipeline for moving from unstructured discourse (via the KGAS) to a parameterized ABM. The resulting simulation is a first-order approximation intended to prove the viability of this data-to-model method, not to be a perfectly calibrated predictive tool. A. Illustrative Application 1: Agent-Based Modeling (ABM) This approach builds on recent breakthroughs in creating believable, LLM-driven social simulations 29 Park, J. S., et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23) (pp. 759-789). Association for Computing Machinery. . A primary contribution of this dissertation is demonstrating a method for grounding agent psychology in large-scale, real-world discourse. A potential workflow for this parameterization, using the Kunst dataset 30 Vezhnevets, A. S., et al. (2023). Generative Agent-Based Modeling with Actions Grounded in Physical, Social, or Digital Space Using Concordia. arXiv. https://arxiv.org/abs/2312.03857 , could be as follows: 31 Kunst, J. R., Gundersen, A. B., Krysińska, K., Cichocka, A., & Pärnamets, P. (2024). Leveraging artificial intelligence to identify the psychological factors associated with conspiracy theory beliefs online. Nature Communications, 15(1), 7497. Empirical Agent Archetype Definition: Agent archetypes would be defined empirically using the offline survey data available for each user. This provides a robust, data-driven basis for agent differentiation. For instance, archetypes could be defined based on validated psychometric scales: High-Conspiracy Actor: A user scoring in the top quartile on the "conspiracy mentality" scale. Low-Conspiracy Actor: A user scoring in the bottom quartile on the same scale. Belief State and Network Initialization: For each archetype, the KGAS would analyze the discourse produced by its members. The initial probability of an agent believing a specific claim could then be set to the proportion of users within that archetype who expressed support for that claim in the knowledge graph. The agent interaction network in the ABM would be initialized to mirror the observed reply/retweet network structure from the KGAS. Behavioral Rule Definition: Behavioral rules would be defined as simple, probabilistic functions based on observed frequencies. For example, a rule could state: "If an agent of type High-Conspiracy receives a message from another High-Conspiracy agent, there is a P(Share | H, H) probability they will share it," where this probability is calculated from the observed frequencies in the KGAS. Once parameterized, this ABM could then be used to simulate narrative diffusion and model the potential impact of interventions, demonstrating the KGAS's utility for the analytical purpose of Intervention . B. Illustrative Application 2: Bayesian Cognitive Modeling As a second potential application, the KGAS is designed to capture the necessary precursors for the formal modeling of belief revision. This approach focuses on how an individual or group might update their beliefs in light of new evidence. The KGAS enables this by systematically extracting and structuring the key inputs for Bayesian models, such as: Priors: An actor's initial expressed stance on a topic, which can serve as a proxy for their prior belief probability. Evidence: A specific claim or piece of information to which an actor is exposed. Likelihoods: Features of the message or its source (e.g., source credibility, use of data) that can inform the perceived strength of the evidence. This demonstrates the KGAS's utility for the analytical purpose of Explanation , by providing the structured data needed to apply Bayesian frameworks to track and explain belief trajectories. 5.5.3. Bayesian Cognitive Modeling and Validation A core analytical approach in Essay 3 will involve applying Bayesian models Prateek Puri 2025-06-15T18:52:00 PP Is it a research risk that offline behavior may drive a substantial fraction of attitudinal shifts, meaning the KGAS will not have access to critical information needed for hypothesis generation? If so, are there other self-contained datasets where casual factors for behavior/attitudinal shifts are more directly represented? to understand belief updating dynamics as reflected in the discourse. Modeling Belief Revision: Utilizing data extracted into the KG during Essay 2 to formally model belief updating (e.g., using Bayes' Theorem). Tracking Belief Trajectories: Analyzing longitudinal data within the KG to track how expressed beliefs or stances of individuals or groups evolve. Integrating with Network Analysis: Examining how belief updates might correlate with an actor's position in the network. Validation Approaches: Validating the models by comparing their predictions of belief shifts against subsequent observed expressions of belief or stance in the discourse data. Further validation will involve comparing outputs and insights from the KGAS with those potentially derivable from other analytical tools or approaches, such as RAND-Lex, where applicable. SME judgment and qualitative coding comparisons on subsets of data will also be employed to assess the plausibility and richness the KGAS-derived explanations. 5.6. Expected Outcomes and Contribution of Essay 3: Policy-Relevant Explanatory Insights and Intervention Scenarios The anticipated outcomes and contributions of this essay include: Policy-Relevant Explanatory Insights: A deeper, evidence-based understanding of the mechanisms and processes appearing to drive the spread and impact of specific fringe narratives. Intervention Scenarios and Evaluations: Simulated evaluations of plausible policy interventions. Validated Advanced Analytical Capabilities: A robust demonstration of the KGAS's utility for conducting sophisticated explanatory analyses. Contribution to SB Theory and Policy Practice: This essay will contribute by showcasing an integrated methodology that bridges rich, context-sensitive discourse data with formal SB modeling techniques. This essay will aim to deliver not just academic findings but also practical considerations for policymakers grappling with the challenges posed by online fringe discourse.