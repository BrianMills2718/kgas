# KGAS Validation Framework
## Based on Davis's 5-Dimensional Model Validity Framework

---

## ORGANIZING PRINCIPLE: Davis's Multi-Dimensional Validity Assessment

Per Davis et al. (2018), model validity should be assessed separately along five dimensions:
1. **Description**: Identify salient structure, variables, and processes
2. **Causal Explanation**: Identify causal variables and processes
3. **Postdiction**: Explain past behavior quantitatively with causal theory
4. **Exploratory Analysis**: Identify and parameterize causal variables for exploration
5. **Prediction**: Predict system behavior accurately (rare in social science)

**Key Insight**: Different models in a family will have different validity profiles. LLM extraction might excel at description but be weak at prediction. Statistical models might predict well but explain poorly.

---

## VALIDATION STRUCTURE

### Core Framework: 14 Uncertainty Dimensions × Validation Methods

**Uncertainty Dimensions** = What the system needs to assess about evidence/theories
**Validation Methods** = Ways to test if the system assesses each dimension correctly

### The 14 Uncertainty Dimensions (WHAT to validate):

1. **Source Credibility** - Can system assess source quality?
2. **Cross-Source Coherence** - Can system detect agreement/contradiction?
3. **Temporal Relevance** - Can system assess how evidence ages?
4. **Extraction Completeness** - Can system capture all relevant information?
5. **Entity Recognition** - Can system identify entities accurately?
6. **Construct Validity** - Can system assess if measures are valid?
7. **Theory-Data Fit** - Can system determine if theory matches data?
8. **Study Limitations** - Can system identify methodological limits?
9. **Sampling Bias** - Can system detect non-representative samples?
10. **Diagnosticity** - Can system assess evidence relevance?
11. **Sufficiency** - Can system determine if enough evidence exists?
12. **Confidence Calibration** - Are confidence levels aligned with accuracy?
13. **Entity Identity Resolution** - Can system determine when different mentions refer to same entity?
14. **Reasoning Chain Validity** - Can system correctly trace multi-step reasoning chains?

### Validation Methods (HOW to test):

**Methods that can apply to multiple dimensions**:
- **Inter-LLM Agreement**: Multiple models assess same content
- **Human Expert Comparison**: Expert ratings as ground truth
- **Crowd Coding**: Mechanical Turk for simple tasks
- **Ground Truth Datasets**: Where objective truth exists
- **Correlation Analysis**: Compare to validated measures

---

## VALIDATION CATEGORIES

### Category 1: System Validation vs Theory Validation

**Critical Distinction**:
- **Theory Validation**: Does the 3-degree influence theory actually hold in social networks? (NOT our focus)
- **System Validation**: Can KGAS correctly extract and apply the 3-degree influence theory? (OUR focus)

We validate whether the system can:
- Extract theories correctly from papers
- Apply theories as specified
- Produce results consistent with the theory's logic

We do NOT validate whether the theories themselves are "true" - that's a different research question.

### Category 2: Validation vs Calibration

**Validation**: Testing if system assessments match reality (our focus)
**Calibration**: Making models internally consistent (engineering task, not validation)

---

## VALIDATION METHODS BY DAVIS DIMENSION

### 1. DESCRIPTION VALIDITY
*"Can the system identify salient structure, variables, and processes?"*

**Validation Methods**:

a) **Entity Extraction Accuracy**
- Method: Crowd worker coding of 500 posts for entities and sentiment
- Metrics: Precision, Recall, F1-scores
- Baseline: Compare to existing NER benchmarks (CoNLL-2003)
- Target: Report actual F1 scores, no predetermined targets

b) **Inter-LLM Reliability** 
- Method: Extract same theory with GPT-4, Claude, Gemini
- Metric: Agreement score across models
- Limitation: Shows consistency, NOT accuracy (all could be wrong similarly)
- Purpose: Demonstrates reproducibility of extraction

c) **Theory Structure Completeness**
- Method: Hand-code 5-10 theories using V13 meta-schema
- Metric: Graph edit distance, concept overlap (Jaccard similarity)
- Report: "System captured X% of concepts, Y% of relationships"

### 2. CAUSAL EXPLANATION VALIDITY
*"Can the system identify causal variables and processes?"*

**Validation Methods**:

a) **Causal Relationship Detection**
- Test: Present "X causes Y" vs "X correlates with Y" statements
- Metric: Accuracy in distinguishing causation from correlation
- Baseline: Use established causal inference test sets

b) **Directionality Accuracy**
- Test: Extract causal chains from papers
- Metric: % of causal directions correctly identified
- Validation: Compare to manually coded ground truth

c) **Mechanism Extraction**
- Test: Can system extract intermediate causal steps?
- Example: "Trust � [risk reduction] � cooperation"
- Metric: Completeness of causal chain extraction

### 3. POSTDICTION VALIDITY
*"Can the system explain past behavior using extracted theories?"*

**Validation Methods**:

a) **Theory Replication Test**
- Method: Extract theory � Apply to paper's original dataset � Compare findings
- Papers: Select 3-5 papers with publicly available data
- Metrics: 
  - Structural similarity (same key variables identified?)
  - Direction agreement (same positive/negative relationships?)
  - Effect size correlation (how close are coefficients?)
- Report: "Replicated X of Y key findings"

b) **COVID Construct Correlation**
- Dataset: 2,506 Twitter users with psychological profiles
- Method: Extract constructs from tweets � Correlate with self-reported measures
- Measures: Conspiracy mentality, narcissism, need for chaos
- Metric: Pearson correlations (report actual r values, no targets)
- Success: Any significant correlations demonstrate meaningful extraction

### 4. EXPLORATORY ANALYSIS VALIDITY
*"Can the system identify and parameterize causal variables for exploration?"*

**Validation Methods**:

a) **Parameter Extraction**
- Test: Does system identify which variables can be varied?
- Validation: Compare to paper's stated independent variables

b) **Multi-Resolution Consistency**
- Test: Analyze same phenomenon at different aggregation levels
  - Individual: User-level construct scores
  - Network: Community-level patterns
  - Population: Overall trends
- Metric: Qualitative assessment - "Patterns coherent across levels: Yes/No"
- Example: Do high-conspiracy individuals cluster in networks?

c) **Cross-Modal Value Assessment**
- NOT about agreement but different capabilities:
  - Graph enables: Path analysis, centrality, community detection
  - Table enables: Regression, correlation, statistical tests
  - Vector enables: Similarity, clustering, semantic distance
- Validation: Document which modality provides which insights
- No quantification of "added value" needed

### 5. PREDICTION VALIDITY
*"Can the system predict future behavior?"*

**Note**: Per Davis, few social science models achieve predictive validity. Not a failure if ours doesn't.

**Validation Methods**:

a) **Time Series Holdout**
- Method: Hide final 20% of temporal data
- Test: Use extracted theories to predict hidden portion
- Metric: Correlation between predicted and actual
- Report: "Predictions correlated at r=X with actual outcomes"

b) **Cross-Dataset Prediction** (if feasible)
- Extract theory from one dataset
- Apply to different but related dataset
- Report predictive accuracy without targets

---

## SYSTEM CAPABILITY ASSESSMENT (14 Dimensions)

These test the system's ability to assess evidence quality, NOT the evidence itself:

### Dimension Validation Methods:

1. **Source Credibility Assessment**
   - Can LLM accurately judge source quality?
   - Test: Present high/low quality sources, compare ratings to expert assessment

2. **Cross-Source Coherence Detection**
   - Can system identify when sources agree/disagree?
   - Test: Feed contradictory sources, check if contradictions flagged

3. **Temporal Decay Estimation**
   - Can LLM accurately estimate how evidence certainty should decay?
   - Test: Present old/new evidence, compare decay estimates to expert judgments

4. **Extraction Completeness**
   - % of human-identified concepts captured
   - Standard metric, well-established

5. **Entity Recognition Accuracy**
   - Standard NER metrics (precision/recall/F1)
   - Benchmark: CoNLL-2003 or similar

6. **Construct Validity Assessment**
   - Can system judge if constructs are valid?
   - Test: Present valid/invalid operationalizations, check detection

7. **Theory-Data Fit Assessment**
   - Can system determine if theory matches data patterns?
   - Method 1: Check if predicted relationships exist in data
   - Method 2: Check if theory-predicted patterns appear

8. **Limitation Detection**
   - Can system identify unstated limitations?
   - Test: Papers with known but unstated limitations

9. **Sampling Bias Detection**
   - Can system identify non-representative samples?
   - Test: Present biased/unbiased samples, check detection

10. **Diagnosticity Assessment**
    - Can system judge if evidence supports conclusions?
    - Test: Present relevant/irrelevant evidence for claims

11. **Sufficiency Determination**
    - Can LLM assess if there's enough evidence?
    - Test: Vary evidence quantity, compare to expert judgments

12. **Confidence Calibration**
    - Do confidence levels match actual accuracy?
    - Test: Compare stated confidence to performance across many cases

13. **Entity Identity Resolution**
    - Can system determine when different mentions refer to same entity?
    - Test: Papers with author name variations, same concepts different terms
    - Method: Hand-code resolutions, entity linking benchmarks

14. **Reasoning Chain Validity**
    - Can system correctly trace multi-step reasoning chains?
    - Test: HotPotQA benchmark, hand-traced chains from papers
    - Method: Multi-hop reasoning tests, expert validation

---

## ADDITIONAL VALIDATION APPROACHES

### HotPotQA for Multi-Hop Reasoning
- **Purpose**: Test complex theoretical connections across sources
- **Implementation**: Extract theories � Build knowledge graph � Test multi-hop queries
- **Metric**: F1 score against HotPotQA gold standard
- **Value**: Tests retrieval and reasoning capabilities

### Agent-Based Model Validation
- **Calibration** (not validation): Match network structure, behavioral patterns
- **Validation**: Compare simulation outcomes to held-out behavioral data
- **Note**: Keep calibration and validation phases distinct

### Leveraging Existing Benchmarks
- **Sentiment Analysis**: Reference existing studies (e.g., "BERT achieves 94% on Twitter")
- **Entity Recognition**: Use established benchmarks
- **Efficiency**: No need to re-validate established capabilities

### Small-Scale Human Evaluation
- **Hand-Coding Comparison**: 5-10 theories coded by researcher
- **Mechanical Turk**: Simple tasks (sentiment, entity identification)
- **Cost-Effective**: ~$200-300 for good coverage

---

## IMPLEMENTATION STRATEGY

### Phase 1: Description & Explanation Validity (Weeks 1-3)
- Entity extraction accuracy (F1 scores)
- Causal relationship detection
- Inter-LLM reliability testing
- Hand-coded theory comparison (5-10 theories)

### Phase 2: Postdiction Validity (Weeks 4-7)
- Theory replication studies (3-5 papers with data)
- COVID construct correlations
- Historical pattern explanation

### Phase 3: Exploratory Validity (Weeks 8-9)
- Parameter identification accuracy
- Multi-resolution consistency checks
- Cross-modal capability documentation

### Phase 4: System Capability Assessment (Weeks 10-11)
- 14-dimension evidence quality assessment
- Confidence calibration testing
- Limitation detection validation
- Entity identity resolution testing
- Reasoning chain validation

### Phase 5: Prediction Validity (Week 12, if time permits)
- Time series holdout tests
- Note: Low expectations per Davis framework

---

## KEY PRINCIPLES

### 1. No Predetermined Targets
- Report actual performance metrics
- Let empirical results speak for themselves
- Proof-of-concept doesn't require hitting specific thresholds

### 2. Mixed Methods Approach
- **Quantitative where established**: F1 scores, correlations, NER metrics
- **Qualitative where appropriate**: Multi-resolution consistency, perspective handling
- **Avoid spurious precision**: Don't quantify what can't be meaningfully measured

### 3. Clear Category Distinctions
- System validation ` Theory validation
- Validation ` Calibration
- Different modalities enable different analyses (not about agreement)

### 4. Realistic Scope
- 500 posts for crowd coding (manageable but meaningful)
- 2,506 users (sufficient statistical power)
- 5-10 hand-coded theories (feasible for researcher)

### 5. Documentation Focus
- Show what each modality enables
- Document patterns found at each resolution
- Report correlations without claiming causation

---

## LIMITATIONS TO ACKNOWLEDGE

### Inherent Limitations
1. **Online discourse only**: Cannot validate offline behavior
2. **Correlation not causation**: System finds patterns, not causal proof
3. **Platform-specific**: Twitter/Reddit findings may not generalize
4. **Theory coverage**: 20+ theories, not exhaustive

### Validation Limitations
1. **Inter-LLM reliability**: Consistency ` accuracy
2. **Crowd coding**: Limited to simple tasks
3. **Prediction**: Social science rarely predicts well (per Davis)
4. **Ground truth**: Often unavailable for complex constructs

### Measurement Limitations
1. **No "true" validity score**: Multi-dimensional assessment only
2. **Context-dependent**: Validity varies by use case
3. **Qualitative aspects**: Some dimensions resist quantification

---

## EXPECTED OUTCOMES (Without Targets)

### Likely Strong Performance
- Entity extraction (established NLP task)
- Description validity (LLMs excel at this)
- Inter-LLM reliability (high consistency expected)

### Moderate Performance Expected
- Causal explanation (challenging for any system)
- Postdiction (depends on theory complexity)
- Construct correlations (r > 0.15 would be meaningful)

### Uncertain/Weak Performance Expected
- Prediction (rare in social science)
- Complex limitation detection
- Theory-data fit for complex theories

---

## REPORTING APPROACH

### For Each Validation Test
1. **Method**: Clear description of test procedure
2. **Data**: Sample size, source, characteristics
3. **Metrics**: Actual values without targets
4. **Comparison**: To baselines where available
5. **Limitations**: What the test does/doesn't show

### Overall Narrative
"The system demonstrates [specific capabilities] with [actual performance metrics]. Description and extraction tasks show [results], while prediction remains challenging as expected in social science modeling (Davis et al., 2018). The proof-of-concept successfully shows [what it shows] while acknowledging [limitations]."

---

## REFERENCES

Davis, P. K., O'Mahony, A., Gulden, T. R., Osoba, O. A., & Sieck, K. (2018). Priority Challenges for Social and Behavioral Research and Its Modeling. RAND Corporation.

[Additional references to be added for benchmarks, baselines, and comparison studies]