# Chapter 1: Introduction and Research Context

## 1.1 Scaling Computational Social Science for Policy Analysis

This dissertation addresses a fundamental challenge in policy analysis: understanding how online discourse, particularly within fringe communities, shapes beliefs and behaviors at scale. Current computational social science methods require extensive human labor for theory application, limiting analysis to small datasets or single theoretical perspectives. This research will demonstrate how automated theory application can scale these methods to analyze millions of interactions while maintaining theoretical rigor, providing policy analysts with tools to describe discourse patterns, explain belief formation mechanisms, predict narrative trajectories, and design targeted interventions.

This dissertation builds directly upon prior academic work. The project's intellectual origins began with a tutorial on conspiracy theories with Dr. Eric Larson in 2021. The research was then developed into the current proposal during an independent study, also with Dr. Larson, in 2024. Since the conclusion of the independent study, we have been meeting weekly to refine the theoretical framework and system architecture, ensuring the research maintains both academic rigor and policy relevance.

## 1.2 System Architecture and Capabilities

The Knowledge Graph Analysis System (KGAS) will demonstrate systematic theory application to large-scale discourse analysis through a low-code tool that processes academic literature to extract theoretical frameworks, converts them into executable computational components, and applies them across multiple analytical modalities while preserving uncertainty.² The system addresses the policy analysis need for scalable, theoretically grounded methods to understand fringe discourse dynamics that previously required teams of analysts months to analyze.

The theory integration process will synthesize multiple theoretical perspectives across disciplines - communication theories for message diffusion patterns, sociological theories for group dynamics, political science theories for mobilization processes, and psychological theories for individual behaviors. This enables policy analysts to apply comprehensive theoretical frameworks to understand complex phenomena without requiring expertise in each individual theory. Cross-modal analysis will enable examination through different analytical lenses - network analysis for social influence patterns, statistical analysis for variable relationships, and semantic analysis for language trajectories - providing the multi-faceted understanding necessary for effective policy interventions.

## 1.3 Empirical Demonstration: COVID Conspiracy Discourse

As one demonstration of the system's general capabilities, this research will apply KGAS to analyze COVID conspiracy discourse using the Kunst et al. (2024) dataset¹ containing 2,506 Twitter users with psychological profiles and 7.7 million behavioral interactions. This particular dataset offers validation opportunities by providing both discourse data and psychological measures including conspiracy mentality, narcissism, need for chaos, and misinformation susceptibility from December 2019 through December 2021. However, this psychological focus represents just one type of theory application - the system is equally capable of applying communication, sociological, political, or economic theories to any discourse dataset.

The COVID analysis will demonstrate how the system extracts construct estimates from discourse across multiple theoretical domains - communication patterns, discourse structures, social dynamics, behavioral indicators, and psychological traits - and tests their correlation with validated self-report measures. The analytical process will follow four phases: extracting relevant theories from academic literature, converting theories into computational specifications, applying specifications to generate construct estimates, and correlating estimates with validated psychological scales. This demonstration will establish baselines for computational theory application while showing how automated methods can identify policy-relevant patterns such as influence networks, narrative evolution, and intervention opportunities.

## 1.4 User Perspective: A Policy Analyst's Workflow

To illustrate the system's capabilities, consider a policy analyst tasked with understanding vaccine hesitancy narratives spreading through online communities. Using KGAS's conversational interface, the analyst types: "I need to understand how anti-vaccine narratives are spreading and why certain groups are more susceptible." The system automatically identifies relevant theories - Diffusion of Innovations for spread patterns, Framing Theory for message construction, Social Network Theory for influence pathways, and Institutional Theory for trust dynamics. Without requiring programming skills or deep theoretical knowledge, the analyst receives an integrated analysis showing communication patterns, network structures, and potential intervention points. This same example will be expanded throughout the proposal to demonstrate theoretical framework selection (Chapter 3), system architecture (Chapter 4), and validation approaches (Chapter 8).

## 1.5 Research Scope and Approach

This research addresses the challenge of applying social science theories to large-scale behavioral data, where current approaches typically apply single theories to limited datasets using one analytical method. Consider the policy analyst from our example - without KGAS, they would need expertise in multiple theories, programming skills for data analysis, and months of manual coding to understand the vaccine discourse. Current tools extract entities or calculate metrics but cannot apply different theoretical lenses to reveal how the same data tells different analytical stories.

The proposed system will demonstrate systematic application of multiple theories to large datasets across multiple analytical modalities. The scope focuses on online discourse as a bounded context, analyzing communication patterns without claiming to predict comprehensive social behavior, validating whether computational constructs correlate with psychological measures rather than claiming causal offline effects. The research will proceed through three integrated essays: Essay 1 establishes the theoretical framework and technical specifications for what system to build, integrating both theoretical foundations and computational methods; Essay 2 describes the actual KGAS codebase implementation, architecture, and validation; Essay 3 demonstrates the system's application to the COVID fringe discourse dataset, showcasing advanced analytical capabilities including cross-modal analysis and agent-based modeling.

## 1.6 Contributions to Policy Analysis and Computational Social Science

This research will establish foundations for theory-aware computational social science with direct policy applications. For policy analysts, the system will provide tools to analyze fringe discourse at scale - tracking extremist narratives, identifying influence operations, and testing intervention strategies without requiring extensive technical expertise or theoretical knowledge. The low-code interface will democratize access to sophisticated analytical capabilities, enabling policy professionals to apply multiple theoretical frameworks to understand complex social phenomena.

The Theory Meta-Schema (detailed in Annex A) will provide standardized representation of social science theories in machine-readable form, creating a bridge between academic theory and policy application. The cross-modal analysis architecture will enable fluid movement between graph, table, and vector representations while maintaining theoretical grounding - essential for policy analysis that requires both network understanding and statistical validation. The validation methodology will establish baselines for computational construct validity without predetermined performance targets, providing transparency crucial for policy decisions.

Technical contributions include automated theory extraction from academic literature, cross-modal analysis with uncertainty propagation, and integration of Intelligence Community standards (ICD-203/206) for academic research. The empirical validation using the COVID conspiracy discourse dataset serves as one demonstration case, testing whether computational methods can extract theoretically meaningful patterns that correlate with available validation measures - in this case psychological scales, though the system is designed to work with any theoretical domain and validation approach. (Human subjects protection considerations detailed in Annex D) The system will demonstrate systematic theory application across diverse social science domains, providing policy analysts with evidence-based tools for addressing fringe discourse challenges.

# Chapter 2: Research Questions and Contributions

## 2.1 Primary Research Questions

This dissertation investigates how computational methods can extend and automate social science research in the era of large language models, with specific application to fringe discourse analysis. The research addresses three interconnected questions that explore the potential for systematic theory application in computational social science.

The first research question examines theory application: Can computational frameworks characterize improvements in the systematic selection and application of social science theories to discourse analysis? This question addresses the current fragmentation of theoretical approaches where researchers typically apply familiar theories without systematic consideration of alternatives. The proposed framework will organize theories using Lasswell's (1948) communication model⁵ - who says what to whom in what channel in what settings with what effect - integrated with analytical goals of description, explanation, prediction, and intervention.

The second research question focuses on construct validation: Can automated systems extract theoretically meaningful construct estimates from fringe discourse that correlate with validated measures across multiple theoretical domains? Using the COVID discourse dataset with 2,506 users and their psychological profiles, the research will test whether computational extraction of constructs - including communication frequency patterns, discourse coherence metrics, social network positions, behavioral engagement indicators, and psychological traits such as identity uncertainty - correlates with self-reported measures of conspiracy mentality and related characteristics. The degree of alignment will be documented, not optimized, establishing baselines rather than pursuing benchmark performance.

The third research question explores cross-modal integration: How do analytical findings differ when the same discourse is analyzed through graph, table, and vector modalities, and what insights emerge from their integration? The research will apply Social Identity Theory across modalities - examining in-group bias through network clustering, testing identity-behavior relationships through regression, and tracking identity language through semantic trajectories. Operational indicators of insight include convergence in identifying top-10 influential users across modalities, variance explained improvements when combining modal-specific features, and reduction in analyst interpretation time through integrated visualizations.

## 2.2 Positioning Relative to Existing Approaches

Current computational approaches to discourse analysis face fundamental limitations in flexibility and theoretical grounding. Standard RAG systems operate through rigid pipelines - retrieve, then generate - without the ability to dynamically chain tools or adapt workflows based on theoretical requirements. GraphRAG systems like Microsoft's implementation and DIGIMON provide fixed sets of operators (16 in DIGIMON's case) but lack the flexibility to compose complex analytical workflows across different data representations.

The critical limitation of these systems is their architectural rigidity. GraphRAG creates a single, fixed knowledge graph from data, while StructGPT provides black-box interfaces for structured data without theoretical awareness. These systems cannot dynamically reconfigure their processing pipelines based on the research question or theoretical framework being applied. They process text to graphs, or graphs to answers, but cannot fluidly move between graph, table, and vector representations as different analytical needs arise.

KGAS fundamentally differs through its modular, theory-driven architecture supporting flexible tool chains across 122+ specialized components. Unlike rigid pipeline systems, KGAS enables dynamic workflow composition where tools can be chained in any sequence guided by theoretical requirements. The system seamlessly transforms data between graph representations for network analysis, table formats for statistical processing, and vector spaces for semantic analysis - with each transformation preserving theoretical constructs and uncertainty measures. This flexibility allows the same discourse to be analyzed through communication theories using text analysis tools, then transformed to graphs for network diffusion analysis, then to tables for statistical validation, all within a single coherent workflow.

## 2.3 Academic Contributions

The methodological contribution will be a systematic framework for computational theory application extending the influence operations framework described by Larson et al. (2009)⁶ and integrating Lasswell's (1948) communication framework⁵ as extended by Druckman (2022).⁷ These frameworks identify the need for adaptive processes that test the relative efficacy of different influence approaches at individual, group, and mass public levels while systematically examining how communication components - who says what to whom in what channel in what settings with what effect - shape persuasion outcomes. This dissertation will operationalize that multi-level approach through computational methods that systematically apply and compare theoretical frameworks across these levels, fundamentally distinguishing between analyzing discourse properties (descriptive) and understanding world phenomena through discourse (explanatory).

The Theory Meta-Schema will provide a standardized format for representing social science theories in machine-readable form. Each theory specification will contain four components. Identity metadata will preserve bibliographic information and theoretical relationships. Classification will enable automated theory selection based on analytical goals and levels of analysis. Ontology will define the entities, relationships, and properties the theory examines. Execution logic will specify how the theory transforms data into analytical results.

For example, Social Identity Theory's meta-schema classifies it as group-level explanatory analysis, defining in-group/out-group entities with identification relationships, and specifying favoritism calculations for correlation with self-reported group identification.

The cross-modal analysis architecture will demonstrate how different analytical modalities provide complementary insights. Following Druckman's (2022) framework for persuasion research,⁷ the system will examine how source credibility, message content, and audience characteristics interact across different analytical representations. Graph analysis will reveal network structures of influence, table analysis will test statistical relationships, and vector analysis will track semantic evolution.

## 2.4 Empirical Validation Approach

The empirical validation using the Kunst et al. (2024) COVID dataset will employ a multi-dimensional assessment framework. The system will be evaluated across fourteen dimensions of uncertainty (detailed in Annex C) spanning source credibility through reasoning chain validity. Additionally, Davis et al.'s (2018) five-dimensional model validity framework will assess description, causal explanation, prediction, intervention, and communication validity. Basic extraction accuracy will establish baselines through crowd-sourced coding. For this demonstration case, construct validity will test correlations between computationally-extracted patterns and the available psychological scales, though the system is designed to validate against any theoretical measures - communication frequency patterns, network centrality metrics, discourse coherence scores, or economic indicators depending on the theories applied and validation data available.

## 2.5 Scope and Limitations

This research focuses on fringe discourse as a bounded domain for demonstrating computational theory application, analyzing patterns without assessing truth claims. The scope encompasses automated extraction and application of established theories to large-scale data, with theory discovery and generation as future work. The research acknowledges limitations in predicting offline behavior, focusing on identifying theoretically meaningful patterns within online discourse to advance computational social science methodology.

## 2.6 Policy Analysis Applications

This research directly addresses critical policy analysis needs for understanding and responding to fringe discourse through diverse theoretical lenses. The system will enable policy analysts to apply communication theories to track message framing and persuasion strategies, sociological theories to understand group formation and collective action, political theories to analyze mobilization and radicalization processes, and economic theories to examine incentive structures and resource flows. The multi-level approach provides comprehensive intelligence: applying diffusion theories at the individual level to predict adoption patterns, network theories at the group level to reveal coordination mechanisms, and cultural theories at the mass public level to understand narrative resonance.

For policy practitioners, the system transforms months of manual analysis into automated workflows while maintaining theoretical rigor. Agent-based modeling enables testing intervention scenarios - such as counter-narrative campaigns or network disruption strategies - by simulating how different approaches might affect discourse dynamics. This provides evidence-based decision support for allocating limited resources to combat misinformation, extremism, and other fringe discourse challenges that threaten social cohesion and democratic institutions.

# Chapter 3: Theoretical Framework

## 3.1 Theory Selection Problem

Social science theories remain fragmented across disciplines, with researchers typically applying familiar frameworks without systematic consideration of alternatives. A researcher studying COVID discourse might use Social Identity Theory because of disciplinary training while missing insights from Terror Management Theory or Network Contagion Theory. This fragmentation limits both the depth and breadth of theoretical analysis.

The proposed framework addresses this challenge through systematic organization of theories by their analytical capabilities rather than academic lineage. Using Lasswell's (1948) communication structure - who says what to whom in what channel in what settings with what effect - combined with analytical goals, the framework creates a navigable space for theory selection and application.

## 3.2 DEPI×Level×Component Framework

The framework organizes theories along three dimensions that capture their analytical capabilities and scope. This three-dimensional space enables systematic matching between research questions and theoretical frameworks.

Figure 3.1: Three-Dimensional Theory Organization Framework

```
    Analytical Goals         Levels              Components
    ┌──────────────┐      ┌──────────┐       ┌──────────┐
    │ • Describe   │      │• Individual│      │ • Who    │
    │ • Explain    │  ×   │• Group    │   ×  │ • What   │
    │ • Predict    │      │• Society  │       │ • To Whom│
    │ • Intervene  │      └──────────┘       │ • Channel│
    └──────────────┘                          │ • Settings│
                                              │ • Effect │
                                              └──────────┘
                              ↓
          72 potential analytical configurations
```

NOTE: The framework creates a navigable space for theory selection by crossing analytical goals (DEPI taxonomy) with levels of analysis and Lasswell's communication components.

The first dimension captures analytical goals following the DEPI taxonomy. Descriptive analysis identifies patterns in discourse such as frequency of conspiracy mentions or network structures. Explanatory analysis reveals mechanisms such as how identity threat leads to out-group derogation. Predictive analysis forecasts outcomes such as which users will adopt fringe beliefs. Intervention analysis designs strategies such as counter-narratives or network interventions.

The second dimension specifies levels of analysis. Individual level examines psychological processes within single actors. Group level analyzes collective dynamics and social influence. Society level studies population-wide patterns and cultural phenomena.

The third dimension identifies communication components from Lasswell's model extended by Druckman (2022). Who refers to source characteristics and credibility. What encompasses message content and framing. To Whom addresses audience characteristics and receptivity. Channel covers medium and technological affordances. Settings includes temporal and contextual factors. Effect captures outcomes and behavioral changes.

## 3.3 Theory Meta-Schema Architecture

The Theory Meta-Schema provides standardized representation of social science theories while preserving indigenous terminology. The schema contains four core components: metadata (e.g., Social Identity Theory by Tajfel & Turner 1979, extends Self-Categorization Theory), theoretical structure (e.g., entities like "vaccine hesitant groups" and "pro-vaccine groups," relations like "opposes" and "threatens," modifiers like "high uncertainty conditions"), computational representation (e.g., graph showing user clusters of vaccine discourse communities, table of hesitancy scores by demographic, vectors of semantic similarity between conspiracy narratives), and algorithms (e.g., in-group favoritism = (vaccine_skeptic_rating - mainstream_rating)/scale_range). This architecture enables computational application while maintaining theoretical fidelity. (See Annex A for complete specification with Social Identity Theory example) 

## 3.4 Computational Implementation

Theories translate into five computational patterns: FORMULAS (mathematical relationships like Social Learning Theory's influence calculations), ALGORITHMS (procedures like breadth-first search for contagion tracing), PROCEDURES (sequential processes like Spiral of Silence decision steps), RULES (logical conditions like Terror Management Theory's threat-defense mechanisms), and SEQUENCES (temporal progressions like radicalization pathways). Each pattern maps to specific implementations that transform theoretical specifications into computational operations. (Detailed specifications in Annex A)

## 3.5 Cross-Modal Data Representation

The framework enables analysis across three representations with theories specifying native formats. Graph representations capture relational structures (nodes for entities, edges for relationships). Table representations organize statistical data (rows for observations, columns for variables). Vector representations enable semantic analysis (embeddings for meaning, distances for similarity). The system handles information-preserving conversions between formats within theoretical constructs of interest while maintaining provenance to source data. Natural language serves as both input and output, with vector embeddings bridging text and computational representations.

## 3.6 Theory Selection and Application Process

The framework enables three complementary approaches to theory selection and application, supporting different research needs and expertise levels.

**Automated Selection** matches research questions to appropriate theories through systematic analysis. The system parses research questions to identify key elements - actors, behaviors, outcomes, and scope. These elements map to the DEPI×Level×Component framework to determine analytical requirements. The Master Concept Library provides standardized vocabulary for matching question concepts to theoretical constructs. Vector embeddings enable semantic similarity matching when exact terminology differs. Theories receive relevance scores based on their classification match and conceptual alignment.

Returning to our policy analyst examining vaccine hesitancy, when they ask "why certain groups are more susceptible," the system maps this to explanatory goals at the group level examining audience characteristics. The DEPI×Level×Component framework identifies Social Identity Theory (group-level, explanatory, "to whom" focus) and Terror Management Theory (individual-level, explanatory, psychological drivers) as relevant frameworks. The system scores these theories based on their ability to explain differential susceptibility, automatically combining insights about group identity threats and mortality salience that the analyst might not have considered independently.

**Interactive Exploration** guides researchers through theory selection via conversational interface. The system asks clarifying questions about analytical goals, phenomenon scope, and desired insights. Based on responses, it presents relevant theoretical options organized by analytical capability. Researchers can explore trade-offs between different theoretical perspectives. This approach supports researchers less familiar with the full range of available theories.

**Manual Browsing** presents theories organized by analytical purpose rather than disciplinary boundaries. Researchers can browse theories that describe patterns, explain mechanisms, predict outcomes, or design interventions. Within each category, theories are grouped by the level of analysis and communication components they address. This organization reveals theoretical options that disciplinary training might overlook.

The application process follows consistent steps regardless of selection method. First, the system extracts theoretical structures from text using the meta-schema specifications. Second, it applies the theory's algorithms to generate analytical outputs. Third, it validates results against theoretical expectations and empirical patterns. Fourth, it quantifies uncertainty using the twelve-dimension framework. Finally, it generates interpretable summaries linking findings to theoretical constructs.

## 3.7 Integration with Empirical Validation

The framework connects theoretical application to empirical validation through systematic comparison of computational outputs with observed patterns. This validation occurs at multiple levels to ensure theoretical fidelity and analytical validity.

Structural validation confirms that extracted theoretical elements match specifications. Using Social Identity Theory with the COVID dataset, the system verifies that identified in-groups and out-groups exhibit expected properties such as internal similarity and external differentiation. Crowd-sourced coding validates entity extraction accuracy.

Behavioral validation tests whether theoretical predictions match observed patterns. Social Identity Theory predicts increased in-group favoritism under threat. The system measures favoritism scores before and during COVID peaks, testing for predicted increases. Significant changes consistent with theoretical predictions provide validation evidence.

Construct validation assesses correspondence between computational estimates and psychological measures. The system generates construct estimates for identity uncertainty, group identification, and perceived threat. These estimates correlate with validated scales in the dataset such as conspiracy mentality and group narcissism. Significant correlations demonstrate that computational methods capture psychologically meaningful patterns.

Cross-modal validation compares findings across analytical approaches. The same theoretical construct examined through graph, table, and vector analyses should yield consistent insights. Convergent findings increase confidence while divergent results highlight unique contributions of each modality.

The framework treats validation as an iterative process. Initial applications reveal gaps between theoretical specifications and empirical patterns. These gaps inform refinements to extraction methods, algorithm parameters, and uncertainty quantification. Over time, the system learns which theoretical frameworks best explain different phenomena in fringe discourse.

# Chapter 4: System Architecture

## 4.1 Positioning Relative to Existing Systems

Current approaches to computational discourse analysis face fundamental limitations in handling theoretical complexity and analytical diversity. Pre-LLM methods such as SpaCy for entity extraction and traditional social network analysis tools provide specific capabilities but require extensive manual integration to apply theoretical frameworks. These tools extract entities or calculate network metrics without understanding the theoretical significance of what they measure. Researchers must manually interpret outputs through theoretical lenses, creating a fragmented workflow where computational tools and theoretical analysis remain disconnected.

Standard Retrieval-Augmented Generation (RAG) systems retrieve text chunks based on semantic similarity, treating documents as flat collections of passages. This approach cannot represent the multi-hop causal relationships central to social science theories, such as Social Identity Theory's chain from threat through categorization to bias. While RAG improves LLM accuracy through external knowledge, it focuses on information retrieval rather than theoretical application.

Microsoft GraphRAG and similar systems like DIGIMON (with its 16 atomic operators for graph retrieval) improve on flat retrieval by extracting entities and relationships to build graph structures. GraphRAG specifically uses hierarchical community summaries and LLM-based graph construction to capture document structure, while DIGIMON provides modular operators for graph traversal and subgraph extraction. However, these systems create a single, data-driven representation based on what appears in the text. They excel at graph-based retrieval optimization but lack the capability to apply different theoretical lenses to the same data. 

StructGPT demonstrates sophisticated reasoning over structured data through its Iterative Reading-then-Reasoning (IRR) framework, using specialized interfaces to query databases and having the LLM reason over results. Google's Concordia framework and Park et al.'s (2023) generative agents show promise for agent-based simulation but require manual behavioral rule specification. These limitations are particularly problematic when analyzing polarized discourse where echo chamber effects¹¹ require different theoretical frameworks to understand information diffusion versus attitude formation.¹⁴

KGAS differs through its theory-first architecture that generates different representations based on the selected theoretical framework. Unlike other systems designed for current applications, KGAS is designed as infrastructure for future autonomous research agents. The system's theory-driven workflow fundamentally differs from data-driven approaches:

Figure 4.2: Comparison of Standard RAG and KGAS Theory-First Workflows

```
    Standard RAG/GraphRAG:
    Query → Retrieve → Generate → Response
    
    KGAS Theory-First:
    Research Question
           ↓
    Theory Selection
           ↓
    Theory Extraction ←─── Academic Literature
           ↓
    Multi-Modal Analysis
    ┌──────┴──────┬──────────┐
    Graph      Table      Vector
    Analysis   Analysis   Analysis
    └──────┬──────┴──────────┘
           ↓
    Integrated Results with Uncertainty
```

NOTE: Unlike data-driven retrieval systems, KGAS uses theoretical frameworks to guide all analytical operations from extraction through integration.

When analyzing COVID discourse through Social Identity Theory, the system constructs graphs with in-group and out-group nodes connected by identification edges. The same discourse analyzed through Network Contagion Theory generates individual nodes with influence pathway edges.

Table 4.1: Computational Discourse Analysis System Capabilities

| Capability                  | Pre-LLM Tools | Standard RAG | GraphRAG/DIGIMON | KGAS      |
|                            | (SpaCy, SNA)  |              |                  |           |
|----------------------------|---------------|--------------|------------------|-----------|
| Entity Extraction          | ✓             | ✓            | ✓                | ✓         |
| Theory-Aware Processing    | ✗             | ✗            | ✗                | ✓         |
| Cross-Modal Analysis       | ✗             | ✗            | Partial          | ✓         |
| Statistical Integration    | Manual Only   | ✗            | ✗                | ✓         |
| ABM Integration           | ✗             | ✗            | ✗                | ✓         |
| Uncertainty Quantification | ✗             | Basic        | Basic            | IC-Std^a  |

NOTE: SNA = social network analysis; ABM = agent-based modeling; IC-Std = Intelligence Community standards.
^a KGAS implements a four-layer uncertainty framework based on ICD-203/206 standards with mathematical propagation through root-sum-squares.

The system's comprehensive scale sets it apart from existing approaches. While DIGIMON offers 16 operators and StructGPT provides 8 interfaces, KGAS integrates tools across multiple categories with standardized contracts, including dedicated suites for document processing, graph operations, statistical analysis (including SEM and factor analysis), vector operations, and cross-modal converters. This scale enables capabilities absent in other systems: automated theory operationalization from academic literature, agent-based model integration for counterfactual analysis, and a complete statistical analysis suite producing APA-formatted outputs.

## 4.2 Cross-Modal Analysis Architecture

The system implements native cross-modal analysis that fundamentally differs from multi-modal RAG systems. While standard multi-modal systems handle different data types (text, image, audio), KGAS provides different analytical representations of the same data. This enables seamless transformation between graph, table, and vector modes with semantic preservation and provenance tracking - a capability absent in compared systems.

Different analytical modes reveal complementary aspects of social phenomena. Graph mode exposes relationships, centrality, and community structures. Table mode enables statistical analysis, correlations, and aggregations. Vector mode supports similarity search, clustering, and semantic trajectories. The system maintains information-preserving transformation between modes within theoretical constructs of interest, preserving not just data but also uncertainty quantification and theoretical grounding throughout conversions.

For example, when analyzing identity uncertainty's effect on conspiracy beliefs, the system constructs group networks in graph mode revealing influence pathways, calculates regression coefficients in table mode testing theoretical predictions, and tracks semantic evolution in vector mode showing language convergence within echo chambers. These three perspectives integrate through the cross-modal converter, providing convergent validation where consistent findings across modalities increase confidence while divergent results highlight unique insights from each analytical lens.

## 4.3 Provenance and Uncertainty Infrastructure

The system implements W3C PROV-compliant provenance tracking with comprehensive uncertainty audit trails. Every analytical operation records not just inputs and outputs but complete decision traces - why theories were selected, how evidence was weighted, what alternatives were considered. Transformation lineage tracks information preservation across format conversions (graph↔table↔vector), maintaining bidirectional links from any finding to originating evidence. The provenance system captures uncertainty provenance including calculation methods, input confidences, mathematical parameters, and confidence bounds on the uncertainty assessments themselves.

The IC-informed uncertainty framework adapts Intelligence Community methodologies (ICD-203/206)² for academic research. Rather than spurious point estimates, the system uses standardized probability bands ("likely" 55-80%, "very likely" 80-95%) with mathematical propagation through root-sum-squares for independent uncertainties. Following Heuer's information paradox,⁸ the system prioritizes evidence quality over quantity, employing structured analytic techniques including devil's advocacy, competing hypotheses analysis, and systematic bias mitigation. This creates transparent uncertainty quantification where every confidence value has traceable justification through the complete analytical pipeline.

## 4.4 Two-Layer Theory Architecture

The system implements a two-layer architecture that fundamentally separates theoretical structure extraction from application, addressing the critical problem of theory-question coupling in traditional approaches.

Figure 4.3: Two-Layer Theory Processing Architecture

```
    Layer 1: Theory Extraction (One-time)
    ┌─────────────────────────────────────┐
    │  Academic Literature                │
    │  ↓                                  │
    │  LLM Extraction with Meta-Schema    │
    │  ↓                                  │
    │  Validated Theory Repository        │
    └─────────────────────────────────────┘
                    ↓
    Layer 2: Question-Driven Application (Repeated)
    ┌─────────────────────────────────────┐
    │  Research Question                  │
    │  ↓                                  │
    │  Select Theory Components           │
    │  ↓                                  │
    │  Apply to Data                      │
    │  ↓                                  │
    │  Generate Results                   │
    └─────────────────────────────────────┘
```

NOTE: Separation of extraction from application enables reuse of theoretical structures across multiple research questions without re-extraction.

Layer 1 performs comprehensive theoretical extraction independent of analytical goals. Using a sophisticated 3-phase LLM extraction process with the Theory Meta-Schema, the system achieves 8.95-10/10 quality scores across diverse academic domains with 100% success rates.* The extraction captures complete theoretical structures - entities with indigenous terminology, relationships with causal directions, assumptions and boundary conditions, mathematical formulations, and analytical methods. Multi-agent validation ensures extraction quality, with evidence showing 100/100 quality standards achieved across 10 theories from 7 academic domains. This comprehensive extraction occurs once per theory, creating reusable theoretical representations.

Layer 2 enables question-driven analysis using pre-extracted theories. When researchers pose questions, the system selects relevant theoretical components from the extracted structures, applies appropriate analytical methods (formulas, algorithms, procedures, rules, sequences), and generates theory-grounded results. This separation enables powerful capabilities: the same extraction supports multiple research questions, theories can be combined for multi-perspective analysis, and analytical approaches can evolve without re-extraction. The architecture ensures theoretical integrity while enabling flexible application to reveal world phenomena through discourse analysis rather than just textual patterns.

## 4.5 Low-Code Interface and User Interaction

The system provides a low-code interface that enables researchers to apply computational methods without programming expertise. The primary interaction occurs through a conversational chatbot interface that translates natural language requests into computational workflows. Advanced users retain full control through configuration options and direct workflow specification.

The chatbot interface guides users through theory selection and application. Continuing our vaccine hesitancy example, after the analyst's initial query, the system might ask: "Are you more interested in understanding how these narratives spread (diffusion) or why people believe them (persuasion)?" Based on the response, it refines theory selection and constructs appropriate analytical workflows. The system explains its choices: "I'm applying Social Identity Theory to identify in-group/out-group dynamics in vaccine discourse, combined with Diffusion of Innovations to trace how narratives spread through the network." This transparency helps analysts understand and trust the analytical process.

For researchers with specific theoretical expertise, the system provides direct theory selection and configuration options. Users can browse available theories organized by analytical capabilities, select specific theories for application, and adjust parameters based on domain knowledge. This flexibility ensures that computational automation enhances rather than replaces scholarly judgment.

The interface abstracts technical complexity while maintaining transparency. Users see which theories are being applied and why, what analytical methods are being used, and how uncertainty is being quantified. Results include natural language explanations linking findings to theoretical constructs, making computational analysis accessible to researchers without technical backgrounds.

## 4.6 Workflow Orchestration and DAG Construction

The system orchestrates complex analytical workflows through Directed Acyclic Graphs (DAGs) that specify sequences of analytical operations. These DAGs ensure reproducible analysis while enabling multi-step processing pipelines.

Figure 4.1: Social Identity Theory Analysis DAG (Parallel Processing Paths)

```
                    COVID Dataset
                          ↓
                  [T01: Load Data]
                    ↙          ↘
        [T23A: Extract      [T15A: Chunk
         Entities]           Text]
              ↓                 ↓
        [T31: Build        [T15B: Generate
         Network Graph]     Embeddings]
         ↙        ↘             ↓
   [T49: Detect   [T68:    [Vector Clustering
    Communities]  PageRank]  for Themes]
        ↓            ↓           ↓
   [Calculate     [Influence  [Semantic
    In/Out-group]  Scores]    Analysis]
         ↘           ↓         ↙
          [T43: Statistical Correlation]
                    ↓
           [Validate Against Scales]
```

NOTE: The DAG demonstrates parallel processing - entity extraction and network analysis occur simultaneously with text embedding and semantic analysis. Results converge for statistical validation, showing true directed acyclic graph capabilities.

Each node in the DAG represents a specific analytical operation implemented by a tool. Tools range from basic operations like entity extraction to complex theoretical calculations. The edges specify data flow between operations, ensuring that outputs from one step become inputs to the next. This structure enables complex workflows while maintaining clear execution logic.

The system constructs DAGs automatically based on research questions and selected theories. When a user asks about identity and conspiracy beliefs, the system identifies Social Identity Theory as relevant, then constructs a DAG that extracts identity-related entities, builds group networks, calculates theoretical measures, and tests relationships. Advanced users can modify generated DAGs or create custom workflows for specific analytical needs.

DAG execution includes comprehensive error handling and validation. Each step validates inputs against expected formats and ranges. Failed operations generate informative error messages rather than silent failures. The system tracks provenance throughout execution, recording which tools were used, what parameters were applied, and how results were generated.

## 4.7 System Integration

The system uses a bi-store architecture optimized for different analytical requirements: graph database for network structures and relational database for metadata and provenance. Processing implements theory-specific algorithms through standardized tool contracts. External integration occurs via Model Context Protocol (MCP), enabling interoperability with statistical packages and visualization tools. The architecture prioritizes reproducibility through complete audit trails and version control, supporting academic requirements for transparent research. (Technical specifications in Annex B)

*Quality scores derived from multi-agent validation where three independent LLM agents rated extraction completeness, accuracy, and theoretical fidelity on a 10-point rubric. Scores represent average across dimensions for each theory extraction.

# Chapter 5: Essay 1 Method - Theory Integration Approach

## 5.1 Framework Development Methodology

This essay develops a systematic framework organizing theories by analytical capabilities rather than disciplinary origins, addressing theoretical fragmentation through unified computational structure. The framework draws on Larson et al.'s (2009) multi-level influence operations approach (individual cognitive processes, group peer dynamics, mass public media effects) and extends Lasswell's (1948) communication model with Druckman's (2022) settings addition, creating six components: Who (source credibility), What (message content), To Whom (audience characteristics), Channel (medium affordances), Settings (temporal context), Effect (behavioral outcomes).

## 5.2 Theory Organization Process

The framework organizes theories from communication, psychological, social, and behavioral sciences based on their analytical capabilities. Each theory maps to the DEPI×Level×Component framework: Social Identity Theory (group level, explanatory, audience focus), Diffusion of Innovations (mass public, predictive, channel/settings focus). This classification reveals complementarities for integration and gaps requiring development, enabling systematic theory selection based on research questions rather than disciplinary boundaries.

## 5.3 Theory Meta-Schema Development

The meta-schema standardizes theory representation while preserving theoretical integrity. Components include: metadata (citations, relationships, scope), classification (DEPI position, data requirements, outputs), and computational specifications (formulas, procedures, rules). This structure enables automated theory selection and execution while maintaining fidelity to original theoretical formulations. (Complete specifications in Annex A)

## 5.4 Integration with Influence Operations Framework

The framework explicitly integrates insights from influence operations research, recognizing that understanding fringe discourse requires analytical tools developed for assessing persuasion and behavioral change. Larson et al.'s framework identifies key analytical questions at each level that guide theory selection and application.

At the individual level, the framework addresses questions about cognitive susceptibility, message processing, and attitude formation. Theories such as the Elaboration Likelihood Model and Motivated Reasoning provide tools for understanding how individuals process information under different conditions. These theories help identify when individuals engage in systematic versus heuristic processing and how prior beliefs affect information interpretation.

At the group level, the framework examines social influence, network effects, and collective dynamics. Theories such as Social Identity Theory and Social Learning Theory explain how group membership and peer observation shape beliefs and behaviors. Network theories address how information and influence flow through social connections, identifying key nodes and pathways for belief transmission.

At the mass public level, the framework analyzes population-wide patterns, media effects, and cultural narratives. Theories such as Cultivation Theory and Agenda Setting explain how media exposure shapes perceptions of reality. Diffusion theories predict how innovations and ideas spread through populations over time.

## 5.5 Validation Approach

The framework validation assesses whether the organizational structure enables effective theory selection and application. This involves testing whether theories classified similarly produce complementary insights and whether the framework guides users to appropriate theories for their research questions.

Theory classification validation examines inter-rater reliability in assigning theories to framework dimensions. Multiple researchers independently classify a subset of theories, with agreement levels indicating classification clarity. Disagreements identify theories that span multiple categories or require refined definitions.

Application validation tests whether the framework enables systematic theory selection for research questions. Sample research questions from fringe discourse analysis are mapped to framework dimensions, and the resulting theory recommendations are evaluated by domain experts. This validates that the framework connects research needs to appropriate theoretical tools.

Integration validation assesses whether theories can be meaningfully combined when addressing complex phenomena. The framework should identify complementary theories that together provide more comprehensive explanations than single theories alone. For example, combining Social Identity Theory's group dynamics with Motivated Reasoning's cognitive processes provides richer understanding of conspiracy belief adoption.

## 5.6 Expected Deliverables

This essay will produce several key deliverables that establish the theoretical foundation for the computational system. The primary deliverable is the three-dimensional organizing framework that classifies theories by their analytical capabilities across the DEPI taxonomy, levels of analysis, and communication components. This framework will be presented through structured tables and visual representations showing theoretical relationships.

The Theory Meta-Schema specification will provide detailed templates for representing theories in computational form. Example schemas for key theories such as Social Identity Theory, Diffusion of Innovations, and Motivated Reasoning will demonstrate how theoretical concepts translate to machine-readable specifications. These examples will show how different types of theories - mathematical, logical, procedural - can be consistently represented.

A populated theory library will include initial meta-schemas for theories spanning the framework dimensions. This library will emphasize theories relevant to fringe discourse analysis, including those addressing identity formation, information processing, social influence, and belief dynamics. Each theory will be documented with its theoretical origins, computational specifications, and application examples.

The framework documentation will include guidelines for adding new theories, extending existing theories, and identifying theoretical gaps. This ensures the framework can evolve as new theories emerge or existing theories are refined. The documentation will also address how to handle theories that span multiple dimensions or challenge the organizational structure.

# Chapter 6: Essay 2 Method - System Development and Validation

## 6.1 Dataset Specification and Preparation

The system development and validation will use the Kunst et al. (2024) COVID conspiracy discourse dataset, which provides a unique combination of behavioral data and psychological profiles. Table 6.1 summarizes the key dataset characteristics that enable validation of computational construct extraction against established psychological measures.

Table 6.1: Kunst COVID-19 Conspiracy Discourse Dataset Specifications

| Dataset Component         | Specification                           |
|--------------------------|----------------------------------------|
| Users with Profiles      | 2,506 Twitter users                   |
| Behavioral Interactions  | 7.7 million tweets and engagements    |
| Time Period             | December 2019 - December 2021         |
| Psychological Measures   | Conspiracy mentality scale            |
|                         | Collective narcissism scale           |
|                         | Need for chaos scale                  |
|                         | Misinformation susceptibility scale   |
| Network Construction    | Interaction-based (replies, reposts)  |
| Validation Approach     | Correlation with psychological scales |

NOTE: The dataset combines self-reported psychological assessments with complete behavioral traces, enabling validation of whether computational methods can extract theoretically meaningful patterns from discourse.

Data preparation will focus on interaction-based network construction rather than follower relationships. Networks will be defined by actual engagement - replies, reposts, and likes - which better reflect active discourse participation than passive following relationships. Data filtering will use LLM-based quality assessment to assign coherence scores (0-1) based on semantic consistency and discourse relevance, with manual validation on a subset to establish inter-rater reliability. Content relevance filtering will identify posts related to COVID, vaccines, and associated conspiracy narratives using keyword matching and semantic similarity.

The psychological profile data provides ground truth for validation. Each user's scores on validated psychological scales serve as comparison points for computationally extracted construct estimates. This enables assessment of whether the system can identify psychological patterns from discourse that correspond to measured psychological traits. The temporal span allows tracking of belief evolution and network dynamics over the pandemic's progression.

Cross-platform considerations will be documented but not implemented in the initial system. While the current dataset focuses on Twitter, the framework will be designed to accommodate other platforms. Platform-specific features such as retweet cascades are Twitter-specific, while general engagement metrics such as likes and replies exist across platforms. The system architecture will maintain flexibility for future multi-platform integration.

## 6.2 System Implementation

The low-code chatbot interface translates natural language requests into computational workflows, democratizing access to theoretical analysis. Directed Acyclic Graphs orchestrate tool execution with automatic construction from research questions. The two-layer architecture separates one-time theory extraction (Layer 1) from repeated application (Layer 2), enabling theory reuse across datasets. (Implementation details in Annex B)

## 6.3 Validation Methodology

Validation will proceed through multiple levels to assess both technical functionality and theoretical validity. The approach recognizes that the system's value lies not in perfect accuracy but in meaningful correlation between computational estimates and theoretical constructs.

Basic extraction validation will use crowd-sourced coding to assess entity and relationship extraction accuracy. A sample of posts will be coded by human annotators for entities relevant to conspiracy theories - actors, claims, sources, and narratives. Inter-rater reliability will establish coding consistency. System extractions will be compared against human coding to calculate precision, recall, and F1 scores. The target is achieving sufficient accuracy for meaningful analysis rather than perfect extraction.

Construct validity assessment will test whether computational estimates correlate with psychological measures. The system will generate construct estimates for each user based on theoretical specifications - identity uncertainty scores from Social Identity Theory, threat perception from Terror Management Theory, and group identification from social network position. These estimates will be correlated with the psychological scales in the dataset. Significant correlations will demonstrate that automated theory application captures psychologically meaningful patterns.

Cross-modal validation will compare findings across analytical approaches. The same theoretical construct will be examined through graph, table, and vector analyses. Convergent findings across modalities will increase confidence in results. Divergent findings will be examined to understand what unique insights each modality provides. This validation approach recognizes that different analytical modes may reveal different aspects of the same phenomenon.

## 6.4 Theory Application Examples

The implementation will demonstrate theory application through concrete examples from the COVID dataset. These examples will show how different theories generate different insights from the same data, validating the framework's capability for multi-theoretical analysis.

Social Identity Theory application will identify in-groups and out-groups within the discourse. The system will extract group-identifying language, construct group membership networks, and calculate in-group favoritism scores. Validation will test whether users with higher computed favoritism scores show stronger conspiracy mentality in psychological assessments.

Motivated Reasoning application will track how users process information consistent or inconsistent with prior beliefs. The system will identify belief-confirming and belief-challenging information in user timelines, measure differential engagement with each type, and compute confirmation bias scores. Validation will test whether these scores correlate with measured conspiracy mentality and misinformation susceptibility.

Diffusion of Innovations¹³ application will trace how conspiracy narratives spread through the network. The system will identify novel claims, track their propagation paths, measure adoption rates, and identify influential spreaders. Validation will compare predicted diffusion patterns against observed spread in the temporal data.

## 6.5 Performance Considerations

While the system prioritizes theoretical validity over computational performance, practical considerations will be addressed to ensure usability. The implementation will balance analytical sophistication with reasonable execution times for research workflows.

Processing efficiency will be achieved through appropriate tool selection rather than optimization. Graph algorithms will use established libraries with reasonable performance characteristics. Statistical analyses will leverage vectorized operations where possible. Language model calls will be batched to reduce latency. The goal is completing typical analytical workflows within hours rather than days.

Scalability will be addressed through sampling strategies when necessary. Complete analysis of 7.7 million interactions may not be feasible for all operations. The system will support representative sampling for computationally expensive analyses while maintaining full data analysis for critical operations. Sampling strategies will be documented to ensure reproducibility.

Error handling will follow fail-fast principles to maintain analytical integrity. Operations that cannot complete successfully will generate clear error messages rather than producing questionable results. The system will validate inputs at each workflow step and halt execution if requirements are not met. This approach ensures that successful analyses can be trusted while failed analyses are clearly identified.

## 6.6 Expected Outcomes

The system development will produce a functional prototype demonstrating theory-aware discourse analysis that will validate the feasibility of the approach and provide a foundation for future development.

The primary deliverable will be the working system capable of extracting theories from academic literature, applying them to the COVID dataset, and generating analytical results across multiple modalities. The system will demonstrate that computational methods can systematically apply social science theories to large-scale discourse analysis.

Validation results will provide evidence about the system's capabilities and limitations. Correlation analyses between computational estimates and psychological measures will establish baseline expectations for construct validity. Extraction accuracy metrics will identify areas where human validation remains necessary. Cross-modal comparisons will reveal which analytical approaches best suit different theoretical questions.

Documentation will include system architecture specifications, theory application examples, and user guides. The architecture documentation will enable future development and extension. Application examples will show how different theories generate insights from discourse data. User guides will help researchers understand when and how to apply the system to their research questions.

# Chapter 7: Essay 3 Method - Analysis and Application

## 7.1 Analytical Methods Progression

This essay demonstrates the full range of analytical capabilities enabled by the Theory Meta-Schema, progressing from basic computational methods to complex simulations. The demonstration uses the COVID discourse dataset to show how theories translate into increasingly sophisticated analytical approaches, culminating in agent-based modeling that tests theoretical predictions through simulation.

The analytical progression follows the natural complexity hierarchy of social science methods. Basic methods apply formulas and rules directly to data. Intermediate methods integrate across data modalities and statistical relationships. Advanced methods model complex systems and emergent behaviors. This progression demonstrates that the Theory Meta-Schema can accommodate the full spectrum of social science analytical approaches.

## 7.2 Basic Computational Methods

The system implements five analytical patterns from theoretical specifications. Formula-based analysis calculates mathematical relationships (Social Learning Theory's influence scores). Algorithmic analysis traces patterns through networks (contagion cascades via breadth-first search). Procedural analysis executes sequential processes (Spiral of Silence decision steps). Rule-based analysis applies logical conditions (Terror Management Theory's threat-defense mechanisms). Sequence analysis tracks temporal progressions, enabling theory combination across time scales - applying Larson's framework for immediate tactical influence processes (emotional appeals, source credibility effects) while simultaneously tracking McGuire's six-step model for longer-term attitude hardening (exposure→attention→comprehension→acceptance→retention→action), revealing how short-term influence operations evolve into durable belief changes.

## 7.3 Cross-Modal Integration

Cross-modal analysis examines theoretical constructs through complementary lenses. Graph analysis constructs Social Identity Theory's in-group/out-group networks with identification edges, measuring density and testing bias predictions. Table analysis organizes identity variables for regression testing whether uncertainty predicts conspiracy adoption. Vector analysis tracks semantic evolution revealing echo chamber convergence. Integration provides convergent validation where consistent findings across modalities strengthen theoretical interpretations while divergent results highlight unique analytical contributions.

## 7.4 Statistical Modeling

The system demonstrates theory-generated statistical models through regression analysis testing Motivated Reasoning predictions with interaction terms for belief moderation effects, multilevel modeling accounting for users nested within groups with random effects and cross-level interactions, and time series analysis validating Diffusion of Innovations' S-curve patterns for conspiracy narrative adoption. These methods move beyond correlations to test complex theoretical relationships.

## 7.5 Structural Equation Modeling

SEM demonstrates the Theory Meta-Schema's capability for sophisticated theoretical specifications through latent construct modeling (identity uncertainty with multiple indicators tested via CFA), path modeling translating Theory of Planned Behavior's attitude-norm-control-intention-behavior sequence into testable coefficients, and model comparison testing competing theoretical explanations for conspiracy belief adoption. This enables assessment of which theoretical structure best fits observed patterns.

## 7.6 Agent-Based Modeling

Agent-based modeling represents the most complex analytical capability, demonstrating how the Theory Meta-Schema enables theory validation through simulation. The approach differs from recent LLM-based agent systems⁹¹⁰ by implementing theory-constrained behavioral rules rather than emergent behaviors from language models.

Agent parameterization will directly utilize the Kunst dataset's personality characteristics. Each agent receives conspiracy mentality, narcissism, need for chaos, and misinformation susceptibility scores drawn from the empirical distributions. Table 7.1 illustrates how psychological traits map to specific behavioral rules derived from theoretical frameworks.

Table 7.1: Theory-Driven Agent Behavioral Rules

| Psychological Trait        | Theory Source            | Behavioral Rule                    |
|---------------------------|--------------------------|-----------------------------------|
| High Conspiracy Mentality | Motivated Reasoning      | Discount contradictory evidence  |
| High Narcissism          | Social Identity Theory   | Seek attention via controversy   |
| High Need for Chaos      | System Justification     | Amplify divisive content         |
| Low Trust in Authority   | Reactance Theory        | Reject official narratives       |
| High Group Identity      | Reasoned Action Model    | Conform to perceived peer norms  |
| High Uncertainty         | Terror Management Theory | Increase worldview defense       |

NOTE: Each behavioral rule translates theoretical predictions into computational operations, enabling agents to exhibit theory-consistent behaviors based on their psychological profiles.

The Reasoned Action Model provides a structured approach for agent decision-making. Attitudes toward conspiracy beliefs combine with subjective norms from network peers and perceived behavioral control to generate behavioral intentions. For example, an agent with moderate conspiracy mentality (attitude) observing peers sharing conspiracy content (subjective norm) and feeling empowered by social media tools (perceived control) calculates intention scores for sharing similar content. The model's mathematical formulation enables precise tracking of how psychological traits interact with social influences to produce behaviors.

The simulation follows established ABM frameworks with theory-driven rules. Agents encounter content based on network position, process it through theory-specified pathways (central versus peripheral routes from ELM based on their elaboration likelihood), update belief states according to theoretical mechanisms, and generate discourse contributions by selecting from observed patterns weighted by current states. Multiple theories operate simultaneously - Social Identity Theory governs group dynamics, Motivated Reasoning filters information processing, and Spiral of Silence modulates expression likelihood.

Calibration matches simulation outputs to empirical patterns through systematic parameter variation within theoretical bounds. Parameter ranges will mirror the empirical distribution's 5th-95th percentiles unless literature specifies otherwise, ensuring agents exhibit realistic behavioral variation while avoiding extreme outliers. Validation compares aggregate patterns including adoption curves and polarization levels against observed data. Intervention modeling tests strategies such as counter-narratives targeting high-influence nodes or network modifications to reduce echo chamber effects, providing tools for exploring outcomes before resource commitment.

# Chapter 8: Validation Strategy

## 8.1 Multi-Dimensional Validity Framework

The validation strategy follows Davis et al.'s (2018) five-dimensional framework: description (identifying salient structures), causal explanation (identifying processes), postdiction (explaining past behavior), exploratory analysis (parameterizing variables), and prediction (forecasting future). The system validation tests whether KGAS correctly extracts and applies theories, not whether theories themselves are true. Validation assesses fourteen uncertainty dimensions spanning the analytical pipeline from source credibility through reasoning chain validity, using multiple assessment methods. Validation includes process metrics (extraction time, DAG complexity, conversion rates) alongside theoretical validation. (Complete dimension specifications in Annex C)

## 8.2 Description and Extraction Validity

Description validity assesses entity extraction through crowd-sourced coding of 500 posts for precision/recall metrics, theory structure completeness via hand-coding 5-10 theories and comparing extractions using graph edit distance, and inter-LLM reliability tests demonstrating reproducibility though consistency doesn't guarantee accuracy. For our vaccine hesitancy example, validation would verify that the system correctly identifies anti-vaccine groups as in-groups, public health authorities as out-groups, and vaccine safety concerns as threat narratives - validating that theoretical constructs map accurately to discourse elements.

## 8.3 Postdiction and Construct Validity

Postdiction tests explain past behavior by extracting construct estimates from COVID dataset tweets and correlating with psychological measures (conspiracy mentality, narcissism, need for chaos). Any significant correlations demonstrate meaningful pattern extraction without predetermined effect sizes. Theory replication applies extracted theories to original paper datasets comparing structural similarity and directional agreement.

## 8.4 Exploratory and System Capability Assessment

Exploratory validity documents modality-specific insights (graph for networks, table for statistics, vector for semantics) and tests multi-resolution coherence across individual, network, and population levels. System capability assessment evaluates fourteen dimensions including source credibility ratings against expert judgments, cross-source contradiction detection, entity identity resolution, reasoning chain validity via HotPotQA benchmarks, and confidence calibration checking whether stated confidence aligns with actual accuracy.

## 8.5 Expected Outcomes Without Predetermined Targets

Validation reports actual metrics without predetermined targets, establishing baselines for this emerging area. Process metrics establish operational baselines: median extraction time per theory (<60s target), percentage of elements captured, and cross-modal conversion loss rates. Entity extraction shows stronger performance given NLP capabilities. Construct correlations demonstrate meaningful extraction without required effect sizes. Prediction remains aspirational per Davis. Limitations include online discourse restriction, correlational findings, platform specificity, inter-LLM consistency not accuracy, and unavailable ground truth for complex constructs - framing this as proof-of-concept infrastructure.

# Chapter 9: Timeline and Limitations

## 9.1 Research Timeline

The dissertation proceeds through three integrated phases over 18 months from proposal approval (September 2025) through defense (February 2027). (See Annex E for detailed timeline with milestones, deliverables, and risk management)

## 9.2 Scope and Limitations

The research demonstrates computational theory application to online discourse without claiming comprehensive social prediction. Scope encompasses extracting/applying established theories rather than discovering new frameworks. Technical limitations include LLM dependence, single-node constraints, and preserving theoretical nuance computationally. Validation limitations include absent ground truth for constructs, observational data causality challenges, and platform-specific dataset biases.

## 9.3 Contributions to Future Research

This dissertation establishes foundations for theory-aware computational social science. The Theory Meta-Schema provides evolving templates for computational theory representation. The validation framework assesses computational theory application without predetermined targets. The cross-modal architecture integrates analytical perspectives while maintaining theoretical fidelity. KGAS serves as proof-of-concept infrastructure, with current metrics providing baselines for next-generation systems - as language models improve, extraction strengthens; as datasets expand, validation multiplies; as resources grow, sophistication increases. The research demonstrates initial capabilities while establishing frameworks accommodating future improvements.

---

## References

¹ Kunst, J. R., Gundersen, A. B., Krysińska, I., et al., "Leveraging Artificial Intelligence to Identify the Psychological Factors Associated with Conspiracy Theory Beliefs Online," Nature Communications, Vol. 15, 2024.

² Office of the Director of National Intelligence, ICD 203: Analytic Standards, 21 June 2007; ICD 206: Sourcing Requirements for Disseminated Analytic Products, 27 June 2012.

³ Tajfel, Henri, and John C. Turner, "The Social Identity Theory of Intergroup Behavior," in Stephen Worchel and William G. Austin, eds., The Social Psychology of Intergroup Relations, Brooks/Cole, 1986, pp. 7–24.

⁴ Bandura, Albert, Social Learning Theory, Prentice-Hall, 1977.

⁵ Lasswell, Harold D., "The Structure and Function of Communication in Society," in Lyman Bryson, ed., The Communication of Ideas, Harper and Row, 1948, pp. 37–51.

⁶ Larson, Eric V., et al., Foundations of Effective Influence Operations: A Framework for Enhancing Army Capabilities, RAND Corporation, 2009.

⁷ Druckman, James N., "A Framework for the Study of Persuasion," Annual Review of Political Science, Vol. 25, No. 1, 2022, pp. 65–88.

⁸ Heuer, Richards J., Psychology of Intelligence Analysis, Center for the Study of Intelligence, 1999.

⁹ Park, J. S., et al., "Generative Agents: Interactive Simulacra of Human Behavior," in Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23), Association for Computing Machinery, 2023, pp. 759–789.

¹⁰ Vezhnevets, A. S., et al., Generative Agent-Based Modeling with Actions Grounded in Physical, Social, or Digital Space Using Concordia, arXiv, 2023.

¹¹ Bail, Chris, et al., "Exposure to Opposing Views on Social Media Can Increase Political Polarization," Proceedings of the National Academy of Sciences, Vol. 115, No. 37, 2018, pp. 9216–9221.

¹² Chong, Dennis, and James N. Druckman, "Framing Theory," Annual Review of Political Science, Vol. 10, 2007, pp. 103–126.

¹³ Rogers, Everett M., Diffusion of Innovations, 5th ed., Simon & Schuster, 2010.

¹⁴ Zaller, John R., The Nature and Origins of Mass Opinion, Cambridge University Press, 1992.

---

(See Annex F for Glossary of Terms)