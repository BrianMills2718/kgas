# Chapter 1: Introduction and Research Context

## 1.1 Scaling Theory-Aware Computational Social Science

This dissertation explores how Large Language Models (LLMs) can be leveraged in the analysis online discourse within the field of computational social science through a theory-first approach. Unlike existing systems that apply computational methods and then interpret results through theoretical lenses, this research proposes that theories should guide the analytical process from the outset, determining how data flows between different representational formats and what patterns to seek. Many current social science methods require human labor for theory application, limiting analysis to small datasets or single theoretical perspectives. This research intends to show that theory-aware computational social science is possible by establishing LLM performance baselines. While current system performance may not meet real-world operational demands, this work establishes foundations for more capable language models that will enable more sophisticated and accurate theory application at scale.

This research will test whether LLMs can automate theory application through systematic operationalization of social science theories, scaling these methods to analyze interactions at scale while maintaining theoretical rigor. This approach lets analysts describe patterns, explain what causes them, predict what will happen next, and guide interventions. The implications of this work for policy analysis, among other applied fields, are significant and will be discussed in Chapter 2, which examines how theory-aware computational methods can address practical analytical challenges.

This dissertation builds directly upon prior academic work. The project's intellectual origins began with a tutorial on conspiracy theories with Dr. Eric Larson in 2021. The research was then developed into the current proposal during an independent study, also with Dr. Larson, in 2024. Since the conclusion of the independent study, we have been meeting weekly to refine the theoretical framework and system architecture, ensuring the research maintains both academic rigor and policy relevance.

## 1.2 System Architecture and Capabilities

The Knowledge Graph Analysis System (KGAS) will operationalize this theory-first vision through a low-code (code only involved in the initial installation, setup, and startup) tool that extracts theories from academic papers and converts them into computer instructions that analyze data. Unlike data-driven systems that find patterns first, KGAS starts with a theory to guide the analysis and data flow.

The system will provide traceability linking results back to theoretical foundations, interpretability through explanations at each analytical step, uncertainty estimation and propagation through traceable methods, and reproducibility within the constraints of LLM non-determinism. The theory-first architecture synthesizes perspectives across disciplines without requiring users to be experts in each field, automating a process that once took teams of analysts months to complete.

## 1.3 Demonstration: COVID Conspiracy Discourse

The Kunst et al. (2024) dataset provides a testbed by combining 7.7 million tweets from 2,506 users with psychological assessments - including conspiracy mentality, collective narcissism, need for chaos, and misinformation susceptibility scales. This combination of user data and psychological profiles lets us test if our methods can find online patterns that match specific psychological traits.

The COVID dataset's scale, psychological validation, temporal depth, and policy relevance make it appropriate for demonstration. This proposal will use Social Identity Theory as a consistent example to demonstrate a complete analytical workflow from theory selection through validation, while also introducing other theoretical frameworks to highlight the system's flexibility and theory-agnostic design.

## 1.4 KGAS in Action: A Policy Analyst's Workflow

Consider a policy analyst investigating vaccine hesitancy narratives. Using the proposed conversational interface, they type: "Why do vaccine hesitant groups reject mainstream health information?". The system identifies Social Identity Theory as a relevant framework because it explains the in-group/out-group dynamics and rejection of outside information that are central to the analyst's question. The user could accept this recommendation or select a different theory. The conversational interface would then work with the user to specify the specific methodology to apply the theory to the data. In this example, Social Identity Theory is selected to demonstrate the complete analytical workflow:

Figure 1.1. Theory Operationalization
```
    User Query: "Why do vaccine hesitant groups 
         reject mainstream health information?"
                      ↓
            Theory Selection:
      Social Identity Theory (SIT)
                      ↓
       SIT Theoretical Requirements:
    ┌──────────────────────────────────────────────┐
    │ 1. Identify group boundaries (in/out-group) │
    │ 2. Measure identity strength for each group │
    │ 3. Assess intergroup bias in information    │
    │ 4. Test: Identity → Bias → Rejection        │
    └──────────────────────────────────────────────┘
                      ↓
    Theory Specifies Cross-Modal Data Flow:
    Graph(communities) → Table(psychology) → Vector(language)
                      ↓
              Hypothesis Testing:
    "High-identity groups will show greater semantic
     distance from out-groups and reject their information"
```
NOTE: Social Identity Theory doesn't just select tools - it specifies how data must flow between modalities to test theoretical predictions. The theory requires identifying groups (graph), measuring their psychological traits (table), and analyzing their language (vector).

Figure 1.2. Theory-Guided Cross-Modal Data Flow
```
    GRAPH MODALITY: Community Detection
    [Input: 7.7M tweets from 2,506 users]
                      ↓
    Output: Community A (512 users: ID001, ID047, ...)
            Community B (887 users: ID023, ID091, ...)
                      ↓
    ┌───────────────────────────────────────────┐
    │ User IDs from communities flow to Table    │
    └───────────────────────────────────────────┘
                      ↓
    TABLE MODALITY: Psychological Analysis
    [Input: User IDs from graph communities]
                      ↓
    Finding: Community A: M=5.8 conspiracy mentality
             Community B: M=2.1 conspiracy mentality
    High-identity users: {ID047, ID091, ID238...}
                      ↓
    ┌───────────────────────────────────────────┐
    │ High-identity user IDs flow to Vector      │
    └───────────────────────────────────────────┘
                      ↓
    VECTOR MODALITY: Language Analysis
    [Input: Tweets from high-identity users]
                      ↓
    Finding: Semantic distance between communities = 0.84
             Network distance × Semantic distance: r=0.67
                      ↓
    THEORY VALIDATION:
    SIT Hypothesis Confirmed: Groups with high identity
    (M>5.0) + high network distance (>3 hops) show 3.2x
    higher rejection of out-group health information
```
NOTE: In this example using Social Identity Theory, data flows from graph to table to vector based on the theory's requirements. Each modality's output becomes the next modality's input, testing specific theoretical predictions about how group identity drives information rejection. Different theories would specify different data flow patterns - Network Contagion Theory might trace information cascades through the graph then analyze adopter characteristics, while Motivated Reasoning might start with psychological profiles then trace their information consumption patterns.

## 1.5 Research Scope and Approach

The proposed system will test application of multiple theories to the COVID dataset through cross-modal analysis. The scope will focus on online discourse as a bounded context, testing whether communication patterns can predict behavioral outcomes while acknowledging the complexity of inferring offline actions from online discourse. The research will be presented in a three-part structure: theoretical framework, system implementation, and empirical demonstration.

## 1.6 Contributions to Policy Analysis and Computational Social Science

This research lays the groundwork for theory-aware computational social science using a standardized format for representing theories (the Theory Meta-Schema), an organizing principle for theory selection (the DEPI framework), and an analytical method that guides data from one representation to another (cross-modal data flow). 

Technical contributions include automated theory extraction from academic literature, uncertainty propagation across modalities, and adaptation of Intelligence Community standards for academic research (Office of the Director of National Intelligence, 2007; 2012). We have developed prototype plugins for qualitative coding and process tracing, which demonstrated feasibility. Integrating them into the full system is a key part of this proposal. The system will be validated using the COVID dataset (see Chapter 8), though it will work with any theoretical domain. (Human subjects protection considerations detailed in Annex C)

# Chapter 2: Research Questions and Contributions

## 2.1 Primary Research Questions

Where Chapter 1 established the policy need for scalable theory application, this chapter articulates the research questions driving the dissertation.

The first research question will assess automated theory operationalization through a theory-first approach: To what extent can LLM-based computational systems automate the operationalization and application of social science theories for large-scale discourse analysis? This question examines the current fragmentation where researchers apply familiar theories without systematic consideration of alternatives, and where computational tools apply data-driven methods that require post-hoc theoretical interpretation. The assessment will measure the system's capability to extract theories from academic literature using the Theory Meta-Schema and convert them into computational specifications that determine analytical workflows. The evaluation will establish baseline performance metrics for generating dynamic workflows (DAGs) based on theoretical needs instead of using generic data processing templates. The framework will organize theories based on established communication principles and analytical goals (description, explanation, prediction, and intervention).

The second research question will investigate multi-theoretical analysis: What is the nature of the complementary insights that emerge when the same discourse is analyzed through different theoretical lenses (such as Social Identity Theory, Network Contagion Theory, or Terror Management Theory) using cross-modal analysis? The research will document and characterize how theoretical lens selection shapes both the data flow patterns and the insights generated.

The third research question will evaluate practical impact: How does theory-aware workflow composition perform in its ability to improve the detection and understanding of policy-relevant patterns in fringe discourse? The assessment will establish baseline measurements for whether theory-guided analysis reveals patterns that single-method approaches miss.

## 2.2 Positioning Relative to Existing Approaches

KGAS's theory-first architecture fundamentally differs from current data-driven computational methods that find patterns first and apply theoretical interpretations post-hoc. Where existing systems optimize for information retrieval and pattern discovery, KGAS starts with theories that specify which patterns to look for and how data should flow between analytical modalities. This approach inverts the traditional relationship between computation and theory, using theoretical frameworks to guide the entire analytical process from the outset. (See Section 4.1 for detailed comparison with specific systems including RAG, GraphRAG, DIGIMON, and other approaches)

## 2.3 Policy and Academic Contributions

For policy practitioners, the system will automate workflows that previously took months of manual analysis. The multi-level analytical approach will enable testing how different intervention strategies might affect discourse dynamics at individual, group, and population levels.

The methodological contribution will be a framework for computational theory application that builds on Larson et al. (2009) and integrates Lasswell's (1948) and Druckman's (2022) communication models. This dissertation will apply their multi-level approach using computational methods (detailed in Chapter 3).

## 2.4 Validation Approach

The system will be validated through comprehensive assessment of its theoretical and computational capabilities (detailed methodology in Chapter 8, framework in Annex B). The validation will focus on whether the system can extract and apply theories correctly, not whether the theories themselves are true. Multiple validation methods will be employed based on data availability and dimension requirements.

## 2.5 Scope and Limitations

The research will focus on fringe discourse for demonstration, analyzing discourse patterns and their theoretical implications. Theory discovery and generation remain future work. The research acknowledges limitations in predicting offline behavior from online discourse.


# Chapter 3: Theoretical Framework

Where Chapter 2 identified the research questions, this chapter develops the theoretical framework that will guide their investigation.

## 3.1 Systematic Theory Selection Framework

Computational social science needs a systematic way to match research questions with theories based on their analytical capabilities, not just disciplinary familiarity. This dissertation proposes a framework that maps research questions across analytical goals (Describe, Explain, Predict, Intervene), analysis levels (Individual, Group, Mass Public), and discourse elements (Who, What, Whom, Channel, Settings, Effect) to enable precision in theory selection.

For example, analyzing COVID discourse with the research question "Why did vaccination become a group identity marker?" would be mapped as: Goal = Explain (causal mechanisms), Level = Group (collective dynamics), Focus = Who/Effect (identity formation/polarization). This mapping reveals options like Social Identity Theory, Terror Management Theory, and Network Contagion Theory. The framework enables researchers to select theories based on precise analytical requirements rather than approximate disciplinary fits.

The proposed framework organizes theories by their analytical capabilities using Lasswell's (1948) communication structure - who says what to whom in what channel in what settings with what effect - combined with analytical goals, creating a structured space for systematic theory selection and application.

## 3.2 DEPI×Level×Component Framework

The framework organizes theories along three dimensions that capture their analytical capabilities and scope. This three-dimensional space enables matching between research questions and theoretical frameworks.

Figure 3.1: Three-Dimensional Theory Organization Framework

```
    Analytical Goals         Levels              Components
    ┌──────────────┐      ┌──────────┐       ┌──────────┐
    │ • Describe   │      │• Individual│      │ • Who    │
    │ • Explain    │  ×   │• Group     │   ×  │ • What   │
    │ • Predict    │      │• Mass Public│     │ • To Whom│
    │ • Intervene  │      └──────────┘       │ • Channel│
    └──────────────┘                          │ • Settings│
                                              │ • Effect │
                                              └──────────┘
                              ↓
          72 potential analytical configurations
```
NOTE: This framework organizes theories by their analytical capabilities rather than disciplinary origins, enabling systematic theory selection based on research needs.

The first dimension uses the DEPI taxonomy: Describe, Explain, Predict, and Intervene. The second dimension specifies levels of analysis (Individual, Group, Mass Public). The third dimension identifies communication components from Lasswell/Druckman (Who, What, To Whom, Channel, Settings, Effect).

## 3.3 Theory Meta-Schema Architecture

The Theory Meta-Schema will operate through a three-level architecture that enables systematic theory application to specific datasets while preserving theoretical fidelity:

**Theory Meta-Schema (Level 1)** defines the universal template structure for representing any social science theory. This template has standardized components like metadata (theory name, authors, and other theories it builds on), theoretical structure (entities, relations, and modifiers), computational representation (data formats), and computational patterns (methods the theory uses). This template ensures consistent theory representation across diverse theoretical frameworks.

**Theory Schema (Level 2)** instantiates specific theories using abstract theoretical concepts. For example, Social Identity Theory contains abstract entities such as "in-group" and "out-group," abstract relations such as "exhibits favoritism toward" and "competes with," and abstract modifiers such as "under intergroup threat conditions." This level preserves the theory's original terminology and conceptual structure without reference to specific empirical contexts.

**Theory-Text Schema (Level 3)** applies the abstract theory to specific data contexts, mapping abstract concepts to concrete instances. When applied to COVID discourse, "in-group" becomes "vaccine hesitant groups," "out-group" becomes "pro-vaccine groups," and the relation "exhibits favoritism toward" becomes "endorses vaccine hesitant posts." This mapping enables computational analysis while maintaining a clear link to the theory.

This architecture allows for systematic theory application by providing structure (Meta-Schema), preserving concepts (Theory Schema), and enabling analysis (Theory-Text Schema). The system maintains provenance across all levels, ensuring results can be traced from specific findings back to abstract theoretical principles. (See Annex A for full specification with Social Identity Theory example) 

## 3.4 Computational Implementation

The Theory Meta-Schema will encode theories using seven methodological patterns that theories prescribe for data analysis:

- **EXPRESSIONS**: Direct mathematical calculations (e.g., Social Identity Theory's in-group favoritism = (in-group_rating - out-group_rating) / scale_range)
- **ALGORITHMS**: Iterative computational methods (e.g., Social Identity Theory's community detection to identify group boundaries through modularity optimization)
- **PROCEDURES**: Branching analytical workflows (e.g., Social Identity Theory's threat assessment: if intergroup competition detected, assess resource scarcity; if resource scarcity high, examine zero-sum beliefs; iterate until threat level determined)
- **SEQUENCES**: Linear analytical stages (e.g., Social Identity Theory's identity formation analysis: categorization → identification → comparison → positive distinctiveness)
- **RULES**: Classification logic (e.g., Social Identity Theory: IF shared_group_membership AND out-group_threat > threshold THEN classify as in-group_bias_context)
- **STATISTICAL_MODELS**: Relationship testing in data (e.g., Social Identity Theory SEM: group_identification → collective_self_esteem → in-group_favoritism with mediation analysis)
- **SIMULATIONS**: Agent-based pattern generation (e.g., Social Identity Theory ABM: agents dynamically form groups, develop identification levels, and exhibit bias based on threat, generating polarization patterns)

Each pattern represents a specific operation a theory requires. EXPRESSIONS calculate metrics directly. ALGORITHMS iterate to convergence. PROCEDURES branch through decision trees. SEQUENCES progress through ordered stages. RULES classify based on conditions. STATISTICAL_MODELS test theoretical relationships. SIMULATIONS generate emergent patterns from theoretical rules. This comprehensive categorization enables KGAS to implement the full range of methodological operations social science theories prescribe.

## 3.5 Cross-Modal Data Representation

The framework allows analysis across graph, table, and vector representations, with theories specifying the data formats and flow patterns (as shown in Chapter 1). The system converts data between formats without losing key information and tracks the data back to its source.

## 3.6 Theory Selection and Application Process

The framework enables two approaches to theory selection and application, supporting different research needs and expertise levels.

**Guided Theory Selection** combines automated matching with interactive exploration to identify appropriate theories. The system parses research questions to identify key elements - actors, behaviors, outcomes, and scope - which map to the DEPI×Level×Component framework to determine analytical requirements. When automated matching requires clarification, the system engages users through conversational interface, asking about analytical goals, phenomenon scope, and desired insights. The Master Concept Library provides vocabulary for matching question concepts to theoretical constructs, while vector embeddings enable semantic similarity matching when exact terminology differs. Based on this analysis, the system presents relevant theoretical options organized by analytical capability, allowing researchers to explore trade-offs between different theoretical perspectives. This approach supports both experienced researchers seeking confirmation of theory choices and those less familiar with the full range of available theories.

Users can either select from existing theories in the system or upload documents containing theories or methodologies they wish to apply. When uploading new theoretical content, the system extracts the theoretical framework using the Theory Meta-Schema format, enabling immediate integration into the analytical workflow.

**Manual Browsing** presents theories organized by analytical purpose rather than disciplinary boundaries. Researchers can browse theories that describe patterns, explain mechanisms, predict outcomes, or design interventions. Within each category, theories are grouped by the level of analysis and communication components they address. This organization reveals theoretical options that disciplinary training might overlook.

The framework focuses on theories with operationalizable methods - those that can be translated into computational procedures for data analysis. Theories that can't be systematically applied through computational methods are outside the system's scope, though this limitation aligns with policy analysis requirements where theoretical insights must translate into actionable analytical approaches. Most theories span multiple analytical categories (such as Social Identity Theory, which can describe group formation, explain polarization mechanisms, and predict response to interventions), so the framework's categorization supports rather than constrains theoretical flexibility.


# Chapter 4: System Architecture

Where Chapter 3 established the theoretical framework for organizing and applying theories, this chapter describes the computational architecture that operationalizes that framework.

## 4.1 Positioning Relative to Existing Systems

Current approaches to computational discourse analysis face limitations in handling theoretical complexity and analytical diversity. Older methods like SpaCy and traditional social network analysis tools offer specific capabilities but need manual integration to apply theories. These tools extract entities or calculate network metrics without understanding the significance of what they measure. Researchers must manually interpret outputs through theoretical lenses, creating a workflow where computational tools and theoretical analysis remain disconnected. In essence, these tools and methods are data-driven, not theory-driven.

Standard Retrieval-Augmented Generation (RAG) systems retrieve text chunks based on semantic similarity, treating documents as collections of passages. This approach cannot represent the causal relationships in social science theories (such as Social Identity Theory's chain) from threat through categorization to bias. While RAG improves LLM accuracy through external knowledge, it focuses on information retrieval rather than theoretical application.

Microsoft GraphRAG and similar systems like DIGIMON (with its 16 atomic operators for graph retrieval) improve on flat retrieval by extracting entities and relationships to build graph structures. GraphRAG specifically uses hierarchical community summaries and LLM-based graph construction to capture document structure, while DIGIMON provides modular operators for graph traversal and subgraph extraction. However, these systems create a single, data-driven representation based on what appears in the text. They excel at graph-based retrieval optimization but lack the capability to apply different theoretical lenses to the same data. 

StructGPT demonstrates reasoning over structured data through its Iterative Reading-then-Reasoning (IRR) framework, using interfaces to query databases and having the LLM reason over results. Google's Concordia framework and Park et al.'s (2023) generative agents show promise for agent-based simulation but require manual behavioral rule specification. These limitations are problematic when analyzing polarized discourse where echo chamber effects require different theoretical frameworks to understand information diffusion versus attitude formation.

KGAS differs by using its theory-first architecture to generate different representations based on the selected theory. Unlike other systems designed for current applications, KGAS will be designed as infrastructure for future autonomous research agents. The system's theory-driven workflow differs from data-driven approaches:

Figure 4.2: Comparison of Standard RAG and KGAS Theory-First Workflows

```
    Standard RAG/GraphRAG:
    Query → Retrieve → Generate → Response
    
    KGAS Theory-First:
    Research Question
           ↓
    Theory Selection
           ↓
    Theory Extraction ←─── Academic Literature
           ↓
    Multi-Modal Analysis
    ┌──────┴──────┬──────────┐
    Graph      Table      Vector
    Analysis   Analysis   Analysis
    └──────┬──────┴──────────┘
           ↓
    Integrated Results with Uncertainty
```
NOTE: Unlike data-driven retrieval systems, KGAS uses theoretical frameworks to guide all analytical operations from extraction through integration. The theory-first approach ensures that analysis is grounded in established social science knowledge rather than emergent patterns alone. 

For example, when analyzing COVID discourse through Social Identity Theory, the system would construct graphs with in-group and out-group nodes connected by identification edges, then flow identified communities to table analysis for psychological profiling, then high-identity users to vector analysis for language patterns. The same discourse analyzed through Network Contagion Theory would generate individual nodes with influence pathway edges, flowing through different analytical paths based on that theory's requirements.

Table 4.1: Computational Discourse Analysis System Capabilities

| Capability                  | Pre-LLM Tools | Standard RAG | GraphRAG/DIGIMON | KGAS      |
|                            | (SpaCy, SNA)  |              |                  |           |
|----------------------------|---------------|--------------|------------------|-----------|
| Entity Extraction          | ✓             | ✓            | ✓                | ✓         |
| Theory-Aware Processing    | ✗             | ✗            | ✗                | ✓         |
| Cross-Modal Analysis       | ✗             | ✗            | Partial          | ✓         |
| Statistical Integration    | Manual Only   | ✗            | ✗                | ✓         |
| ABM Integration           | ✗             | ✗            | ✗                | ✓         |
| Uncertainty Quantification | ✗             | Basic        | Basic            | IC-Std    |


The system's scale is a key difference from existing approaches. While DIGIMON offers 16 operators and StructGPT provides 8 interfaces, KGAS integrates tools from multiple categories, with suites for document processing, graph operations, statistical analysis (like SEM), vector operations, and cross-modal converters. This scale enables capabilities absent in other systems: automated theory operationalization from academic literature, agent-based model integration for counterfactual analysis, and complete statistical analysis producing APA-formatted outputs.

## 4.2 Cross-Modal Analysis Architecture

The system will implement the cross-modal analysis approach described in Chapter 1, differing from standard RAG systems through theory-guided data flow between analytical representations rather than parallel processing of different data types.


## 4.3 Provenance and Uncertainty Infrastructure

The system will implement W3C PROV-compliant provenance tracking (recording the complete history of data transformations and analytical decisions) with uncertainty audit trails. Every operation records not just its inputs and outputs, but a trail of decisions—like why a theory was chosen or how evidence was weighted. Transformation lineage tracks preservation across format conversions (graph↔table↔vector), maintaining bidirectional links from any finding to originating evidence. The provenance system will capture uncertainty provenance including calculation methods, input confidences, mathematical parameters, and confidence bounds on the uncertainty assessments themselves.

The IC-informed uncertainty framework adapts Intelligence Community methodologies for academic research, using probability bands with mathematical propagation through root-sum-squares.

## 4.4 Two-Layer Theory Architecture

The system implements the Theory Meta-Schema (detailed in Section 3.3) through a two-layer architecture, separating the one-time extraction process from repeatable application to data.

Figure 4.3: Two-Layer Theory Processing Architecture

```
    Layer 1: Theory Extraction (One-time)
    ┌─────────────────────────────────────┐
    │  Academic Literature                │
    │  ↓                                  │
    │  LLM Extraction with Meta-Schema    │
    │  ↓                                  │
    │  Theory Repository                  │
    └─────────────────────────────────────┘
                    ↓
    Layer 2: Question-Driven Application (Repeated)
    ┌─────────────────────────────────────┐
    │  Research Question                  │
    │  ↓                                  │
    │  Select Theory Components           │
    │  ↓                                  │
    │  Apply to Data                      │
    │  ↓                                  │
    │  Generate Results                   │
    └─────────────────────────────────────┘
```
NOTE: Separation of extraction from application enables reuse of theoretical structures across multiple research questions without re-extraction. This architecture solves the efficiency problem of traditional approaches that re-extract theories for each new analysis.

Layer 1 performs theoretical extraction independent of analytical goals. Using a 3-phase LLM extraction process with the Theory Meta-Schema, the system extracts theoretical structures - entities with original terminology, relationships with causal directions, assumptions and boundary conditions, mathematical formulations, and analytical methods. Multi-agent validation helps improve extraction consistency through consensus-based assessment. This extraction occurs once per theory, creating reusable theoretical representations.

Layer 2 enables question-driven analysis using pre-extracted theories. When researchers pose questions, the system selects relevant theoretical components from the extracted structures, applies appropriate analytical methods (formulas, algorithms, procedures, rules, sequences), and generates theory-grounded results. This separation enables capabilities: the same extraction supports multiple research questions, theories can be combined for multi-perspective analysis, and analytical approaches can evolve without re-extraction. The architecture ensures theoretical integrity and allows flexible application to analyze real-world phenomena, not just textual patterns.

## 4.5 Low-Code Interface and User Interaction

The system will provide a low-code interface that enables researchers to apply computational methods without programming expertise. The primary interaction occurs through a conversational chatbot interface that translates natural language requests into computational workflows. Advanced users retain full control through configuration options and direct workflow specification.

The chatbot interface guides users through theory selection by asking clarifying questions about analytical goals. Based on responses, it refines theory selection and explains its choices, helping analysts understand the analytical process.

For researchers with specific theoretical expertise, the system will provide direct theory selection and configuration options. Users can browse available theories organized by analytical capabilities, select specific theories for application, and adjust parameters based on domain knowledge. This flexibility ensures that computational automation enhances rather than replaces scholarly judgment.

The interface abstracts technical complexity while maintaining transparency. Users see which theories are being applied and why, what analytical methods are being used, and how uncertainty is being quantified. Results include natural language explanations linking findings to theoretical constructs, making computational analysis accessible to researchers without technical backgrounds.

## 4.6 Workflow Orchestration and DAG Construction

The system orchestrates complex analytical workflows through Directed Acyclic Graphs (DAGs) that specify sequences of analytical operations. These DAGs ensure reproducible analysis while enabling multi-step processing pipelines.

Each node in the DAG represents a specific analytical operation implemented by a tool. Tools range from basic operations like entity extraction to complex theoretical calculations. The edges specify data flow between operations, ensuring that outputs from one step become inputs to the next. This structure enables complex workflows while maintaining clear execution logic.

The system constructs DAGs automatically based on research questions and selected theories. Advanced users can modify generated DAGs or create custom workflows for specific analytical needs.

DAG execution will include error handling and validation. Each step validates inputs against expected formats and ranges. Failed operations generate informative error messages rather than silent failures. The system tracks provenance throughout execution, recording which tools were used, what parameters were applied, and how results were generated.

## 4.7 System Integration and Tool Architecture

The system will use a bi-store architecture optimized for different analytical requirements: graph database for network structures and relational database for metadata and provenance. Processing will implement theory-specific algorithms through standardized tool contracts. The modular architecture will support specialized analytical plugins, with existing prototypes demonstrating qualitative coding capabilities for systematic discourse categorization and process tracing functionality for tracking causal sequences through temporal data.

## 4.8 Tool Suite Organization

The KGAS system integrates specialized tools across multiple functional categories with common interfaces, enabling flexible composition of analytical workflows. Unlike rigid pipeline systems, KGAS's modular architecture allows tools to be chained in any sequence guided by theoretical requirements. Each tool implements a common interface ensuring behavior and integration patterns.

### Document Processing Suite
- **PDF and document extraction**: Multi-format support including academic papers, reports, and web content
- **Text preprocessing**: Language detection, cleaning, normalization
- **Metadata extraction**: Author, date, citation information
- **Reference parsing**: Bibliography and citation extraction

### Graph Operations Suite
- **Entity extraction**: Theory-guided identification of actors, concepts, and relationships
- **Network construction**: Multiple network types (social, semantic, causal)
- **Graph algorithms**: Centrality, clustering, path analysis, community detection
- **Graph transformation**: Format conversion, projection, filtering

### Statistical Analysis Suite
- **Descriptive statistics**: Summary statistics and distributions
- **Inferential testing**: Hypothesis testing, confidence intervals, effect sizes
- **Regression modeling**: Linear, logistic, multilevel, time series
- **Structural equation modeling (SEM)**: Latent variable models, path analysis
- **Factor analysis**: Exploratory and confirmatory approaches

### Vector Operations Suite
- **Embedding generation**: Multiple embedding models and techniques
- **Similarity calculation**: Cosine, Euclidean, and custom metrics
- **Clustering**: K-means, hierarchical, DBSCAN, spectral
- **Dimensionality reduction**: PCA, t-SNE, UMAP
- **Semantic search**: Efficient vector similarity search

### Model Context Protocol (MCP) Integration Suite
- **Tool interface standardization**: Common contracts for all analytical tools
- **Cross-system communication**: Standardized messaging between components
- **External tool integration**: Connections to R, Python statistical packages
- **Workflow orchestration**: Coordination of multi-tool analytical pipelines

## 4.9 Data Storage Architecture

### Graph Database (Neo4j)
Stores knowledge graph structures including:
- **Entities**: Actors, concepts, organizations with properties
- **Relationships**: Typed connections with attributes
- **Communities**: Hierarchical clustering results
- **Temporal networks**: Time-indexed interaction patterns

### Relational Database (SQLite)
Maintains operational metadata:
- **Provenance records**: Complete transformation history
- **Uncertainty tracking**: Confidence scores and propagation
- **Workflow states**: DAG execution status and checkpoints
- **Analysis results**: Structured outputs from tools

### Document Store
Preserves original sources:
- **Raw documents**: PDFs, web pages, social media posts
- **Extracted text**: Clean, processed content
- **Metadata**: Bibliographic information, timestamps
- **Annotations**: Theory applications, coding results


# Chapter 5: Essay 1 Method - Theory Integration Approach

Where Chapter 4 specified the system architecture, this chapter details the methodology for developing the theoretical integration framework that will drive that system.

## 5.1 Framework Development Methodology

This essay develops a framework for organizing theories by analytical capabilities using the DEPI framework presented in Chapter 3. The methodology integrates Larson et al.'s (2009) analytical levels, Lasswell's (1948) communication model as extended by Druckman (2022), and the DEPI taxonomy to create the three-dimensional organizational space detailed in Section 3.2.

## 5.2 Theory Organization Process

The framework organizes theories from communication, psychological, social, and behavioral sciences based on their analytical capabilities. Most theories can operate across multiple dimensions of the DEPI×Level×Component framework. For example, Social Identity Theory can be descriptive (describing group formation patterns), explanatory (explaining bias mechanisms), and predictive (predicting intergroup conflict), while operating primarily at the group level but extending to individual psychology and Mass Public dynamics. Similarly, Diffusion of Innovations spans descriptive (adoption patterns), predictive (forecasting spread), and intervention (designing adoption strategies) purposes across individual adoption decisions, group influence networks, and Mass Public communication channels. This multi-dimensional nature reveals complementarities for integration and gaps requiring development, enabling systematic theory selection based on research questions rather than disciplinary boundaries.

## 5.3 Theory Meta-Schema Development

The Theory Meta-Schema specification (detailed in Section 3.3, full specification in Annex A) provides templates for representing theories in computational form. Example schemas for key theories demonstrate how theoretical concepts translate to machine-readable specifications.

## 5.4 Framework Assessment

The framework's effectiveness will be assessed through its ability to guide theory selection and application (see Chapter 8 for validation methodology). Key aspects include whether the organizational structure helps users identify appropriate theories and whether multi-theoretical analysis produces complementary insights.

## 5.5 Expected Deliverables

This essay will produce several key deliverables that establish the theoretical foundation for the computational system. The primary deliverable is the three-dimensional organizing framework (Section 3.2) that classifies theories by their analytical capabilities. The Theory Meta-Schema specification (Section 3.3, Annex A) provides templates with example schemas demonstrating how different types of theories translate to machine-readable specifications.

A populated theory library will include initial theory-schemas for theories spanning the framework dimensions. This library will emphasize theories relevant to fringe discourse analysis, including those addressing identity formation, information processing, social influence, and belief dynamics. Each theory will be documented with its theoretical origins, computational specifications, and application examples.

The framework documentation will include guidelines for adding new theories, extending existing theories, and identifying theoretical gaps. This ensures the framework can evolve as new theories emerge or existing theories are refined. The documentation will also address how to handle theories that span multiple dimensions or challenge the organizational structure.

# Chapter 6: Essay 2 Method - System Development

Where Chapter 5 developed the theoretical integration framework, this chapter describes the methodology for implementing the computational system.

## 6.1 Dataset Specification and Preparation

The system development will use the Kunst et al. (2024) COVID conspiracy discourse dataset, which provides a combination of behavioral data and psychological profiles. Table 6.1 summarizes the key dataset characteristics.

Table 6.1: Kunst COVID-19 Conspiracy Discourse Dataset Specifications

| Dataset Component         | Specification                           |
|--------------------------|----------------------------------------|
| Users with Profiles      | 2,506 Twitter users                   |
| Behavioral Interactions  | 7.7 million tweets and engagements    |
| Time Period             | December 2019 - December 2021         |
| Psychological Measures   | Conspiracy mentality scale            |
|                         | Collective narcissism scale           |
|                         | Need for chaos scale                  |
|                         | Misinformation susceptibility scale   |
| Network Construction    | Interaction-based (replies, reposts)  |
| Analysis Approach       | Theory-guided cross-modal analysis    |

NOTE: The dataset combines self-reported psychological assessments with behavioral traces, enabling testing of computational methods against psychological measures.

Data preparation will focus on interaction-based network construction rather than follower relationships. Networks will be defined by actual engagement - replies, reposts, and likes - which better reflect active discourse participation than passive following relationships. Data filtering will use LLM-based quality assessment to assign coherence scores (0-1) based on semantic consistency and discourse relevance. Content relevance filtering will identify posts related to COVID, vaccines, and associated conspiracy narratives using keyword matching and semantic similarity.

The psychological data provides a way to compare our computational estimates. The temporal span allows tracking of belief evolution and network dynamics over the pandemic's progression.

Cross-platform considerations will be documented but not implemented in the initial system. While the current dataset focuses on Twitter, the framework will be designed to accommodate other platforms. Platform-specific features such as retweet cascades are Twitter-specific, while general engagement metrics such as likes and replies exist across platforms. The system architecture will maintain flexibility for future multi-platform integration.

## 6.2 System Implementation

The low-code chatbot interface translates natural language requests into computational workflows, democratizing access to theoretical analysis. Directed Acyclic Graphs orchestrate tool execution with automatic construction from research questions. The two-layer architecture separates one-time theory extraction (Layer 1) from repeated application (Layer 2), enabling theory reuse across datasets. 

Building on prototype work, the system incorporates plugins for qualitative analysis methods. A qualitative coding plugin enables systematic categorization of discourse elements according to theoretical frameworks, while a process tracing plugin tracks causal sequences and decision pathways through discourse evolution. These prototypes demonstrate the feasibility of integrating traditional qualitative methods with computational approaches. (Implementation details in Annex B)

## 6.3 Theory Application Examples

The implementation will demonstrate application of multiple theories (such as Social Identity Theory for group identification, Motivated Reasoning for information processing, and Diffusion of Innovations for narrative spread) to test the system's multi-theoretical analysis capabilities. Performance assessment will follow the validation framework described in Chapter 8.

## 6.4 Performance Considerations

While the system prioritizes theoretical validity over computational performance, practical considerations ensure usability through appropriate tool selection, vectorized operations, and batched language model calls to finish workflows within hours rather than days.

Scalability employs sampling strategies when necessary - complete analysis of 7.7 million interactions may require representative sampling for computationally expensive operations while maintaining data analysis for important steps. Error handling follows fail-fast principles, validating inputs at each step and generating clear error messages rather than questionable results.

## 6.6 Expected Outcomes

The system development will produce a prototype demonstrating theory-aware discourse analysis that validates the feasibility of the approach and provide a foundation for future development.

The primary deliverable will be a system capable of extracting theories from academic literature, applying them to the COVID dataset, and generating analytical results across multiple modalities. The system will demonstrate that computational methods can systematically apply social science theories to large-scale discourse analysis.

Validation results provide evidence about the system's capabilities and limitations. Correlation analyses between computational estimates and psychological measures establish baseline expectations for construct validity. Extraction accuracy metrics identify areas where human validation remains necessary. Cross-modal comparisons reveal which analytical approaches best suit different theoretical questions.

Documentation will include system architecture specifications, theory application examples, and user guides. The architecture documentation will enable future development and extension. Application examples will show how different theories generate insights from discourse data. User guides will help researchers understand when and how to apply the system to their research questions.

# Chapter 7: Essay 3 Method - Analysis and Application

Where Chapter 6 described system implementation and validation, this chapter details the analytical methods enabled by the integrated theoretical and computational framework.

## 7.1 Analytical Methods Progression

This essay demonstrates analytical capabilities enabled by the Theory Meta-Schema, progressing from basic computational methods to cross-modal integration. The demonstration uses the COVID discourse dataset to show how theories translate into analytical approaches.

The analysis moves from simple to complex social science methods. Basic methods apply formulas and rules directly to data. Intermediate methods integrate across data modalities and statistical relationships. Advanced methods include agent-based modeling (ABM) and structural equation modeling (SEM). This progression demonstrates that the Theory Meta-Schema can accommodate the spectrum of social science analytical approaches.

## 7.2 Basic Computational Methods

The system will implement five patterns from theoretical specifications. Formula-based analysis calculates mathematical relationships (Social Learning Theory's influence scores). Algorithmic analysis traces patterns through networks (contagion cascades via breadth-first search). Procedural analysis executes sequential processes (Spiral of Silence decision steps). Rule-based analysis applies logical conditions (Terror Management Theory's threat-defense mechanisms). Sequence analysis tracks temporal progressions, enabling theory combination across time scales - applying Larson's framework for immediate tactical influence processes (emotional appeals, source credibility effects) while simultaneously tracking McGuire's six-step model for longer-term attitude hardening (exposure→attention→comprehension→acceptance→retention→action), revealing how short-term influence operations evolve into durable belief changes.

## 7.3 Cross-Modal Integration

The system will implement cross-modal analysis as established in Chapter 1, testing whether theoretical relationships (such as identity → bias → rejection for Social Identity Theory) hold across connected analytical steps.

## 7.4 Statistical Modeling

The system will demonstrate theory-generated statistical models through regression analysis testing Motivated Reasoning predictions with interaction terms for belief moderation effects, multilevel modeling accounting for users nested within groups with random effects and cross-level interactions, and time series analysis validating Diffusion of Innovations' S-curve patterns for conspiracy narrative adoption. These methods move beyond correlations to test theoretical relationships.

## 7.5 Statistical Integration

Statistical integration demonstrates correlation analysis between graph metrics and psychological measures, pattern detection across different data representations, and testing of theoretical predictions against empirical data. This enables assessment of which theoretical perspectives reveal patterns.

## 7.6 Agent-Based Modeling Implementation

Agent-based modeling (ABM - computational simulation where individual agents follow rules to generate emergent collective behaviors) demonstrates how the Theory Meta-Schema enables theory testing through simulation. The approach would differ from recent LLM-based agent systems by implementing theory-constrained behavioral rules rather than emergent behaviors from language models.

Agent parameterization will utilize the Kunst dataset's personality characteristics. Each agent will receive conspiracy mentality, narcissism, need for chaos, and misinformation susceptibility scores drawn from the distributions. Table 7.1 provides illustrative examples of how psychological traits map to behavioral rules.

Table 7.1: Agent Behavioral Rules

| Psychological Trait        | Potential Behavioral Rule                 |
|---------------------------|--------------------------------------------|
| High Conspiracy Mentality | Preferentially share confirming content   |
| High Narcissism          | Increase posting frequency                |
| High Need for Chaos      | Engage with controversial topics          |
| Low Trust in Authority   | Question official sources                 |

NOTE: These rules demonstrate potential mappings between dataset traits and behaviors. The implementation will explore various theory-behavior combinations to identify patterns that align with observed data, treating these as exploratory relationships rather than validated theoretical claims.

The Reasoned Action Model provides an approach where attitudes, subjective norms, and perceived control generate behavioral intentions.

Figure 7.1. Theory of Reasoned Action and Theory of Planned Behavior
```
External Variables     Beliefs           Evaluations        Core Constructs    Intention       Outcome

┌─────────────────┐                                                            
│ Demographic     │   ┌─────────────┐  ┌─────────────────┐  ┌─────────────┐               
│ variables       │──▶│ Behavioral  │──▶│ Evaluations of  │──▶│  Attitude   │──┐            
├─────────────────┤   │ beliefs     │  │ behavioral      │  │  toward     │  │            
│ Attitudes       │   └─────────────┘  │ outcomes        │  │  Behavior   │  │            
│ towards         │                     └─────────────────┘  └─────────────┘  │            
│ targets         │                                                            │ ┌──────────┐ ┌──────────┐
├─────────────────┤   ┌─────────────┐  ┌─────────────────┐  ┌─────────────┐  │ │Intention │ │          │
│ Personality     │──▶│ Normative   │──▶│ Motivation to   │──▶│ Subjective  │──┼▶│to Perform│─▶│ Behavior │
│ traits          │   │ beliefs     │  │ comply          │  │ Norm        │  │ │Behavior  │ │          │
├─────────────────┤   └─────────────┘  └─────────────────┘  └─────────────┘  │ └──────────┘ └──────────┘
│ Other individual│                                                            │              
│ difference      │   ┌─────────────┐  ┌─────────────────┐  ┌─────────────┐  │              
│ variables       │──▶│ Control     │──▶│ Perceived       │──▶│ Perceived   │──┘              
└─────────────────┘   │ beliefs     │  │ power           │  │ Behavioral  │────────────────▶
                      └─────────────┘  └─────────────────┘  │ Control     │  (Direct path to
                                                             └─────────────┘   behavior in TPB)
                                                                                   
         Each behavior is defined by: Action, Target, Context, Time
```
NOTE: Upper light area shows Theory of Reasoned Action (behavioral beliefs → attitude, normative beliefs → subjective norm, both leading to intention then behavior). Entire figure shows Theory of Planned Behavior, adding control beliefs → perceived behavioral control, which influences both intention AND directly influences behavior.

The simulation uses ABM frameworks where agents interact with content, process it based on a theory, update their beliefs, and generate discourse. Multiple theories operate simultaneously governing different behavioral aspects.

The simulation can explore parameter variation within theoretical bounds, with parameter ranges drawn from the distribution's 5th-95th percentiles unless literature specifies otherwise. Assessment would compare aggregate patterns including adoption curves and polarization levels against observed data. Intervention modeling could test strategies such as counter-narratives targeting high-influence nodes or network modifications to reduce echo chamber effects, providing tools for exploring outcomes before resource commitment.

# Chapter 8: Validation Strategy

Where Chapter 7 demonstrated the analytical capabilities of the system, this chapter specifies the validation strategy for assessing its effectiveness.

## 8.1 Multi-Dimensional Baseline Establishment Framework  

The validation strategy aims to establish performance baselines for a novel methodology rather than meeting predetermined targets. Because this is a foundational investigation into theory-aware computational social science, no established metrics or benchmarks currently exist for many of the system's capabilities. The research will measure the system's capability to extract and apply theories, establishing first-ever performance metrics that will serve as initial standards for future work in this area. A comprehensive uncertainty assessment framework spanning fourteen dimensions appears in Annex B, with validation efforts prioritizing dimensions most relevant to demonstrating system capabilities based on data availability and feasibility.

## 8.2 Extraction and Theory Application Validity

Entity extraction validity will assess theory structure completeness via hand-coding theories and comparing extractions using graph edit distance, and inter-LLM reliability tests demonstrating reproducibility though consistency doesn't guarantee accuracy. Continuing with the Social Identity Theory example from Chapter 1, validation would verify that the system correctly identifies anti-vaccine groups as in-groups, public health authorities as out-groups, and vaccine safety concerns as threat narratives - validating that theoretical constructs map accurately to discourse elements.

## 8.3 Construct Validity Assessment

Construct validity will test computational construct accuracy by extracting construct estimates from COVID dataset tweets and correlating with psychological measures (conspiracy mentality, narcissism, need for chaos). Proposed crowd-sourced coding may supplement validation of computational constructs against human judgments. Correlations will be reported to assess pattern extraction capabilities. Theory replication applies extracted theories to original paper datasets comparing structural similarity and directional agreement.

## 8.4 Cross-Modal Coherence and System Capability Assessment

Cross-modal coherence assessment will document modality-specific insights (graph for networks, table for statistics, vector for semantics) and test multi-resolution coherence across individual, network, and population levels. System capability assessment can evaluate multiple dimensions including source credibility ratings, cross-source contradiction detection, entity identity resolution, and reasoning chain validity.

## 8.5 Baseline Performance Measurement Approach

This study's validation approach focuses on establishing a baseline of performance rather than meeting predetermined targets. Because this is a foundational investigation into a novel methodology, the research will report actual performance metrics across  validated dimensions, which will serve as the initial standard for future work in this area. Process metrics will establish operational baselines: median extraction time per theory, percentage of elements captured, and cross-modal conversion rates. Entity extraction capabilities will be documented as first-ever measurements for theory-aware systems. Construct correlations will be reported without required effect sizes, providing initial benchmarks for computational construct validity. Prediction remains aspirational given social science complexity. Limitations include online discourse restriction, correlational findings, platform specificity, inter-LLM consistency not accuracy, and unavailable ground truth for complex constructs.

# Chapter 9: Timeline and Limitations

Where Chapter 8 established the validation strategy, this final chapter addresses practical implementation considerations and acknowledges research boundaries.

## 9.1 Research Timeline

The dissertation proceeds through three integrated phases over 12 months from proposal approval (September 2025) through defense (February 2026). (See Annex D for detailed timeline with milestones, deliverables, and risk management)

## 9.2 Scope and Limitations

The research demonstrates computational theory application to online discourse without claiming social prediction. Scope encompasses extracting/applying established theories rather than discovering new frameworks. Technical limitations include LLM dependence, single-node constraints, and preserving theoretical nuance computationally. Validation limitations include absent ground truth for constructs, observational data causality challenges, and platform-specific dataset biases.

## 9.3 Contributions to Future Research

This dissertation will establish foundations for theory-aware computational social science through a theory-first architecture designed for the era of increasingly capable language models. The Theory Meta-Schema will provide evolving templates for computational theory representation that will improve as LLM theoretical understanding advances. The validation framework will assess computational theory application against field-standard thresholds, establishing baselines that future systems can build upon. The cross-modal architecture will integrate analytical perspectives while maintaining theoretical fidelity, providing infrastructure that will scale with LLM capabilities.

KGAS will serve as research infrastructure for a future where autonomous research agents conduct sophisticated theoretical analysis. While current system performance may not meet operational demands, the theory-first foundations established here will enable dramatic improvements as LLMs advance. Current metrics provide baselines for next-generation systems - as language models strengthen theoretical understanding, extraction accuracy will improve; as reasoning capabilities expand, workflow generation will become more sophisticated; as context windows grow, multi-theoretical integration will become more nuanced. The research demonstrates that theory-aware computational social science is possible while establishing frameworks that will accommodate the rapidly advancing capabilities of future language models.

---



{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Theory Meta-Schema v13.0",
  "description": "Value-driven meta-schema focusing on two-layer architecture with added theory validity evidence tracking",
  "version": "13.0",
  "type": "object",
  "required": [
    "theory_id",
    "theory_name",
    "version",
    "metadata",
    "theoretical_structure",
    "computational_representation",
    "algorithms",
    "telos",
    "theory_validity_evidence"
  ],
  "properties": {
    "theory_id": {
      "type": "string",
      "pattern": "^[a-z_][a-z0-9_]*$",
      "description": "Unique identifier for the theory"
    },
    "theory_name": {
      "type": "string",
      "description": "Human-readable name of the theory"
    },
    "version": {
      "type": "string",
      "pattern": "^\\d+\\.\\d+(\\.\\d+)?$",
      "description": "Semantic version of this theory schema"
    },
    "metadata": {
      "type": "object",
      "required": [
        "authors",
        "publication_year",
        "citation",
        "description"
      ],
      "properties": {
        "authors": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "Key theorists and contributors"
        },
        "publication_year": {
          "type": "integer",
          "minimum": 1900,
          "maximum": 2030,
          "description": "Year of seminal publication"
        },
        "citation": {
          "type": "string",
          "description": "Full bibliographic citation for the source document"
        },
        "description": {
          "type": "string",
          "description": "Concise summary of the theory"
        },
        "theoretical_provenance": {
          "type": "object",
          "description": "How this theory relates to other theories",
          "properties": {
            "extends": {
              "type": "string",
              "description": "Theory this modifies/extends (e.g., TPB extends TRA)"
            },
            "replaces": {
              "type": "string",
              "description": "Theory this supersedes"
            },
            "synthesizes": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Theories this combines"
            },
            "contradicts": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Theories this challenges"
            },
            "related_theories": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "theory": {
                    "type": "string"
                  },
                  "relationship": {
                    "type": "string"
                  },
                  "description": {
                    "type": "string"
                  }
                }
              }
            }
          }
        },
        "scope_from_paper": {
          "type": "object",
          "description": "Scope and boundaries as stated by authors",
          "properties": {
            "phenomena_explained": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "What the authors claim their theory explains"
            },
            "boundary_conditions": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "When theory doesn't apply according to authors"
            },
            "level_of_analysis": {
              "type": "string",
              "enum": [
                "individual",
                "dyad",
                "group",
                "organization",
                "society",
                "multiple"
              ],
              "description": "Primary level addressed by theory"
            }
          }
        }
      }
    },
    "theory_validity_evidence": {
      "type": "object",
      "description": "Evidence supporting the validity and quality of the theory",
      "required": [
        "empirical_support",
        "citation_metrics",
        "replication_status"
      ],
      "properties": {
        "empirical_support": {
          "type": "object",
          "description": "Empirical evidence supporting the theory",
          "properties": {
            "supporting_studies": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "citation": {
                    "type": "string",
                    "description": "Citation of supporting study"
                  },
                  "sample_size": {
                    "type": "integer",
                    "description": "Sample size of the study"
                  },
                  "effect_size": {
                    "type": "number",
                    "description": "Reported effect size"
                  },
                  "p_value": {
                    "type": "number",
                    "description": "Statistical significance"
                  },
                  "study_type": {
                    "type": "string",
                    "enum": [
                      "experimental",
                      "quasi-experimental",
                      "correlational",
                      "longitudinal",
                      "meta-analysis",
                      "systematic_review"
                    ]
                  }
                }
              }
            },
            "contradicting_studies": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "citation": {
                    "type": "string"
                  },
                  "nature_of_contradiction": {
                    "type": "string"
                  },
                  "proposed_explanation": {
                    "type": "string"
                  }
                }
              }
            },
            "meta_analyses": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "citation": {
                    "type": "string"
                  },
                  "number_of_studies": {
                    "type": "integer"
                  },
                  "total_sample_size": {
                    "type": "integer"
                  },
                  "pooled_effect_size": {
                    "type": "number"
                  },
                  "heterogeneity_i2": {
                    "type": "number",
                    "description": "I-squared heterogeneity statistic"
                  }
                }
              }
            }
          }
        },
        "citation_metrics": {
          "type": "object",
          "description": "Citation-based metrics of theory impact",
          "properties": {
            "total_citations": {
              "type": "integer",
              "description": "Total citations of seminal paper"
            },
            "citations_per_year": {
              "type": "number",
              "description": "Average citations per year"
            },
            "h_index_authors": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "author": {
                    "type": "string"
                  },
                  "h_index": {
                    "type": "integer"
                  }
                }
              }
            },
            "field_normalized_citation_score": {
              "type": "number",
              "description": "Citations relative to field average"
            },
            "highly_cited_paper": {
              "type": "boolean",
              "description": "Top 1% in field"
            }
          }
        },
        "replication_status": {
          "type": "object",
          "description": "Replication attempts and outcomes",
          "properties": {
            "direct_replications": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "citation": {
                    "type": "string"
                  },
                  "outcome": {
                    "type": "string",
                    "enum": [
                      "successful",
                      "partial",
                      "failed",
                      "mixed"
                    ]
                  },
                  "notes": {
                    "type": "string"
                  }
                }
              }
            },
            "conceptual_replications": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "citation": {
                    "type": "string"
                  },
                  "outcome": {
                    "type": "string",
                    "enum": [
                      "successful",
                      "partial",
                      "failed",
                      "mixed"
                    ]
                  }
                }
              }
            },
            "replication_index": {
              "type": "number",
              "minimum": 0,
              "maximum": 1,
              "description": "Overall replication success rate"
            }
          }
        },
        "practical_applications": {
          "type": "object",
          "description": "Real-world applications and interventions",
          "properties": {
            "interventions_based_on_theory": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "description": {
                    "type": "string"
                  },
                  "domain": {
                    "type": "string"
                  },
                  "effectiveness": {
                    "type": "string",
                    "enum": [
                      "highly_effective",
                      "moderately_effective",
                      "limited_effectiveness",
                      "ineffective",
                      "unknown"
                    ]
                  },
                  "reference": {
                    "type": "string"
                  }
                }
              }
            },
            "policy_influence": {
              "type": "array",
              "items": {
                "type": "string"
              },
              "description": "Policies influenced by this theory"
            }
          }
        },
        "criticisms_and_limitations": {
          "type": "object",
          "description": "Known criticisms and acknowledged limitations",
          "properties": {
            "theoretical_criticisms": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "criticism": {
                    "type": "string"
                  },
                  "source": {
                    "type": "string"
                  },
                  "response_from_authors": {
                    "type": "string"
                  }
                }
              }
            },
            "methodological_limitations": {
              "type": "array",
              "items": {
                "type": "string"
              }
            },
            "scope_limitations": {
              "type": "array",
              "items": {
                "type": "string"
              }
            }
          }
        },
        "validity_assessment": {
          "type": "object",
          "description": "Overall assessment of theory validity",
          "properties": {
            "internal_validity": {
              "type": "string",
              "enum": ["strong", "moderate", "weak", "unclear"],
              "description": "Logical consistency and coherence"
            },
            "external_validity": {
              "type": "string",
              "enum": ["strong", "moderate", "weak", "unclear"],
              "description": "Generalizability across contexts"
            },
            "construct_validity": {
              "type": "string",
              "enum": ["strong", "moderate", "weak", "unclear"],
              "description": "Measurement validity of constructs"
            },
            "predictive_validity": {
              "type": "string",
              "enum": ["strong", "moderate", "weak", "unclear"],
              "description": "Accuracy of predictions"
            },
            "overall_credibility_score": {
              "type": "number",
              "minimum": 0,
              "maximum": 1,
              "description": "Automated credibility assessment"
            }
          }
        }
      }
    },
    "theoretical_structure": {
      "type": "object",
      "required": [
        "entities",
        "relations"
      ],
      "description": "Layer 1: Complete theoretical structure for extraction",
      "properties": {
        "entities": {
          "type": "array",
          "items": {
            "type": "object",
            "required": [
              "indigenous_name",
              "description"
            ],
            "properties": {
              "indigenous_name": {
                "type": "string",
                "description": "Exact term used by authors"
              },
              "standard_name": {
                "type": "string",
                "description": "Standardized name for cross-theory comparison (optional)"
              },
              "description": {
                "type": "string",
                "description": "Definition from the paper"
              },
              "properties": {
                "type": "array",
                "items": {
                  "type": "object",
                  "required": [
                    "name",
                    "measurement"
                  ],
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "indigenous_name": {
                      "type": "string"
                    },
                    "measurement": {
                      "type": "object",
                      "properties": {
                        "type": {
                          "type": "string",
                          "enum": [
                            "numeric",
                            "categorical",
                            "ordinal",
                            "boolean",
                            "text",
                            "vector"
                          ]
                        },
                        "scale": {
                          "type": "string"
                        },
                        "range": {
                          "type": "array"
                        },
                        "categories": {
                          "type": "array"
                        },
                        "extraction_method": {
                          "type": "string"
                        }
                      }
                    }
                  }
                }
              },
              "constraints": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Constraints from paper (e.g., 'must be non-empty', 'power > 0')"
              },
              "examples_from_paper": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Verbatim examples from source text"
              }
            }
          }
        },
        "relations": {
          "type": "array",
          "items": {
            "type": "object",
            "required": [
              "indigenous_name",
              "from_entity",
              "to_entity"
            ],
            "properties": {
              "indigenous_name": {
                "type": "string",
                "description": "Exact term used by authors"
              },
              "standard_name": {
                "type": "string"
              },
              "from_entity": {
                "type": "string"
              },
              "to_entity": {
                "type": "string"
              },
              "description": {
                "type": "string"
              },
              "properties": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "measurement": {
                      "type": "object"
                    }
                  }
                }
              },
              "constraints": {
                "type": "array",
                "items": {
                  "type": "string",
                  "enum": [
                    "symmetric",
                    "asymmetric",
                    "transitive",
                    "reflexive",
                    "irreflexive",
                    "one-to-one",
                    "one-to-many",
                    "many-to-many"
                  ]
                }
              },
              "logical_properties": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Logical rules about this relation from paper"
              }
            }
          }
        },
        "modifiers": {
          "type": "array",
          "description": "Qualifiers that condition entities/relations",
          "items": {
            "type": "object",
            "properties": {
              "indigenous_name": {
                "type": "string"
              },
              "category": {
                "type": "string",
                "enum": [
                  "temporal",
                  "modal",
                  "certainty",
                  "normative",
                  "contextual"
                ]
              },
              "applies_to": {
                "type": "array",
                "items": {
                  "type": "string"
                }
              },
              "values": {
                "type": "array"
              }
            }
          }
        }
      }
    },
    "computational_representation": {
      "type": "object",
      "required": [
        "primary_format",
        "data_structure"
      ],
      "description": "How to store extracted theoretical structure for computation",
      "properties": {
        "primary_format": {
          "type": "string",
          "enum": [
            "graph",
            "table",
            "matrix",
            "vector",
            "tree",
            "sequence",
            "hypergraph"
          ],
          "description": "Native computational format for this theory"
        },
        "data_structure": {
          "type": "object",
          "description": "Specification for the chosen format",
          "properties": {
            "graph_spec": {
              "type": "object",
              "properties": {
                "directed": {
                  "type": "boolean"
                },
                "weighted": {
                  "type": "boolean"
                },
                "node_types": {
                  "type": "array"
                },
                "edge_types": {
                  "type": "array"
                },
                "allows_self_loops": {
                  "type": "boolean"
                },
                "allows_multi_edges": {
                  "type": "boolean"
                }
              }
            },
            "table_spec": {
              "type": "object",
              "properties": {
                "entity_tables": {
                  "type": "array"
                },
                "relation_tables": {
                  "type": "array"
                },
                "event_tables": {
                  "type": "array"
                }
              }
            },
            "matrix_spec": {
              "type": "object",
              "properties": {
                "type": {
                  "type": "string",
                  "enum": [
                    "adjacency",
                    "correlation",
                    "distance",
                    "transition",
                    "feature"
                  ]
                },
                "dimensions": {
                  "type": "integer"
                },
                "sparse": {
                  "type": "boolean"
                }
              }
            },
            "vector_spec": {
              "type": "object",
              "properties": {
                "dimensions": {
                  "type": "integer"
                },
                "basis": {
                  "type": "array"
                },
                "normalized": {
                  "type": "boolean"
                }
              }
            }
          }
        },
        "projections": {
          "type": "array",
          "description": "Alternative representations for specific computations",
          "items": {
            "type": "object",
            "properties": {
              "format": {
                "type": "string"
              },
              "purpose": {
                "type": "string"
              },
              "conversion_method": {
                "type": "string"
              }
            }
          }
        }
      }
    },
    "algorithms": {
      "type": "object",
      "description": "Calculations and logical procedures from the theory",
      "properties": {
        "mathematical": {
          "type": "array",
          "items": {
            "type": "object",
            "required": [
              "name",
              "formula"
            ],
            "properties": {
              "name": {
                "type": "string"
              },
              "indigenous_name": {
                "type": "string"
              },
              "formula": {
                "type": "string"
              },
              "latex": {
                "type": "string"
              },
              "parameters": {
                "type": "object"
              },
              "interpretation": {
                "type": "string"
              },
              "computational_complexity": {
                "type": "string"
              }
            }
          }
        },
        "logical": {
          "type": "array",
          "items": {
            "type": "object",
            "required": [
              "name",
              "rules"
            ],
            "properties": {
              "name": {
                "type": "string"
              },
              "indigenous_name": {
                "type": "string"
              },
              "description": {
                "type": "string"
              },
              "rules": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "condition": {
                      "type": "string"
                    },
                    "conclusion": {
                      "type": "string"
                    },
                    "logic_notation": {
                      "type": "string"
                    }
                  }
                }
              },
              "implementation_type": {
                "type": "string",
                "enum": [
                  "forward_chaining",
                  "backward_chaining",
                  "constraint_satisfaction",
                  "pattern_matching"
                ]
              }
            }
          }
        },
        "procedural": {
          "type": "array",
          "description": "Step-by-step procedures from the theory",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "steps": {
                "type": "array",
                "items": {
                  "type": "string"
                }
              },
              "requires_data": {
                "type": "array"
              },
              "produces": {
                "type": "array"
              }
            }
          }
        }
      }
    },
    "telos": {
      "type": "object",
      "required": [
        "purpose",
        "analytical_questions",
        "success_criteria"
      ],
      "description": "What the theory is for and what questions it answers",
      "properties": {
        "purpose": {
          "type": "string",
          "description": "Primary purpose of the theory"
        },
        "analytical_questions": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "question_type": {
                "type": "string",
                "enum": [
                  "descriptive",
                  "explanatory",
                  "predictive",
                  "prescriptive",
                  "evaluative"
                ]
              },
              "example_questions": {
                "type": "array",
                "items": {
                  "type": "string"
                }
              },
              "requires_algorithms": {
                "type": "array",
                "items": {
                  "type": "string"
                }
              }
            }
          }
        },
        "success_criteria": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "How to know if theory was successfully applied"
        },
        "value_propositions": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "for_question": {
                "type": "string"
              },
              "provides": {
                "type": "string"
              },
              "example": {
                "type": "string"
              }
            }
          }
        }
      }
    },
    "extraction_process": {
      "type": "object",
      "description": "How to extract theoretical structure from text",
      "properties": {
        "entity_extraction": {
          "type": "object",
          "properties": {
            "method": {
              "type": "string",
              "enum": [
                "llm_guided",
                "pattern_matching",
                "ml_model",
                "hybrid"
              ]
            },
            "prompts": {
              "type": "array"
            },
            "patterns": {
              "type": "array"
            },
            "validation_rules": {
              "type": "array"
            }
          }
        },
        "relation_extraction": {
          "type": "object",
          "properties": {
            "method": {
              "type": "string"
            },
            "dependency_patterns": {
              "type": "array"
            },
            "validation_rules": {
              "type": "array"
            }
          }
        },
        "measurement_extraction": {
          "type": "object",
          "properties": {
            "numeric_patterns": {
              "type": "array"
            },
            "scale_mappings": {
              "type": "object"
            },
            "uncertainty_handling": {
              "type": "string"
            }
          }
        }
      }
    },
    "validation": {
      "type": "object",
      "description": "How to validate extracted theoretical structures",
      "properties": {
        "structural_tests": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "test_name": {
                "type": "string"
              },
              "constraint_checked": {
                "type": "string"
              },
              "failure_action": {
                "type": "string"
              }
            }
          }
        },
        "logical_consistency": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "rule": {
                "type": "string"
              },
              "check_method": {
                "type": "string"
              }
            }
          }
        },
        "empirical_tests": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "claim_from_paper": {
                "type": "string"
              },
              "test_description": {
                "type": "string"
              },
              "expected_result": {
                "type": "string"
              },
              "actual_results": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "date_tested": {
                      "type": "string",
                      "format": "date"
                    },
                    "result": {
                      "type": "string"
                    },
                    "notes": {
                      "type": "string"
                    }
                  }
                },
                "description": "Results from running this test"
              }
            }
          }
        }
      }
    },
    "configuration": {
      "type": "object",
      "description": "Runtime configuration options",
      "properties": {
        "extraction_model": {
          "type": "string",
          "default": "gpt-4",
          "description": "LLM to use for extraction"
        },
        "confidence_threshold": {
          "type": "number",
          "default": 0.7,
          "description": "Minimum confidence for extracted elements"
        },
        "enable_reasoning": {
          "type": "boolean",
          "default": true,
          "description": "Use logical reasoning engine if available"
        },
        "parallel_extraction": {
          "type": "boolean",
          "default": false,
          "description": "Extract entities and relations in parallel"
        }
      }
    },
    "metadata_tracking": {
      "type": "object",
      "properties": {
        "schema_version": {
          "type": "string"
        },
        "created": {
          "type": "string",
          "format": "date-time"
        },
        "last_modified": {
          "type": "string",
          "format": "date-time"
        },
        "validated_against_paper": {
          "type": "boolean"
        },
        "mcl_mappings": {
          "type": "object",
          "description": "Mappings to Master Concept Library (added post-hoc)"
        },
        "dolce_mappings": {
          "type": "object",
          "description": "DOLCE alignments for theory composition (added post-hoc)"
        }
      }
    }
  },
  "mode": "open",
  "schema_id": "theory_meta_schema_v13"
}