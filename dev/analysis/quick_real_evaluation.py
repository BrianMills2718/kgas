#!/usr/bin/env python3
"""
Quick Real Evaluation - Get actual results fast

Use single strategy and small sample to get real performance numbers
"""

import json
import numpy as np
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from statistical_evaluation_framework import StatisticalEvaluator, PersonalityScale
from quick_test_personality_system import QuickPersonalitySystem

def load_and_process_data(n_users: int = 10):
    """Load real data and process personality scores"""
    
    dataset_path = Path(__file__).parent.parent / "uncertainty_stress_test" / "100_users_500tweets_dataset.json"
    
    with open(dataset_path, 'r') as f:
        data = json.load(f)
    
    processed_users = []
    for user in data[:n_users]:  # Limit for speed
        user_info = user.get('user_info', {})
        tweets = user.get('tweets', [])
        
        if len(tweets) < 5:
            continue
            
        # Process personality scores
        processed_scores = {}
        
        # Political orientation (single item)
        if 'political' in user_info and user_info['political']:
            try:
                processed_scores['political'] = float(user_info['political'])
            except:
                pass
        
        if processed_scores:
            processed_users.append({
                'tweets': [tweet['text'] for tweet in tweets[:20] if tweet.get('text')],
                'scores': processed_scores
            })
    
    return processed_users

def run_quick_real_evaluation():
    """Run quick evaluation with real data and real LLM"""
    
    print("=== QUICK REAL EVALUATION ===")
    print("Using actual dataset + actual LLM predictions")
    
    # Load real data
    users = load_and_process_data(n_users=10)
    print(f"Loaded {len(users)} users with real data")
    
    # Initialize systems
    evaluator = StatisticalEvaluator()
    personality_system = QuickPersonalitySystem()
    
    # Focus on political orientation (most reliable single item)
    scale = 'political'
    scale_info = PersonalityScale("Political Orientation", "political", 1, 11, 2.0, 6.0, 2.5)
    
    # Extract ground truth
    valid_users = [u for u in users if scale in u['scores']]
    if len(valid_users) < 5:
        print("âŒ Insufficient users with political scores")
        return
    
    print(f"Evaluating {len(valid_users)} users for political orientation")
    
    y_true = np.array([u['scores'][scale] for u in valid_users])
    
    print(f"Ground truth: mean={np.mean(y_true):.2f}, std={np.std(y_true):.2f}, range={np.min(y_true):.1f}-{np.max(y_true):.1f}")
    
    # Make real predictions
    print("Making real LLM predictions...")
    y_pred = []
    
    for i, user in enumerate(valid_users):
        print(f"  User {i+1}/{len(valid_users)}: ", end="")
        try:
            result = personality_system.predict_user_personality(user['tweets'])
            
            if result and 'political' in result and 'ensemble_prediction' in result['political']:
                pred = result['political']['ensemble_prediction'].get('prediction')
                if pred is not None:
                    y_pred.append(float(pred))
                    print(f"predicted {pred:.1f} (true: {y_true[i]:.1f})")
                else:
                    y_pred.append(6.0)  # Middle value fallback
                    print(f"fallback 6.0 (true: {y_true[i]:.1f})")
            else:
                y_pred.append(6.0)
                print(f"error - fallback 6.0 (true: {y_true[i]:.1f})")
                
        except Exception as e:
            y_pred.append(6.0)
            print(f"exception - fallback 6.0 (true: {y_true[i]:.1f})")
    
    y_pred = np.array(y_pred)
    
    print(f"Predictions: mean={np.mean(y_pred):.2f}, std={np.std(y_pred):.2f}, range={np.min(y_pred):.1f}-{np.max(y_pred):.1f}")
    
    # Statistical evaluation
    result = evaluator.evaluate_prediction_system(y_true, y_pred, scale_info)
    
    print(f"\n=== REAL RESULTS ===")
    print(f"Sample size: {len(y_true)}")
    print(f"Correlation: {result.correlation:.3f} (p = {result.correlation_pvalue:.4f})")
    print(f"MAE: {result.mae:.3f}")
    print(f"RMSE: {result.rmse:.3f}")
    print(f"RÂ²: {result.r_squared:.3f}")
    print(f"Effect size: {result.effect_size_cohens_d:.3f}")
    print(f"Significant: {result.is_significant}")
    
    # Honest interpretation
    if result.correlation < 0.1:
        interpretation = "NEGLIGIBLE - essentially random"
    elif result.correlation < 0.3:
        interpretation = "WEAK - limited utility"
    elif result.correlation < 0.5:
        interpretation = "MODERATE - some utility" 
    else:
        interpretation = "STRONG - substantial utility"
    
    print(f"\nðŸ” HONEST ASSESSMENT:")
    print(f"   {interpretation}")
    print(f"   These are REAL LLM predictions on REAL data")
    
    # Save results
    results = {
        'n_users': len(y_true),
        'ground_truth': y_true.tolist(),
        'predictions': y_pred.tolist(),
        'correlation': float(result.correlation),
        'correlation_pvalue': float(result.correlation_pvalue),
        'mae': float(result.mae),
        'rmse': float(result.rmse),
        'r_squared': float(result.r_squared),
        'is_significant': bool(result.is_significant),
        'interpretation': interpretation
    }
    
    with open('quick_real_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"   Results saved to quick_real_results.json")
    
    return results

if __name__ == "__main__":
    results = run_quick_real_evaluation()