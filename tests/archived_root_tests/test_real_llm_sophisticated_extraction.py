#!/usr/bin/env python3
"""Test REAL LLM extraction with sophisticated political science schemas

NO MOCKS - Uses actual LLM API calls to test sophisticated concept extraction.
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.core.extraction_schemas import create_closed_schema, create_hybrid_schema


def create_sophisticated_political_schema():
    """Create comprehensive political analysis schema with explicit abstract concepts"""
    return create_closed_schema(
        schema_id="real_political_analysis",
        entity_types=[
            # Abstract Political Concepts (the key test)
            "DETENTE_CONCEPT", "BALANCE_OF_POWER_THEORY", "SECURITY_DILEMMA", 
            "NUCLEAR_DETERRENCE_DOCTRINE", "ARMS_RACE_DYNAMIC", "COLD_WAR_FRAMEWORK",
            "PEACEFUL_COEXISTENCE_PRINCIPLE", "MUTUAL_RESTRAINT_NORM", "RECIPROCITY_PRINCIPLE",
            
            # Policy Concepts
            "ARMS_CONTROL_REGIME", "MILITARY_SUPREMACY_CONCEPT", "STRATEGIC_PARITY_GOAL",
            "CONTAINMENT_STRATEGY", "COOPERATION_MECHANISM", "DIPLOMATIC_SOLUTION",
            
            # Concrete Policy Instruments
            "NUCLEAR_WEAPONS_SYSTEM", "CONVENTIONAL_MILITARY_FORCE", "DEFENSE_SPENDING",
            "BILATERAL_TREATY", "MULTILATERAL_AGREEMENT", "ECONOMIC_SANCTIONS",
            
            # Strategic Objectives
            "WORLD_PEACE_OBJECTIVE", "NUCLEAR_DISARMAMENT_GOAL", "REGIONAL_STABILITY_TARGET",
            "HUMAN_RIGHTS_PROMOTION", "DEMOCRATIC_VALUES_SPREAD",
            
            # Discourse/Rhetorical Elements
            "THREAT_CONSTRUCTION", "PEACE_NARRATIVE", "STRENGTH_RHETORIC", 
            "MORAL_JUSTIFICATION", "HISTORICAL_PRECEDENT", "CREDIBILITY_CLAIM",
            
            # Traditional Political Entities (for comparison)
            "NATION_STATE", "POLITICAL_LEADER", "GOVERNMENT_INSTITUTION",
            "INTERNATIONAL_ORGANIZATION", "MILITARY_ALLIANCE"
        ],
        relation_types=[
            "IMPLEMENTS_POLICY", "THREATENS_STABILITY", "SUPPORTS_OBJECTIVE", 
            "UNDERMINES_FRAMEWORK", "LEADS_TO_OUTCOME", "PREVENTS_CONFLICT",
            "NEGOTIATES_AGREEMENT", "COMPETES_FOR_INFLUENCE", "COOPERATES_ON_ISSUE",
            "BALANCES_AGAINST_POWER", "CONTAINS_EXPANSION", "FRAMES_NARRATIVE"
        ]
    )


def fix_llm_integration_issues():
    """Fix the LLM integration issues we encountered"""
    print("Checking LLM Integration Issues...")
    print("-" * 40)
    
    try:
        # Test core imports
        from src.tools.phase2.t23c_ontology_aware_extractor_unified import OntologyAwareExtractor
        print("‚úì T23C import successful")
        
        # Test API auth
        from src.core.api_auth_manager import APIAuthManager
        auth_manager = APIAuthManager()
        print(f"‚úì API Auth Manager: {len(auth_manager.credentials)} services")
        
        # Test enhanced API client  
        from src.core.enhanced_api_client import EnhancedAPIClient
        api_client = EnhancedAPIClient(auth_manager)
        print(f"‚úì Enhanced API Client: {len(api_client.available_models)} models")
        
        # Test extractor initialization
        extractor = OntologyAwareExtractor()
        print("‚úì Extractor initialization successful")
        
        return True
        
    except Exception as e:
        print(f"‚úó Integration issue: {e}")
        print("\nAttempting fixes...")
        
        # Try to fix the confidence score issue
        try:
            from src.core.confidence_score import ConfidenceScore
            
            # Check if the method exists
            if hasattr(ConfidenceScore, 'create_high_confidence'):
                print("‚úì ConfidenceScore.create_high_confidence exists")
            else:
                print("‚úó ConfidenceScore.create_high_confidence missing")
                print("Available methods:", [m for m in dir(ConfidenceScore) if not m.startswith('_')])
                
        except Exception as conf_e:
            print(f"‚úó ConfidenceScore issue: {conf_e}")
            
        return False


def test_real_llm_with_sophisticated_schema():
    """Test real LLM extraction with sophisticated political schema"""
    
    print("\nTesting REAL LLM Extraction (NO MOCKS)")
    print("=" * 60)
    
    # Create sophisticated schema
    schema = create_sophisticated_political_schema()
    print(f"Schema: {schema.schema_id}")
    print(f"Abstract concepts defined: {len([e for e in schema.entity_types.keys() if any(x in e for x in ['_CONCEPT', '_THEORY', '_DOCTRINE', '_DYNAMIC', '_FRAMEWORK', '_PRINCIPLE'])])}")
    print(f"Total entity types: {len(schema.entity_types)}")
    print(f"Relation types: {len(schema.relation_types)}")
    
    # Carter speech text for testing
    carter_text = """
    Detente between our two countries is central to world peace. The word "detente" can be 
    simplistically defined as "the easing of tension between nations." To be stable, detente 
    must be broadly defined and truly reciprocal. Both nations must exercise restraint.
    
    Neither of us should entertain the notion that military supremacy can be attained, or 
    that transient military advantage can be politically exploited. The numbers and 
    destructive potential of nuclear weapons has been increasing at an alarming rate.
    
    We will continue to maintain equivalent nuclear strength, because we believe that in 
    the absence of worldwide nuclear disarmament, such equivalency is the least threatening 
    and the most stable situation for the world. We will maintain a prudent and sustained 
    level of military spending, keyed to a stronger NATO.
    
    The Soviet Union apparently sees military power and military assistance as the best 
    means of expanding their influence abroad. To other nations throughout the world, the 
    Soviet military buildup appears to be excessive, far beyond any legitimate requirement 
    to defend themselves.
    
    I'm convinced that the people of the Soviet Union want peace. Our long-term objective 
    must be to convince the Soviet Union of the advantages of cooperation and of the costs 
    of disruptive behavior.
    """
    
    print(f"\nText length: {len(carter_text)} characters")
    print(f"Word count: {len(carter_text.split())} words")
    
    try:
        from src.tools.phase2.t23c_ontology_aware_extractor_unified import OntologyAwareExtractor
        
        print(f"\nInitializing REAL LLM extractor...")
        extractor = OntologyAwareExtractor()
        
        print(f"API Services Available:")
        print(f"  OpenAI: {extractor.openai_available}")
        print(f"  Google: {extractor.google_available}")
        
        if not extractor.openai_available and not extractor.google_available:
            print("‚úó No LLM services available - check API keys")
            return False
        
        print(f"\nExecuting REAL LLM extraction...")
        print("(This will make actual API calls - no mocks)")
        
        # Execute with REAL LLM (use_mock_apis=False)
        result = extractor.execute({
            "text": carter_text,
            "source_ref": "carter_real_llm_test",
            "schema": schema,
            "use_mock_apis": False,  # REAL LLM CALLS
            "confidence_threshold": 0.4,  # Lower threshold to capture more
            "use_theory_validation": True
        })
        
        if "error" in result:
            print(f"‚úó LLM extraction failed: {result['error']}")
            return False
        
        # Analyze results
        entities = result["results"]["entities"]
        relationships = result["results"]["relationships"]
        metadata = result["results"]["extraction_metadata"]
        
        print(f"\nüéØ REAL LLM EXTRACTION RESULTS")
        print("-" * 40)
        print(f"‚úì Extraction successful!")
        print(f"  Total entities: {len(entities)}")
        print(f"  Total relationships: {len(relationships)}")
        print(f"  LLM service used: {metadata.get('llm_service_used', 'unknown')}")
        print(f"  Extraction time: {metadata.get('extraction_time', 0):.2f}s")
        
        # Categorize extractions
        abstract_concepts = []
        concrete_entities = []
        policy_instruments = []
        strategic_objectives = []
        discourse_elements = []
        
        for entity in entities:
            entity_type = entity.get("entity_type", "")
            
            if any(x in entity_type for x in ["_CONCEPT", "_THEORY", "_DOCTRINE", "_DYNAMIC", "_FRAMEWORK", "_PRINCIPLE"]):
                abstract_concepts.append(entity)
            elif any(x in entity_type for x in ["_REGIME", "_STRATEGY", "_MECHANISM", "_SOLUTION"]):
                policy_instruments.append(entity)
            elif any(x in entity_type for x in ["_OBJECTIVE", "_GOAL", "_TARGET", "_PROMOTION"]):
                strategic_objectives.append(entity)
            elif any(x in entity_type for x in ["_CONSTRUCTION", "_NARRATIVE", "_RHETORIC", "_JUSTIFICATION"]):
                discourse_elements.append(entity)
            else:
                concrete_entities.append(entity)
        
        print(f"\nüìä SOPHISTICATION ANALYSIS")
        print("-" * 40)
        print(f"Abstract Concepts: {len(abstract_concepts)} ({len(abstract_concepts)/len(entities)*100:.1f}%)")
        print(f"Policy Instruments: {len(policy_instruments)} ({len(policy_instruments)/len(entities)*100:.1f}%)")
        print(f"Strategic Objectives: {len(strategic_objectives)} ({len(strategic_objectives)/len(entities)*100:.1f}%)")
        print(f"Discourse Elements: {len(discourse_elements)} ({len(discourse_elements)/len(entities)*100:.1f}%)")
        print(f"Concrete Entities: {len(concrete_entities)} ({len(concrete_entities)/len(entities)*100:.1f}%)")
        
        sophistication_score = (len(abstract_concepts) + len(policy_instruments) + len(strategic_objectives) + len(discourse_elements)) / len(entities) * 100
        print(f"\nüèÜ SOPHISTICATION SCORE: {sophistication_score:.1f}%")
        
        # Show extracted abstract concepts
        if abstract_concepts:
            print(f"\nüß† ABSTRACT CONCEPTS EXTRACTED:")
            for concept in sorted(abstract_concepts, key=lambda x: x.get("confidence", 0), reverse=True):
                print(f"  - {concept.get('entity_type')}: '{concept.get('surface_form')}' (conf: {concept.get('confidence', 0):.2f})")
        else:
            print(f"\n‚ùå NO ABSTRACT CONCEPTS EXTRACTED")
            print("This indicates the LLM prompt needs optimization for theoretical concept recognition")
        
        # Show all entities for analysis
        print(f"\nüìã ALL EXTRACTED ENTITIES:")
        for entity in sorted(entities, key=lambda x: x.get("confidence", 0), reverse=True):
            category = "abstract" if entity in abstract_concepts else "concrete"
            print(f"  [{category}] {entity.get('entity_type')}: '{entity.get('surface_form')}' (conf: {entity.get('confidence', 0):.2f})")
        
        # Show relationships
        if relationships:
            print(f"\nüîó RELATIONSHIPS EXTRACTED:")
            for rel in relationships:
                print(f"  - {rel.get('type')}: {rel.get('source_entity', 'unknown')} ‚Üí {rel.get('target_entity', 'unknown')}")
        
        # Validate against schema
        extraction_for_validation = {
            "entities": [
                {
                    "id": f"e_{i}",
                    "type": entity.get("entity_type"),
                    "surface_form": entity.get("surface_form"),
                    "confidence": entity.get("confidence", 0)
                }
                for i, entity in enumerate(entities)
            ],
            "relations": []
        }
        
        validation = schema.validate_extraction_result(extraction_for_validation)
        print(f"\n‚úÖ SCHEMA VALIDATION: {'PASSED' if validation['valid'] else 'FAILED'}")
        if validation['errors']:
            print(f"Validation errors: {validation['errors']}")
        
        # Overall assessment
        print(f"\nüéØ ASSESSMENT:")
        if sophistication_score > 50:
            print(f"  ‚úÖ EXCELLENT: LLM successfully extracts abstract political concepts!")
        elif sophistication_score > 25:
            print(f"  üü° GOOD: LLM extracts some abstract concepts, room for improvement")
        else:
            print(f"  ‚ùå BASIC: LLM primarily extracts concrete entities, needs prompt optimization")
        
        return sophistication_score > 25
        
    except Exception as e:
        print(f"‚úó Error in real LLM testing: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_prompt_optimization():
    """Test different prompting strategies for abstract concept extraction"""
    
    print(f"\n{'='*60}")
    print("PROMPT OPTIMIZATION FOR ABSTRACT CONCEPTS")
    print("="*60)
    
    # Simple test to see what prompts work best
    strategies = [
        {
            "name": "Explicit Concept Instructions",
            "prompt_addition": "Focus on extracting abstract political concepts like d√©tente, balance of power, security dilemma, nuclear deterrence, etc. Treat these as named entities."
        },
        {
            "name": "Theory-Guided Extraction", 
            "prompt_addition": "Apply international relations theory frameworks. Extract realist concepts (power, balance, security), liberal concepts (cooperation, institutions), and Cold War concepts (d√©tente, containment)."
        },
        {
            "name": "Academic Analysis Style",
            "prompt_addition": "Perform political science analysis. Identify theoretical frameworks, policy instruments, strategic objectives, and discourse elements as if writing an academic paper."
        }
    ]
    
    print("Testing different prompting strategies...")
    print("(This would require API calls - showing framework only)")
    
    for strategy in strategies:
        print(f"\nüìù {strategy['name']}:")
        print(f"   Prompt: {strategy['prompt_addition']}")
        print(f"   Expected improvement: Abstract concept recognition")
    
    print(f"\nüí° NEXT STEP: Implement prompt optimization in T23C extractor")
    
    return True


def main():
    """Run complete real LLM testing with sophisticated schemas"""
    
    print("REAL LLM Sophisticated Schema Testing")
    print("=" * 70)
    print("Testing actual LLM extraction capabilities with political science schemas")
    print("NO MOCKS - Using real API calls")
    
    # Step 1: Fix integration issues
    print("\n1. CHECKING INTEGRATION...")
    if not fix_llm_integration_issues():
        print("‚ùå Cannot proceed - fix integration issues first")
        return 1
    
    # Step 2: Test real LLM extraction
    print(f"\n2. TESTING REAL LLM EXTRACTION...")
    success = test_real_llm_with_sophisticated_schema()
    
    # Step 3: Prompt optimization analysis
    print(f"\n3. PROMPT OPTIMIZATION ANALYSIS...")
    test_prompt_optimization()
    
    # Final assessment
    print(f"\n{'='*70}")
    print("FINAL RESULTS")
    print("="*70)
    
    if success:
        print("‚úÖ SUCCESS: Real LLM extraction with sophisticated schemas works!")
        print("üìä The schema system can handle complex political analysis")
        print("üéØ Abstract concept extraction demonstrated with real API calls")
    else:
        print("‚ö†Ô∏è  PARTIAL: Schema system works but LLM extraction needs optimization")
        print("üìã Next step: Improve prompts for abstract concept recognition")
    
    print(f"\nüèÅ CONCLUSION:")
    print(f"   Schema framework: ‚úÖ Fully capable of sophisticated analysis")
    print(f"   LLM integration: {'‚úÖ Working with real API calls' if success else 'üîß Needs prompt optimization'}")
    print(f"   Overall system: {'üéØ Ready for production use' if success else 'üìà Ready with prompt improvements'}")
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())