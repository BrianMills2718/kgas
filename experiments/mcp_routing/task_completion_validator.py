#!/usr/bin/env python3
"""
Real Task Completion Validator

Tests whether tool selections actually complete tasks successfully,
rather than comparing against theoretical "optimal" choices.
"""

import os
import json
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import statistics

# Import our components
from mock_tool_generator import MockToolGenerator
from simple_gemini_test import SimpleGeminiTester
from run_with_env import load_env

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_env()

@dataclass
class TaskCompletionResult:
    """Results of testing whether selected tools can complete a task"""
    task_name: str
    tools_selected: List[str]
    execution_successful: bool
    quality_scores: Dict[str, float]
    processing_time: float
    requirements_met: Dict[str, bool]
    completion_rate: float
    errors: List[str]
    raw_output: Optional[Dict] = None

@dataclass
class GroundTruthData:
    """Known correct answers for validation"""
    document_id: str
    methodologies: List[str]
    datasets: List[str] 
    metrics: List[str]
    relationships: List[Tuple[str, str, str]]  # (entity1, relation, entity2)
    key_entities: List[str]

class TaskCompletionValidator:
    """Validates tool selections by testing actual task completion"""
    
    def __init__(self):
        self.tool_generator = MockToolGenerator()
        self.gemini_tester = SimpleGeminiTester()
        self.ground_truths = self._create_ground_truth_data()
        
    def _create_ground_truth_data(self) -> Dict[str, GroundTruthData]:
        """Create test cases with known correct answers"""
        return {
            "bert_sentiment_paper": GroundTruthData(
                document_id="bert_sentiment_analysis.pdf",
                methodologies=["BERT", "RoBERTa", "DistilBERT", "Transformer"],
                datasets=["IMDB", "SST-2", "Amazon Reviews", "Yelp Reviews"],
                metrics=["94.2% accuracy", "91.8% F1", "93.1% precision", "89.5% recall"],
                relationships=[
                    ("BERT", "trained_on", "IMDB"),
                    ("BERT", "achieved", "94.2% accuracy"),
                    ("RoBERTa", "outperformed", "BERT"),
                    ("DistilBERT", "faster_than", "BERT")
                ],
                key_entities=["BERT", "sentiment analysis", "transformer", "IMDB", "accuracy"]
            ),
            "financial_report": GroundTruthData(
                document_id="q3_financial_report.pdf",
                methodologies=["DCF analysis", "comparative analysis", "trend analysis"],
                datasets=["Q3 2024 earnings", "market data", "competitor data"],
                metrics=["15% revenue growth", "8.2% profit margin", "$2.1M net income"],
                relationships=[
                    ("Q3 2024", "showed", "15% revenue growth"),
                    ("profit margin", "improved_to", "8.2%"),
                    ("net income", "reached", "$2.1M")
                ],
                key_entities=["revenue", "profit", "Q3 2024", "growth", "margin"]
            ),
            "simple_news_article": GroundTruthData(
                document_id="tech_news_article.txt",
                methodologies=["interviews", "market research", "data analysis"],
                datasets=["user surveys", "market reports", "company statements"], 
                metrics=["50% user adoption", "25% market share", "100+ companies"],
                relationships=[
                    ("new AI tool", "achieved", "50% user adoption"),
                    ("company", "gained", "25% market share"),
                    ("100+ companies", "using", "new AI tool")
                ],
                key_entities=["AI tool", "adoption", "market share", "companies", "users"]
            )
        }
    
    def simulate_pipeline_execution(self, selected_tools: List[str], 
                                  ground_truth: GroundTruthData,
                                  task_complexity: str = "medium") -> Dict[str, Any]:
        """Simulate executing a pipeline with selected tools"""
        
        start_time = time.time()
        errors = []
        extracted_data = {
            "methodologies": [],
            "datasets": [],
            "metrics": [],
            "relationships": [],
            "key_entities": []
        }
        
        # Simulate tool execution based on tool capabilities
        available_tools = {tool.tool_id: tool for tool in self.tool_generator.generate_all_tools()}
        
        # Track what capabilities we have
        capabilities = {
            "document_loading": False,
            "text_processing": False,
            "entity_extraction": False,
            "relationship_extraction": False,
            "graph_building": False,
            "output_formatting": False
        }
        
        # Check each selected tool
        for tool_name in selected_tools:
            if tool_name not in available_tools:
                # Handle tool variants (e.g., tool_v2, tool_v3)
                base_tool = tool_name.split('_v')[0] if '_v' in tool_name else tool_name
                if base_tool not in available_tools:
                    errors.append(f"Tool {tool_name} not found")
                    continue
                tool = available_tools[base_tool]
            else:
                tool = available_tools[tool_name]
            
            # Simulate tool execution based on its category and description
            if "load" in tool.tool_id.lower() or "document" in tool.tool_id.lower():
                capabilities["document_loading"] = True
                
            if "chunk" in tool.tool_id.lower() or "process" in tool.tool_id.lower():
                capabilities["text_processing"] = True
                
            if "extract" in tool.tool_id.lower() and "entities" in tool.tool_id.lower():
                capabilities["entity_extraction"] = True
                # Simulate entity extraction success rate based on tool type
                if "scientific" in tool.tool_id or "llm" in tool.tool_id:
                    success_rate = 0.8  # Higher accuracy for specialized tools
                else:
                    success_rate = 0.6  # Lower for generic tools
                
                # Simulate extracting entities based on success rate
                for entity_type in ["methodologies", "datasets", "key_entities"]:
                    ground_truth_entities = getattr(ground_truth, entity_type)
                    extracted_count = int(len(ground_truth_entities) * success_rate)
                    extracted_data[entity_type] = ground_truth_entities[:extracted_count]
                    
            if "relationship" in tool.tool_id.lower() or "graph" in tool.tool_id.lower():
                capabilities["relationship_extraction"] = True
                # Simulate relationship extraction
                success_rate = 0.7
                relationship_count = int(len(ground_truth.relationships) * success_rate)
                extracted_data["relationships"] = ground_truth.relationships[:relationship_count]
                
            if "build" in tool.tool_id.lower() and "graph" in tool.tool_id.lower():
                capabilities["graph_building"] = True
                
            if "export" in tool.tool_id.lower() or "output" in tool.tool_id.lower():
                capabilities["output_formatting"] = True
        
        # Check if we have minimum viable pipeline
        essential_capabilities = ["document_loading", "entity_extraction"]
        missing_essential = [cap for cap in essential_capabilities if not capabilities[cap]]
        
        if missing_essential:
            errors.append(f"Missing essential capabilities: {missing_essential}")
            execution_successful = False
        else:
            execution_successful = True
        
        processing_time = time.time() - start_time + (len(selected_tools) * 2)  # Simulate processing time
        
        return {
            "execution_successful": execution_successful,
            "extracted_data": extracted_data,
            "capabilities_available": capabilities,
            "processing_time": processing_time,
            "errors": errors
        }
    
    def measure_extraction_quality(self, extracted: Dict[str, Any], 
                                 ground_truth: GroundTruthData) -> Dict[str, float]:
        """Measure quality of extraction against ground truth"""
        
        quality_scores = {}
        
        # Measure each extraction type
        for field_name in ["methodologies", "datasets", "key_entities"]:
            extracted_items = set(extracted.get(field_name, []))
            ground_truth_items = set(getattr(ground_truth, field_name))
            
            if not ground_truth_items:
                quality_scores[f"{field_name}_recall"] = 1.0  # Perfect if nothing to extract
                quality_scores[f"{field_name}_precision"] = 1.0 if not extracted_items else 0.0
            else:
                true_positives = len(extracted_items & ground_truth_items)
                
                recall = true_positives / len(ground_truth_items) if ground_truth_items else 0
                precision = true_positives / len(extracted_items) if extracted_items else 0
                
                quality_scores[f"{field_name}_recall"] = recall
                quality_scores[f"{field_name}_precision"] = precision
                quality_scores[f"{field_name}_f1"] = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0
        
        # Measure relationships
        extracted_rels = set(extracted.get("relationships", []))
        ground_truth_rels = set(ground_truth.relationships)
        
        if ground_truth_rels:
            rel_true_positives = len(extracted_rels & ground_truth_rels)
            rel_recall = rel_true_positives / len(ground_truth_rels)
            rel_precision = rel_true_positives / len(extracted_rels) if extracted_rels else 0
            
            quality_scores["relationships_recall"] = rel_recall
            quality_scores["relationships_precision"] = rel_precision
            quality_scores["relationships_f1"] = (2 * rel_precision * rel_recall / (rel_precision + rel_recall)) if (rel_precision + rel_recall) > 0 else 0
        
        # Overall quality score
        f1_scores = [score for key, score in quality_scores.items() if key.endswith("_f1")]
        quality_scores["overall_f1"] = statistics.mean(f1_scores) if f1_scores else 0.0
        
        return quality_scores
    
    def check_task_requirements(self, extracted: Dict[str, Any], 
                              capabilities: Dict[str, bool],
                              task_type: str) -> Dict[str, bool]:
        """Check if task requirements were met"""
        
        requirements = {
            "academic_analysis": {
                "extracted_methodologies": len(extracted.get("methodologies", [])) > 0,
                "extracted_datasets": len(extracted.get("datasets", [])) > 0,
                "found_relationships": len(extracted.get("relationships", [])) > 0,
                "document_processed": capabilities.get("document_loading", False),
                "entities_identified": capabilities.get("entity_extraction", False)
            },
            "simple_processing": {
                "document_loaded": capabilities.get("document_loading", False),
                "entities_extracted": len(extracted.get("key_entities", [])) > 0,
                "output_formatted": capabilities.get("output_formatting", False)
            },
            "business_analysis": {
                "financial_data_extracted": len(extracted.get("metrics", [])) > 0,
                "entities_identified": len(extracted.get("key_entities", [])) > 0,
                "document_processed": capabilities.get("document_loading", False),
                "relationships_found": len(extracted.get("relationships", [])) > 0
            }
        }
        
        return requirements.get(task_type, {})
    
    def validate_tool_selection(self, selected_tools: List[str], 
                              task_name: str, document_id: str) -> TaskCompletionResult:
        """Validate a tool selection by testing task completion"""
        
        if document_id not in self.ground_truths:
            raise ValueError(f"No ground truth data for document: {document_id}")
        
        ground_truth = self.ground_truths[document_id]
        
        # Simulate pipeline execution
        execution_result = self.simulate_pipeline_execution(selected_tools, ground_truth)
        
        # Measure quality
        quality_scores = self.measure_extraction_quality(
            execution_result["extracted_data"], ground_truth
        )
        
        # Check requirements
        task_type = "academic_analysis" if "paper" in document_id else "simple_processing"
        requirements_met = self.check_task_requirements(
            execution_result["extracted_data"],
            execution_result["capabilities_available"], 
            task_type
        )
        
        completion_rate = sum(requirements_met.values()) / len(requirements_met) if requirements_met else 0.0
        
        return TaskCompletionResult(
            task_name=task_name,
            tools_selected=selected_tools,
            execution_successful=execution_result["execution_successful"],
            quality_scores=quality_scores,
            processing_time=execution_result["processing_time"],
            requirements_met=requirements_met,
            completion_rate=completion_rate,
            errors=execution_result["errors"],
            raw_output=execution_result
        )
    
    def compare_tool_selections(self, task_name: str, document_id: str) -> Dict[str, Any]:
        """Compare different tool selection approaches on the same task"""
        
        # Get Gemini's tool selection
        tools = self.tool_generator.generate_tools(100, seed=42)
        formatted_tools = self._format_tools_for_gemini(tools)
        
        scenario = self.gemini_tester.test_scenarios[1]  # academic_analysis_complex
        gemini_result = self.gemini_tester.test_gemini_structured(
            formatted_tools, scenario, "direct_exposure"
        )
        gemini_tools = gemini_result.selected_tools
        
        # Create alternative selections for comparison
        alternative_selections = {
            "gemini_selected": gemini_tools,
            "workflow_based": ["load_document_pdf", "chunk_text_semantic", "extract_entities_scientific", "build_graph_relationships", "export_json"],
            "comprehensive": ["load_document_pdf", "extract_entities_llm_gpt4", "extract_relationships_llm", "build_knowledge_graph", "analyze_graph_centrality", "export_academic_summary"],
            "minimal": ["load_document_pdf", "extract_entities_basic", "export_csv"],
            "random_sample": self._get_random_tools(5)
        }
        
        comparison_results = {}
        
        for approach_name, selected_tools in alternative_selections.items():
            logger.info(f"Testing {approach_name} approach...")
            
            validation_result = self.validate_tool_selection(
                selected_tools, f"{task_name}_{approach_name}", document_id
            )
            
            comparison_results[approach_name] = {
                "tools": selected_tools,
                "success": validation_result.execution_successful,
                "completion_rate": validation_result.completion_rate,
                "overall_f1": validation_result.quality_scores.get("overall_f1", 0.0),
                "processing_time": validation_result.processing_time,
                "requirements_met": validation_result.requirements_met,
                "errors": validation_result.errors
            }
        
        # Statistical analysis
        successful_approaches = [name for name, result in comparison_results.items() if result["success"]]
        
        if successful_approaches:
            f1_scores = [comparison_results[name]["overall_f1"] for name in successful_approaches]
            completion_rates = [comparison_results[name]["completion_rate"] for name in successful_approaches]
            
            best_f1_approach = max(successful_approaches, key=lambda x: comparison_results[x]["overall_f1"])
            best_completion_approach = max(successful_approaches, key=lambda x: comparison_results[x]["completion_rate"])
            
            statistical_analysis = {
                "successful_approaches": len(successful_approaches),
                "total_approaches": len(alternative_selections),
                "success_rate": len(successful_approaches) / len(alternative_selections),
                "avg_f1_score": statistics.mean(f1_scores),
                "avg_completion_rate": statistics.mean(completion_rates),
                "best_f1_approach": best_f1_approach,
                "best_completion_approach": best_completion_approach,
                "gemini_rank_by_f1": sorted(successful_approaches, key=lambda x: comparison_results[x]["overall_f1"], reverse=True).index("gemini_selected") + 1 if "gemini_selected" in successful_approaches else "failed"
            }
        else:
            statistical_analysis = {
                "successful_approaches": 0,
                "total_approaches": len(alternative_selections),
                "success_rate": 0.0,
                "message": "No approaches succeeded"
            }
        
        return {
            "task_name": task_name,
            "document_id": document_id,
            "approach_results": comparison_results,
            "statistical_analysis": statistical_analysis,
            "timestamp": time.time()
        }
    
    def _format_tools_for_gemini(self, tools):
        """Format tools for Gemini consumption"""
        formatted_tools = []
        for tool in tools:
            formatted_tools.append({
                "name": tool.tool_id,
                "description": tool.description,
                "category": tool.category.value,
                "inputs": tool.input_types,
                "outputs": tool.output_types,
                "complexity": tool.complexity_score
            })
        return formatted_tools
    
    def _get_random_tools(self, count: int) -> List[str]:
        """Get random tool selection for baseline comparison"""
        all_tools = self.tool_generator.generate_all_tools()
        import random
        selected = random.sample(all_tools, min(count, len(all_tools)))
        return [tool.tool_id for tool in selected]
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive validation across multiple scenarios"""
        
        logger.info("üî¨ Starting Real Task Completion Validation")
        
        test_scenarios = [
            ("academic_ml_analysis", "bert_sentiment_paper"),
            ("business_report_analysis", "financial_report"),
            ("simple_document_processing", "simple_news_article")
        ]
        
        all_results = {}
        
        for task_name, document_id in test_scenarios:
            logger.info(f"Testing scenario: {task_name} with {document_id}")
            
            comparison_result = self.compare_tool_selections(task_name, document_id)
            all_results[task_name] = comparison_result
        
        # Overall analysis
        overall_stats = self._analyze_overall_results(all_results)
        
        final_results = {
            "validation_summary": {
                "total_scenarios_tested": len(test_scenarios),
                "timestamp": time.time(),
                "framework_version": "1.0_real_validation"
            },
            "scenario_results": all_results,
            "overall_analysis": overall_stats,
            "conclusions": self._draw_conclusions(overall_stats)
        }
        
        return final_results
    
    def _analyze_overall_results(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze results across all scenarios"""
        
        gemini_successes = 0
        total_tests = 0
        gemini_f1_scores = []
        gemini_completion_rates = []
        
        approach_success_counts = {}
        approach_f1_scores = {}
        
        for scenario_name, scenario_result in all_results.items():
            total_tests += 1
            
            approach_results = scenario_result["approach_results"]
            
            # Track Gemini performance
            if "gemini_selected" in approach_results:
                gemini_result = approach_results["gemini_selected"]
                if gemini_result["success"]:
                    gemini_successes += 1
                    gemini_f1_scores.append(gemini_result["overall_f1"])
                    gemini_completion_rates.append(gemini_result["completion_rate"])
            
            # Track all approaches
            for approach_name, result in approach_results.items():
                if approach_name not in approach_success_counts:
                    approach_success_counts[approach_name] = 0
                    approach_f1_scores[approach_name] = []
                
                if result["success"]:
                    approach_success_counts[approach_name] += 1
                    approach_f1_scores[approach_name].append(result["overall_f1"])
        
        # Calculate statistics
        gemini_success_rate = gemini_successes / total_tests if total_tests > 0 else 0
        gemini_avg_f1 = statistics.mean(gemini_f1_scores) if gemini_f1_scores else 0
        gemini_avg_completion = statistics.mean(gemini_completion_rates) if gemini_completion_rates else 0
        
        approach_rankings = []
        for approach_name, success_count in approach_success_counts.items():
            success_rate = success_count / total_tests
            avg_f1 = statistics.mean(approach_f1_scores[approach_name]) if approach_f1_scores[approach_name] else 0
            
            approach_rankings.append({
                "approach": approach_name,
                "success_rate": success_rate,
                "avg_f1": avg_f1,
                "total_successes": success_count
            })
        
        # Sort by F1 score
        approach_rankings.sort(key=lambda x: x["avg_f1"], reverse=True)
        
        return {
            "gemini_performance": {
                "success_rate": gemini_success_rate,
                "avg_f1_score": gemini_avg_f1,
                "avg_completion_rate": gemini_avg_completion,
                "rank_by_f1": next((i+1 for i, r in enumerate(approach_rankings) if r["approach"] == "gemini_selected"), "not_ranked")
            },
            "approach_rankings": approach_rankings,
            "best_approach": approach_rankings[0] if approach_rankings else None
        }
    
    def _draw_conclusions(self, overall_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Draw evidence-based conclusions from validation results"""
        
        gemini_perf = overall_stats["gemini_performance"]
        best_approach = overall_stats["best_approach"]
        
        conclusions = {
            "gemini_assessment": "",
            "comparative_ranking": "",
            "recommendations": [],
            "confidence_level": ""
        }
        
        # Assess Gemini's performance
        if gemini_perf["success_rate"] >= 0.8:
            conclusions["gemini_assessment"] = "Gemini tool selection is highly reliable"
        elif gemini_perf["success_rate"] >= 0.6:
            conclusions["gemini_assessment"] = "Gemini tool selection is moderately reliable"
        elif gemini_perf["success_rate"] >= 0.3:
            conclusions["gemini_assessment"] = "Gemini tool selection has mixed reliability"
        else:
            conclusions["gemini_assessment"] = "Gemini tool selection shows poor reliability"
        
        # Comparative ranking
        if isinstance(gemini_perf["rank_by_f1"], int):
            rank = gemini_perf["rank_by_f1"]
            if rank == 1:
                conclusions["comparative_ranking"] = "Gemini ranks #1 in tool selection quality"
            elif rank <= 3:
                conclusions["comparative_ranking"] = f"Gemini ranks #{rank} out of tested approaches"
            else:
                conclusions["comparative_ranking"] = f"Gemini ranks #{rank}, indicating room for improvement"
        else:
            conclusions["comparative_ranking"] = "Gemini failed to complete tasks successfully"
        
        # Recommendations
        if gemini_perf["avg_f1_score"] >= 0.7:
            conclusions["recommendations"].append("Gemini tool selection suitable for production use")
        elif gemini_perf["avg_f1_score"] >= 0.5:
            conclusions["recommendations"].append("Gemini tool selection viable with monitoring")
        else:
            conclusions["recommendations"].append("Alternative tool selection methods recommended")
        
        if best_approach and best_approach["approach"] != "gemini_selected":
            conclusions["recommendations"].append(f"Consider {best_approach['approach']} approach for better results")
        
        # Confidence level
        if overall_stats["approach_rankings"]:
            f1_range = max(r["avg_f1"] for r in overall_stats["approach_rankings"]) - min(r["avg_f1"] for r in overall_stats["approach_rankings"])
            if f1_range < 0.2:
                conclusions["confidence_level"] = "high - approaches perform similarly"
            elif f1_range < 0.4:
                conclusions["confidence_level"] = "medium - noticeable differences between approaches"
            else:
                conclusions["confidence_level"] = "high - clear performance differences observed"
        else:
            conclusions["confidence_level"] = "low - insufficient data for confident assessment"
        
        return conclusions
    
    def save_validation_results(self, results: Dict[str, Any], filename: Optional[str] = None) -> str:
        """Save validation results to file"""
        if not filename:
            timestamp = int(time.time())
            filename = f"task_completion_validation_{timestamp}.json"
        
        filepath = Path(__file__).parent / filename
        
        with open(filepath, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        logger.info(f"üíæ Validation results saved to: {filepath}")
        return str(filepath)


def main():
    """Run task completion validation"""
    
    validator = TaskCompletionValidator()
    
    try:
        results = validator.run_comprehensive_validation()
        
        # Save results
        results_file = validator.save_validation_results(results)
        
        # Print summary
        print("\n" + "="*70)
        print("üî¨ REAL TASK COMPLETION VALIDATION COMPLETE")
        print("="*70)
        
        overall = results["overall_analysis"]
        gemini_perf = overall["gemini_performance"]
        conclusions = results["conclusions"]
        
        print(f"Scenarios tested: {results['validation_summary']['total_scenarios_tested']}")
        print(f"\nü§ñ GEMINI PERFORMANCE:")
        print(f"Success rate: {gemini_perf['success_rate']:.1%}")
        print(f"Average F1 score: {gemini_perf['avg_f1_score']:.2f}")
        print(f"Ranking: #{gemini_perf['rank_by_f1']} by quality")
        
        print(f"\nüèÜ BEST APPROACH:")
        if overall["best_approach"]:
            best = overall["best_approach"]
            print(f"Method: {best['approach']}")
            print(f"Success rate: {best['success_rate']:.1%}")
            print(f"Average F1: {best['avg_f1']:.2f}")
        
        print(f"\nüìã CONCLUSIONS:")
        print(f"Assessment: {conclusions['gemini_assessment']}")
        print(f"Ranking: {conclusions['comparative_ranking']}")
        print(f"Confidence: {conclusions['confidence_level']}")
        
        print(f"\nüí° RECOMMENDATIONS:")
        for rec in conclusions["recommendations"]:
            print(f"‚Ä¢ {rec}")
        
        print(f"\nüìÑ Detailed results: {results_file}")
        print("="*70)
        
    except Exception as e:
        logger.error(f"Validation failed: {e}")
        raise


if __name__ == "__main__":
    main()