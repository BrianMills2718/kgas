#!/usr/bin/env python3
"""
REAL INVESTIGATION: Are Gemini's tool choices better, worse, or just different?

This script eliminates ALL mocking/simulation and provides complete visibility into:
1. What tools Gemini actually selected and WHY
2. What we assumed were optimal and WHY  
3. Objective comparison of the quality of these choices
4. Raw agent reasoning and decision-making process
"""

import os
import json
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any
import logging

# Import real components only
from mock_tool_generator import MockToolGenerator
from real_agents import create_real_agent, AgentType

logger = logging.getLogger(__name__)

# Load environment variables
def load_env():
    env_path = Path("/home/brian/projects/Digimons/.env")
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

load_env()

class ToolChoiceInvestigator:
    """Investigate the quality and reasoning behind tool choices"""
    
    def __init__(self):
        self.tool_generator = MockToolGenerator()
        self.all_tools = self.tool_generator.generate_all_tools()
        
        # Create detailed tool database for analysis
        self.tool_database = {}
        for tool in self.all_tools:
            self.tool_database[tool.tool_id] = {
                "id": tool.tool_id,
                "name": tool.name,
                "description": tool.description,
                "category": tool.category.value,
                "input_types": tool.input_types,
                "output_types": tool.output_types,
                "complexity_score": tool.complexity_score,
                "dependencies": tool.dependencies,
                "parallel_compatible": tool.parallel_compatible
            }
    
    def create_detailed_test_scenario(self) -> Dict[str, Any]:
        """Create a specific, measurable test scenario"""
        return {
            "task_id": "academic_ml_paper_analysis",
            "description": """
Analyze this academic machine learning paper: "Deep Neural Networks for Sentiment Analysis: A Comparative Study"

The paper contains:
- Abstract with key contributions
- Related work section citing 15 papers  
- Methodology using BERT, RoBERTa, and custom architectures
- Experiments on IMDB, Yelp, and Amazon review datasets
- Results showing 94.2% accuracy on IMDB, 91.8% on Yelp
- Discussion of limitations and future work

Your task: Extract the methodologies, datasets, performance metrics, and create a knowledge graph showing relationships between methods and their performance on different datasets.
            """.strip(),
            "context": {
                "document_type": "academic_paper",
                "domain": "machine_learning",
                "subdomain": "sentiment_analysis", 
                "complexity": "high",
                "expected_entities": ["BERT", "RoBERTa", "IMDB", "Yelp", "Amazon", "94.2%", "91.8%"],
                "expected_relationships": ["method-dataset", "method-performance", "dataset-accuracy"],
                "document_structure": ["abstract", "related_work", "methodology", "experiments", "results", "discussion"]
            },
            "success_criteria": {
                "must_extract": ["methodologies", "datasets", "performance_metrics"],
                "must_identify": ["relationships between methods and performance"],
                "must_create": ["structured knowledge graph"],
                "quality_metrics": ["completeness", "accuracy", "relationship_quality"]
            }
        }
    
    def analyze_our_assumptions(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze why we assumed certain tools were optimal"""
        
        # Our original assumptions - let's be explicit about our reasoning
        assumed_optimal = {
            "load_document_comprehensive_pdf": {
                "reason": "Academic papers are PDFs, need metadata extraction for citations/structure",
                "assumption": "PDF loading with metadata is essential for academic analysis",
                "inputs_needed": ["pdf_file"],
                "outputs_provided": ["text", "metadata", "structure"]
            },
            "extract_entities_spacy_advanced": {
                "reason": "Need to extract technical terms like BERT, RoBERTa, dataset names",
                "assumption": "Advanced NER is needed for technical entity extraction",
                "inputs_needed": ["structured_text"], 
                "outputs_provided": ["entities", "entity_types", "confidence_scores"]
            },
            "build_knowledge_graph_academic": {
                "reason": "Task explicitly requires knowledge graph creation",
                "assumption": "Academic-specific graph building is superior to generic",
                "inputs_needed": ["entities", "relationships"],
                "outputs_provided": ["knowledge_graph", "academic_structure"]
            },
            "analyze_methodology_patterns": {
                "reason": "Need to identify method-performance relationships",
                "assumption": "Specialized methodology analysis is required",
                "inputs_needed": ["knowledge_graph", "domain_entities"],
                "outputs_provided": ["methodology_insights", "performance_analysis"]
            },
            "export_academic_summary": {
                "reason": "Academic format output with proper citations and structure",
                "assumption": "Academic-specific export format is preferred",
                "inputs_needed": ["knowledge_graph", "methodology_analysis"],
                "outputs_provided": ["academic_report", "structured_summary"]
            }
        }
        
        return {
            "assumed_optimal_tools": assumed_optimal,
            "total_assumed": len(assumed_optimal),
            "reasoning_basis": "domain_specificity_and_task_requirements",
            "assumptions_made": [
                "Academic papers need PDF-specific processing",
                "Advanced NER is better than basic for technical terms", 
                "Domain-specific tools outperform generic ones",
                "Multi-step pipeline is necessary for complex tasks",
                "Academic output format is required"
            ]
        }
    
    async def get_detailed_agent_reasoning(self, scenario: Dict[str, Any], 
                                         available_tools: List[Dict]) -> Dict[str, Any]:
        """Get complete agent reasoning with full visibility"""
        
        print(f"\nüîç Getting detailed reasoning from Gemini-2.5-flash...")
        print(f"   Available tools: {len(available_tools)}")
        
        # Create enhanced prompt that asks for detailed reasoning
        enhanced_prompt = f"""
You are a tool selection expert. You need to select the best tools for this specific task.

TASK: {scenario['description']}

CONTEXT: {json.dumps(scenario['context'], indent=2)}

SUCCESS CRITERIA: {json.dumps(scenario['success_criteria'], indent=2)}

AVAILABLE TOOLS ({len(available_tools)} total):
{json.dumps(available_tools, indent=2)}

Please analyze this task carefully and select the most appropriate tools. For each tool you select, explain:

1. WHY you chose this specific tool over alternatives
2. HOW this tool contributes to the success criteria  
3. WHAT alternatives you considered and why you rejected them
4. WHAT order you would use these tools in

Return your response as JSON:
{{
  "selected_tools": [
    {{
      "tool": "exact_tool_name",
      "parameters": {{"param": "value"}},
      "reasoning": "detailed explanation of why this tool",
      "alternatives_considered": ["tool1", "tool2"],
      "alternatives_rejected_because": "specific reasons",
      "contribution_to_success": "how this helps achieve success criteria",
      "execution_order": 1
    }}
  ],
  "overall_strategy": "your high-level approach to this task",
  "potential_weaknesses": "what might not work perfectly with this selection",
  "confidence_level": "high/medium/low and why"
}}
        """
        
        try:
            agent = create_real_agent(AgentType.GEMINI_FLASH)
            
            start_time = time.time()
            selected_tools = await agent.select_tools_for_workflow(
                enhanced_prompt,
                available_tools,
                scenario['context']
            )
            selection_time = time.time() - start_time
            
            return {
                "selection_time_seconds": selection_time,
                "raw_selection": selected_tools,
                "agent_type": "gemini-2.5-flash",
                "timestamp": time.time(),
                "prompt_used": enhanced_prompt,
                "tools_available": len(available_tools)
            }
            
        except Exception as e:
            print(f"‚ùå Error getting agent reasoning: {e}")
            return {"error": str(e)}
    
    def compare_tool_choices(self, our_assumptions: Dict, agent_choice: Dict, 
                           scenario: Dict) -> Dict[str, Any]:
        """Objective comparison of tool choice quality"""
        
        if "error" in agent_choice:
            return {"error": "Cannot compare due to agent error"}
        
        # Extract agent's actual choices
        agent_tools = []
        if isinstance(agent_choice.get("raw_selection"), list):
            for tool in agent_choice["raw_selection"]:
                if isinstance(tool, dict):
                    tool_name = tool.get("tool", tool.get("name", "unknown"))
                    agent_tools.append(tool_name)
        
        our_tools = list(our_assumptions["assumed_optimal_tools"].keys())
        
        comparison = {
            "our_assumptions": {
                "tools": our_tools,
                "count": len(our_tools),
                "approach": "comprehensive_multi_step_academic_pipeline"
            },
            "agent_choice": {
                "tools": agent_tools,
                "count": len(agent_tools),
                "selection_time": agent_choice.get("selection_time_seconds", 0)
            },
            "overlap": {
                "common_tools": list(set(our_tools) & set(agent_tools)),
                "overlap_count": len(set(our_tools) & set(agent_tools)),
                "overlap_percentage": len(set(our_tools) & set(agent_tools)) / max(len(our_tools), len(agent_tools)) * 100
            }
        }
        
        # Analyze each choice
        tool_analysis = {}
        
        # Analyze our assumptions
        for tool_name, details in our_assumptions["assumed_optimal_tools"].items():
            tool_analysis[tool_name] = {
                "our_reasoning": details["reason"],
                "our_assumption": details["assumption"],
                "selected_by_agent": tool_name in agent_tools,
                "tool_exists": tool_name in self.tool_database,
                "tool_details": self.tool_database.get(tool_name, "Tool not found in database")
            }
        
        # Analyze agent's choices
        for tool_name in agent_tools:
            if tool_name not in tool_analysis:
                tool_analysis[tool_name] = {
                    "selected_by_agent": True,
                    "selected_by_us": False,
                    "tool_exists": tool_name in self.tool_database,  
                    "tool_details": self.tool_database.get(tool_name, "Tool not found in database")
                }
        
        comparison["detailed_tool_analysis"] = tool_analysis
        
        # Quality assessment
        comparison["quality_assessment"] = self._assess_choice_quality(
            our_tools, agent_tools, scenario, tool_analysis
        )
        
        return comparison
    
    def _assess_choice_quality(self, our_tools: List[str], agent_tools: List[str], 
                             scenario: Dict, tool_analysis: Dict) -> Dict[str, Any]:
        """Objective quality assessment of tool choices"""
        
        assessment = {
            "task_coverage": {},
            "efficiency": {},
            "appropriateness": {},
            "overall_verdict": {}
        }
        
        # Task coverage analysis
        required_capabilities = ["document_loading", "entity_extraction", "knowledge_graph_creation"]
        
        our_coverage = self._check_capability_coverage(our_tools, required_capabilities)
        agent_coverage = self._check_capability_coverage(agent_tools, required_capabilities)
        
        assessment["task_coverage"] = {
            "our_coverage": our_coverage,
            "agent_coverage": agent_coverage,
            "coverage_comparison": "our" if sum(our_coverage.values()) > sum(agent_coverage.values()) else "agent"
        }
        
        # Efficiency analysis
        assessment["efficiency"] = {
            "our_tool_count": len(our_tools),
            "agent_tool_count": len(agent_tools), 
            "more_efficient": "agent" if len(agent_tools) < len(our_tools) else "our" if len(our_tools) < len(agent_tools) else "equal"
        }
        
        # Appropriateness analysis based on actual tool descriptions
        our_appropriateness = self._assess_tool_appropriateness(our_tools, scenario)
        agent_appropriateness = self._assess_tool_appropriateness(agent_tools, scenario)
        
        assessment["appropriateness"] = {
            "our_score": our_appropriateness,
            "agent_score": agent_appropriateness,
            "more_appropriate": "agent" if agent_appropriateness > our_appropriateness else "our"
        }
        
        return assessment
    
    def _check_capability_coverage(self, tools: List[str], required_capabilities: List[str]) -> Dict[str, bool]:
        """Check if tools cover required capabilities"""
        coverage = {}
        
        for capability in required_capabilities:
            coverage[capability] = False
            for tool_name in tools:
                if tool_name in self.tool_database:
                    tool_info = self.tool_database[tool_name]
                    # Simple heuristic based on tool name and category
                    if capability == "document_loading" and ("load" in tool_name.lower() or tool_info["category"] == "document_loaders"):
                        coverage[capability] = True
                    elif capability == "entity_extraction" and ("extract" in tool_name.lower() or tool_info["category"] == "entity_extraction"):
                        coverage[capability] = True
                    elif capability == "knowledge_graph_creation" and ("graph" in tool_name.lower() or "build" in tool_name.lower()):
                        coverage[capability] = True
        
        return coverage
    
    def _assess_tool_appropriateness(self, tools: List[str], scenario: Dict) -> float:
        """Assess how appropriate tools are for the scenario"""
        if not tools:
            return 0.0
        
        total_score = 0.0
        for tool_name in tools:
            if tool_name in self.tool_database:
                tool_info = self.tool_database[tool_name]
                score = 0.0
                
                # Domain appropriateness
                if scenario["context"]["domain"] == "machine_learning":
                    if "academic" in tool_name.lower() or "advanced" in tool_name.lower():
                        score += 0.3
                
                # Task appropriateness  
                if "knowledge_graph" in scenario["task_id"]:
                    if "graph" in tool_name.lower() or "knowledge" in tool_name.lower():
                        score += 0.4
                
                # Complexity appropriateness
                if scenario["context"]["complexity"] == "high":
                    if tool_info["complexity_score"] > 0.6:
                        score += 0.3
                
                total_score += score
        
        return total_score / len(tools)
    
    async def run_investigation(self) -> Dict[str, Any]:
        """Run complete investigation with full visibility"""
        
        print("üîç INVESTIGATION: LLM Tool Choices vs Our Assumptions")
        print("=" * 60)
        print("Eliminating ALL mocking, simulation, and fallbacks...")
        
        # Verify real API access
        if not os.getenv("GOOGLE_API_KEY"):
            raise ValueError("GOOGLE_API_KEY required - no fallbacks allowed")
        
        # Create test scenario
        scenario = self.create_detailed_test_scenario()
        print(f"\nüìã Test Scenario: {scenario['task_id']}")
        print(f"   Domain: {scenario['context']['domain']}")
        print(f"   Complexity: {scenario['context']['complexity']}")
        
        # Analyze our assumptions
        our_assumptions = self.analyze_our_assumptions(scenario)
        print(f"\nü§î Our Assumptions:")
        print(f"   Assumed optimal tools: {len(our_assumptions['assumed_optimal_tools'])}")
        for tool, details in our_assumptions['assumed_optimal_tools'].items():
            print(f"   - {tool}: {details['reason']}")
        
        # Prepare tools for agent (all 100 tools available)
        available_tools = []
        for tool in self.all_tools:
            available_tools.append({
                "name": tool.tool_id,
                "description": tool.description,
                "category": tool.category.value,
                "inputs": tool.input_types,
                "outputs": tool.output_types,
                "complexity": tool.complexity_score
            })
        
        print(f"\nüõ†Ô∏è Available to Agent: {len(available_tools)} tools")
        
        # Get agent's detailed reasoning (NO FALLBACKS)
        agent_reasoning = await self.get_detailed_agent_reasoning(scenario, available_tools)
        
        if "error" in agent_reasoning:
            raise Exception(f"Agent selection failed: {agent_reasoning['error']}")
        
        print(f"\nü§ñ Agent Selection Complete:")
        print(f"   Selection time: {agent_reasoning['selection_time_seconds']:.1f}s")
        if isinstance(agent_reasoning.get("raw_selection"), list):
            tools_selected = [tool.get("tool", tool.get("name", "unknown")) for tool in agent_reasoning["raw_selection"]]
            print(f"   Tools selected: {tools_selected}")
        
        # Compare choices objectively
        comparison = self.compare_tool_choices(our_assumptions, agent_reasoning, scenario)
        
        # Compile complete investigation
        investigation = {
            "investigation_info": {
                "timestamp": time.time(),
                "scenario": scenario,
                "no_mocking": True,
                "no_simulation": True,
                "no_fallbacks": True,
                "real_api_used": True
            },
            "our_assumptions": our_assumptions,
            "agent_reasoning": agent_reasoning,
            "comparison": comparison,
            "conclusions": self._draw_conclusions(comparison)
        }
        
        return investigation
    
    def _draw_conclusions(self, comparison: Dict) -> Dict[str, Any]:
        """Draw objective conclusions from the comparison"""
        
        conclusions = {
            "choice_quality": "unknown",
            "reasoning": [],
            "recommendations": []
        }
        
        if "error" in comparison:
            conclusions["choice_quality"] = "cannot_assess"
            conclusions["reasoning"] = ["Agent error prevented comparison"]
            return conclusions
        
        # Analyze the comparison data
        quality = comparison.get("quality_assessment", {})
        overlap = comparison.get("overlap", {})
        
        # Determine choice quality
        if quality.get("appropriateness", {}).get("more_appropriate") == "agent":
            if quality.get("efficiency", {}).get("more_efficient") == "agent":
                conclusions["choice_quality"] = "agent_better"
                conclusions["reasoning"].append("Agent choices are more appropriate AND more efficient")
            else:
                conclusions["choice_quality"] = "agent_different_better"
                conclusions["reasoning"].append("Agent choices are more appropriate but less efficient")
        elif quality.get("appropriateness", {}).get("more_appropriate") == "our":
            conclusions["choice_quality"] = "our_assumptions_better"
            conclusions["reasoning"].append("Our assumptions are more appropriate for the task")
        else:
            if overlap.get("overlap_percentage", 0) > 50:
                conclusions["choice_quality"] = "similar_approaches"
                conclusions["reasoning"].append("High overlap suggests similar thinking")
            else:
                conclusions["choice_quality"] = "fundamentally_different"
                conclusions["reasoning"].append("Low overlap suggests different approaches")
        
        # Generate recommendations
        if conclusions["choice_quality"] in ["agent_better", "agent_different_better"]:
            conclusions["recommendations"].append("Consider revising our assumptions based on agent preferences")
            conclusions["recommendations"].append("Investigate why agent approach may be superior")
        elif conclusions["choice_quality"] == "our_assumptions_better":
            conclusions["recommendations"].append("Consider improving tool descriptions to guide agent selection")
            conclusions["recommendations"].append("Investigate why agent missed optimal choices")
        else:
            conclusions["recommendations"].append("Further investigation needed to determine optimal approach")
        
        return conclusions


async def main():
    """Run the complete investigation"""
    
    logging.basicConfig(level=logging.INFO)
    
    print("üéØ REAL INVESTIGATION: No Mocking, No Simulation, No Fallbacks")
    print("Full visibility into LLM tool choice quality vs our assumptions")
    
    investigator = ToolChoiceInvestigator()
    
    try:
        investigation = await investigator.run_investigation()
        
        # Save complete investigation
        timestamp = int(time.time())
        filename = f"tool_choice_investigation_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(investigation, f, indent=2, default=str)
        
        # Print analysis
        print(f"\nüìä INVESTIGATION RESULTS")
        print("=" * 40)
        
        comparison = investigation["comparison"]
        conclusions = investigation["conclusions"]
        
        print(f"\nüîç Choice Comparison:")
        print(f"   Our tools: {comparison['our_assumptions']['count']}")
        print(f"   Agent tools: {comparison['agent_choice']['count']}")  
        print(f"   Overlap: {comparison['overlap']['overlap_count']} tools ({comparison['overlap']['overlap_percentage']:.1f}%)")
        
        print(f"\nüéØ Quality Assessment:")
        quality = comparison.get("quality_assessment", {})
        if quality:
            print(f"   Task coverage: {quality.get('task_coverage', {}).get('coverage_comparison', 'unknown')} approach better")
            print(f"   Efficiency: {quality.get('efficiency', {}).get('more_efficient', 'unknown')} approach more efficient") 
            print(f"   Appropriateness: {quality.get('appropriateness', {}).get('more_appropriate', 'unknown')} approach more appropriate")
        
        print(f"\nüèÜ Conclusions:")
        print(f"   Choice Quality: {conclusions['choice_quality']}")
        for reason in conclusions["reasoning"]:
            print(f"   ‚Ä¢ {reason}")
        
        print(f"\nüí° Recommendations:")
        for rec in conclusions["recommendations"]:
            print(f"   ‚Ä¢ {rec}")
        
        print(f"\nüìÑ Complete investigation saved to: {filename}")
        
        return investigation
        
    except Exception as e:
        print(f"‚ùå Investigation failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())