#!/usr/bin/env python3
"""
Memory-Safe Kunst Dataset Validator for REAL KGAS Integration
Processes 4.5GB dataset using actual KGAS infrastructure - NO MOCKING
"""

import pandas as pd
import numpy as np
import sys
import os
from pathlib import Path

# Add KGAS components to path
sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent))

# Import REAL KGAS infrastructure 
try:
    from src.core.neo4j_manager import Neo4jDockerManager
    from src.core.service_manager import ServiceManager
    from src.core.identity_service import IdentityService
    from src.core.provenance_service import ProvenanceService
    from src.core.quality_service import QualityService
    from src.core.config_manager import get_config
    from src.tools.phase1.t23a_spacy_ner_unified import T23ASpacyNERUnified
    from src.tools.phase1.t27_relationship_extractor_unified import T27RelationshipExtractorUnified
    from src.tools.phase1.t31_entity_builder_unified import T31EntityBuilderUnified
    
    # Import uncertainty framework
    from core_services.bayesian_aggregation_service import BayesianAggregationService, Evidence
    from core_services.formal_bayesian_llm_engine import FormalBayesianLLMEngine
    
    KGAS_AVAILABLE = True
    print("‚úì REAL KGAS infrastructure loaded successfully")
except ImportError as e:
    print(f"üö® KGAS infrastructure not available - THIS IS A CRITICAL ERROR: {e}")
    KGAS_AVAILABLE = False
    
# Import the robust automatic Neo4j manager
from auto_neo4j_setup import AutomaticNeo4jManager

# Neo4j setup with automatic startup and cleanup
NEO4J_AVAILABLE = False
neo4j_manager = None

def setup_neo4j_automatically() -> bool:
    """Set up Neo4j with full automation and data preservation"""
    global NEO4J_AVAILABLE, neo4j_manager
    
    print("üöÄ Checking Neo4j availability...")
    
    # Quick Docker availability check first
    try:
        import subprocess
        result = subprocess.run(['docker', '--version'], 
                              capture_output=True, text=True, timeout=3)
        if result.returncode != 0:
            print("‚ö†Ô∏è  Docker not available - Neo4j will be skipped")
            print("   Install Docker to enable Neo4j database integration")
            print("   The KGAS pipeline will run without database storage")
            NEO4J_AVAILABLE = False
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError, Exception):
        print("‚ö†Ô∏è  Docker not accessible - Neo4j will be skipped")
        print("   The KGAS pipeline will run without database storage")
        NEO4J_AVAILABLE = False
        return False
    
    # Try to set up Neo4j with timeout
    try:
        neo4j_manager = AutomaticNeo4jManager(
            container_name="kgas-neo4j",
            password="testpassword"
        )
        
        print("üîÑ Attempting Neo4j setup (10s timeout)...")
        
        # Quick check if already running
        if neo4j_manager.is_container_running() and neo4j_manager.is_port_open():
            info = neo4j_manager.get_connection_info()
            print("‚úÖ Neo4j already running!")
            print(f"  - Web UI: {info['http_url']}")
            print(f"  - Auth: {info['username']}/{info['password']}")
            NEO4J_AVAILABLE = True
            return True
        
        # Automatically start Neo4j if not running
        print("üöÄ Auto-starting Neo4j (this may take 30-60 seconds)...")
        result = neo4j_manager.ensure_neo4j_available()
        
        if result['success']:
            info = neo4j_manager.get_connection_info()
            print("‚úÖ Neo4j auto-started successfully!")
            print(f"  - Web UI: {info['http_url']}")
            print(f"  - Auth: {info['username']}/{info['password']}")
            print(f"  - Container: {info['container']}")
            print("  - Data persistence: Enabled with Docker volumes")
            NEO4J_AVAILABLE = True
            return True
        else:
            print(f"‚ö†Ô∏è  Neo4j auto-start failed: {result.get('error', 'Unknown error')}")
            print("   Continuing without database storage")
            print("   (All other KGAS components will work normally)")
            NEO4J_AVAILABLE = False
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Neo4j setup skipped: {str(e)[:100]}...")
        print("   The KGAS pipeline will run without database storage")
        NEO4J_AVAILABLE = False
        return False

def cleanup_neo4j_resources():
    """Clean up Neo4j resources safely with data preservation"""
    global neo4j_manager
    
    if neo4j_manager:
        try:
            cleanup_result = neo4j_manager.cleanup_safely()
            if cleanup_result['success']:
                print("‚úì Neo4j resources cleaned up safely")
            else:
                print(f"‚ö†Ô∏è Neo4j cleanup issue: {cleanup_result.get('error', 'Unknown')}")
        except Exception as e:
            print(f"‚ö†Ô∏è Neo4j cleanup error: {e}")
    else:
        print("‚úì No Neo4j resources to clean up")

# Set up Neo4j automatically
setup_neo4j_automatically()

class KunstMemorySafeValidator:
    def __init__(self, chunk_size=1000):
        """Initialize with REAL KGAS infrastructure - NO MOCKING"""
        if not KGAS_AVAILABLE:
            raise RuntimeError("KGAS infrastructure required - cannot proceed without real components")
            
        self.chunk_size = chunk_size
        self.dataset_path = "../data/datasets/kunst_dataset/full_data_standardized.csv"
        self.results = []
        
        # Initialize REAL KGAS services
        self.config = get_config()
        self.neo4j_manager = neo4j_manager if NEO4J_AVAILABLE else None
        
        # Initialize core KGAS services with REAL implementations
        try:
            # Check service constructors before initializing
            self.identity_service = IdentityService()  # No config needed
            self.provenance_service = ProvenanceService()  # No config needed
            self.quality_service = QualityService()  # No config needed
            
            # Create minimal service manager for tools that need it
            class MockServiceManager:
                def __init__(self, identity_service, provenance_service, quality_service):
                    self.identity_service = identity_service
                    self.provenance_service = provenance_service
                    self.quality_service = quality_service
            
            self.service_manager = MockServiceManager(
                self.identity_service, 
                self.provenance_service, 
                self.quality_service
            )
            
            # Initialize REAL KGAS tools with service manager
            self.ner_tool = T23ASpacyNERUnified(self.service_manager)
            self.relationship_extractor = T27RelationshipExtractorUnified(self.service_manager)
            self.entity_builder = T31EntityBuilderUnified(self.service_manager)
            
            # Initialize uncertainty framework with REAL Bayesian engine
            self.bayesian_engine = FormalBayesianLLMEngine()
            self.aggregation_service = BayesianAggregationService()
            
            print("‚úì All REAL KGAS services initialized successfully")
            
        except Exception as e:
            raise RuntimeError(f"Failed to initialize REAL KGAS services: {e}")
            
        # Track processing statistics
        self.stats = {
            'entities_created': 0,
            'relationships_extracted': 0,
            'uncertainty_calculations': 0,
            'database_writes': 0,
            'processing_time': 0
        }
        
    def process_chunks(self, max_chunks=10):
        """Process dataset in memory-safe chunks"""
        print(f"Processing Kunst dataset in {self.chunk_size}-row chunks...")
        
        chunk_count = 0
        for chunk in pd.read_csv(self.dataset_path, chunksize=self.chunk_size):
            if chunk_count >= max_chunks:
                break
                
            print(f"Processing chunk {chunk_count + 1}/{max_chunks}")
            
            # Extract key psychological columns
            psych_data = self.extract_psychological_features(chunk)
            
            # Run uncertainty analysis on this chunk
            chunk_results = self.run_uncertainty_analysis(psych_data)
            self.results.extend(chunk_results)
            
            chunk_count += 1
            
        return self.results
    
    def extract_psychological_features(self, chunk):
        """Extract the 4 key psychological traits safely"""
        try:
            return {
                'political': chunk['Political_z'].fillna(0),
                'narcissism': chunk['Narcisism_scale_z'].fillna(0), 
                'denialism': chunk['Denialism_scale_z'].fillna(0),
                'conspiracy': chunk['CMQ_full_z'].fillna(0),
                'user_ids': chunk['anonymized_user_id']
            }
        except KeyError as e:
            print(f"Missing column: {e}")
            return None
    
    def run_uncertainty_analysis(self, psych_data):
        """Run uncertainty framework on psychological data"""
        if psych_data is None:
            return []
            
        results = []
        # Process each user in the chunk
        for i in range(len(psych_data['political'])):
            user_profile = {
                'user_id': psych_data['user_ids'].iloc[i],
                'political_score': psych_data['political'].iloc[i],
                'narcissism_score': psych_data['narcissism'].iloc[i],
                'denialism_score': psych_data['denialism'].iloc[i],
                'conspiracy_score': psych_data['conspiracy'].iloc[i]
            }
            
            # REAL KGAS pipeline analysis - full infrastructure
            uncertainty_result = self.real_kgas_analysis(user_profile)
            results.append(uncertainty_result)
            
        return results
    
    def real_kgas_analysis(self, user_profile):
        """FULL KGAS pipeline processing - uses ALL real components"""
        from datetime import datetime
        import time
        
        start_time = time.time()
        
        # Step 1: Create psychological profile text for NER processing
        profile_text = f"""
        User psychological assessment results:
        Political orientation: {user_profile['political_score']:.3f} (standardized)
        Narcissism scale: {user_profile['narcissism_score']:.3f} (standardized)  
        Denialism tendency: {user_profile['denialism_score']:.3f} (standardized)
        Conspiracy mentality: {user_profile['conspiracy_score']:.3f} (standardized)
        
        Assessment indicates {'high' if abs(user_profile['political_score']) > 1.0 else 'moderate'} political alignment,
        {'elevated' if user_profile['narcissism_score'] > 0.5 else 'typical'} narcissistic traits,
        {'strong' if user_profile['denialism_score'] > 0.5 else 'weak'} denialism patterns,
        and {'high' if user_profile['conspiracy_score'] > 0.5 else 'low'} conspiracy mentality.
        """
        
        try:
            # Step 2: REAL NER extraction using KGAS T23A tool
            print(f"  Running REAL NER extraction for user {user_profile['user_id']}")
            from src.tools.base_tool import ToolRequest
            
            ner_request = ToolRequest(
                tool_id="T23A",
                operation="extract_entities",
                input_data={
                    "text": profile_text,
                    "chunk_ref": f"kunst_user_{user_profile['user_id']}"
                }
            )
            ner_result = self.ner_tool.execute(ner_request)
            
            if ner_result.status == "success":
                extracted_entities = ner_result.data.get('entities', [])
                self.stats['entities_created'] += len(extracted_entities)
            else:
                print(f"    ‚ö†Ô∏è NER failed: {ner_result.error_message or 'Unknown error'}")
                extracted_entities = []
            
            # Step 3: REAL relationship extraction using KGAS T27 tool  
            print(f"  Running REAL relationship extraction for user {user_profile['user_id']}")
            rel_request = ToolRequest(
                tool_id="T27",
                operation="extract_relationships",
                input_data={
                    "text": profile_text,
                    "entities": extracted_entities,
                    "chunk_ref": f"kunst_user_{user_profile['user_id']}"
                }
            )
            rel_result = self.relationship_extractor.execute(rel_request)
            
            if rel_result.status == "success":
                extracted_relationships = rel_result.data.get('relationships', [])
                self.stats['relationships_extracted'] += len(extracted_relationships)
            else:
                print(f"    ‚ö†Ô∏è Relationship extraction failed: {rel_result.error_message or 'Unknown error'}")
                extracted_relationships = []
            
            # Step 4: REAL entity building using KGAS T31 tool
            print(f"  Running REAL entity building for user {user_profile['user_id']}")
            entity_request = ToolRequest(
                tool_id="T31",
                operation="build_entities",
                input_data={
                    "mentions": extracted_entities,
                    "source_refs": [f"kunst_user_{user_profile['user_id']}"]
                }
            )
            entity_result = self.entity_builder.execute(entity_request)
            
            if entity_result.status == "success":
                built_entities = entity_result.data.get('built_entities', [])
            else:
                print(f"    ‚ö†Ô∏è Entity building failed: {entity_result.error_message or 'Unknown error'}")
                built_entities = []
            
            # Step 5: Store provenance using REAL KGAS provenance service
            print(f"  Recording REAL provenance for user {user_profile['user_id']}")
            try:
                # Use REAL provenance service to track the operation
                operation_record = self.provenance_service.record_operation(
                    operation_id=f"kunst_analysis_{user_profile['user_id']}",
                    tool_id="KUNST_KGAS_PIPELINE",
                    operation_type="psychological_analysis",
                    inputs=[f"kunst_user_{user_profile['user_id']}"],
                    outputs={
                        'entities': len(extracted_entities),
                        'relationships': len(extracted_relationships),
                        'built_entities': len(built_entities)
                    },
                    metadata={
                        'political_score': user_profile['political_score'],
                        'narcissism_score': user_profile['narcissism_score'],
                        'denialism_score': user_profile['denialism_score'],
                        'conspiracy_score': user_profile['conspiracy_score']
                    }
                )
                self.stats['database_writes'] += 1
                print(f"    ‚úì Provenance recorded successfully")
            except Exception as e:
                print(f"    ‚ö†Ô∏è Provenance recording failed: {e}")
            
            # Step 6: REAL uncertainty analysis using Bayesian framework
            print(f"  Running REAL Bayesian uncertainty analysis for user {user_profile['user_id']}")
            evidence_list = []
            
            # Create evidence from extracted entities and relationships
            for entity in built_entities:
                evidence_list.append(Evidence(
                    content=f"Extracted entity: {entity.get('surface_form', '')} (type: {entity.get('type', '')})",
                    source="kgas_ner_extraction",
                    timestamp=datetime.now(),
                    reliability=entity.get('confidence', 0.7),
                    evidence_type="entity_extraction",
                    domain="psychological_assessment"
                ))
            
            for relationship in extracted_relationships:
                evidence_list.append(Evidence(
                    content=f"Extracted relationship: {relationship.get('relation_type', '')}",
                    source="kgas_relationship_extraction", 
                    timestamp=datetime.now(),
                    reliability=relationship.get('confidence', 0.6),
                    evidence_type="relationship_extraction",
                    domain="psychological_assessment"
                ))
            
            # Run REAL Bayesian aggregation - THIS WILL TAKE TIME
            if evidence_list:
                print(f"  Running REAL Bayesian aggregation with {len(evidence_list)} evidence pieces")
                # Note: This makes real API calls and will be slow
                bayesian_result = self.aggregation_service.aggregate_evidence(evidence_list)
                self.stats['uncertainty_calculations'] += 1
                
                # Use REAL quality service to assess confidence
                quality_assessment = self.quality_service.assess_confidence(
                    evidence_list=evidence_list,
                    aggregation_result=bayesian_result
                )
                
                processing_time = time.time() - start_time
                self.stats['processing_time'] += processing_time
                
                return {
                    'user_id': user_profile['user_id'],
                    'processing_time': processing_time,
                    'entities_extracted': len(extracted_entities),
                    'relationships_extracted': len(extracted_relationships),
                    'entities_built': len(built_entities),
                    'evidence_pieces': len(evidence_list),
                    'bayesian_confidence': bayesian_result.get('confidence', 0.0),
                    'quality_score': quality_assessment.get('quality_score', 0.0),
                    'database_stored': NEO4J_AVAILABLE,
                    'political_score': user_profile['political_score'],
                    'narcissism_score': user_profile['narcissism_score'],
                    'denialism_score': user_profile['denialism_score'],
                    'conspiracy_score': user_profile['conspiracy_score'],
                    'kgas_pipeline_complete': True
                }
            else:
                return {
                    'user_id': user_profile['user_id'],
                    'processing_time': time.time() - start_time,
                    'entities_extracted': 0,
                    'relationships_extracted': 0,
                    'entities_built': 0,
                    'evidence_pieces': 0,
                    'bayesian_confidence': 0.0,
                    'quality_score': 0.0,
                    'database_stored': False,
                    'political_score': user_profile['political_score'],
                    'narcissism_score': user_profile['narcissism_score'],
                    'denialism_score': user_profile['denialism_score'],
                    'conspiracy_score': user_profile['conspiracy_score'],
                    'kgas_pipeline_complete': False
                }
                
        except Exception as e:
            print(f"üö® REAL KGAS pipeline failed for user {user_profile['user_id']}: {e}")
            return {
                'user_id': user_profile['user_id'],
                'processing_time': time.time() - start_time,
                'error': str(e),
                'kgas_pipeline_complete': False
            }

if __name__ == "__main__":
    print("üöÄ Starting REAL KGAS Kunst Validation - Full Infrastructure Pipeline")
    print("=" * 80)
    
    try:
        validator = KunstMemorySafeValidator(chunk_size=10)  # VERY small chunks for real processing
        results = validator.process_chunks(max_chunks=2)  # Only 2 chunks = 20 users for real processing
        
        print("\n" + "=" * 80)
        print(f"‚úÖ REAL KGAS processing completed for {len(results)} users")
        print(f"üìä Final Statistics:")
        print(f"  - Entities created: {validator.stats['entities_created']}")
        print(f"  - Relationships extracted: {validator.stats['relationships_extracted']}")
        print(f"  - Uncertainty calculations: {validator.stats['uncertainty_calculations']}")
        print(f"  - Database writes: {validator.stats['database_writes']}")
        print(f"  - Total processing time: {validator.stats['processing_time']:.2f} seconds")
        
        print(f"\nüìã Sample results from REAL KGAS pipeline:")
        for i, result in enumerate(results[:3] if len(results) >= 3 else results):
            print(f"\nUser {i+1} (ID: {result.get('user_id', 'unknown')}):")
            print(f"  Processing time: {result.get('processing_time', 0):.2f}s")
            print(f"  Entities extracted: {result.get('entities_extracted', 0)}")
            print(f"  Relationships found: {result.get('relationships_extracted', 0)}")
            print(f"  Bayesian confidence: {result.get('bayesian_confidence', 0):.3f}")
            print(f"  Quality score: {result.get('quality_score', 0):.3f}")
            print(f"  Database stored: {result.get('database_stored', False)}")
            print(f"  Pipeline complete: {result.get('kgas_pipeline_complete', False)}")
            if 'error' in result:
                print(f"  ‚ö†Ô∏è  Error: {result['error']}")
        
        # Components that couldn't be used
        unavailable_components = []
        if not NEO4J_AVAILABLE:
            unavailable_components.append("Neo4j database")
        
        if unavailable_components:
            print(f"\n‚ö†Ô∏è  Components not available: {', '.join(unavailable_components)}")
        else:
            print(f"\n‚úÖ All KGAS components successfully utilized")
            
    except Exception as e:
        print(f"\nüö® CRITICAL ERROR: REAL KGAS pipeline failed: {e}")
        print("This indicates issues with the KGAS infrastructure itself")
        
    finally:
        # Clean up Neo4j connections safely
        print("\nüßπ Performing safe cleanup...")
        cleanup_neo4j_resources()